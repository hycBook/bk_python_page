{"./":{"url":"./","title":"Introduction","keywords":"","body":" &#x1F40D; python学习记录 其中有些来自一些博客论坛，能加上原文地址的都已在文中尽可能加上 如有侵权，欢迎联系作者~ 1832044043@qq.com 个人论文主页链接 gitbook使用教程: gitbook使用教程 markdwon高阶语法: Markdown进阶（更改字体、颜色、大小，设置文字背景色，调整图片大小设置居中） Cmd Markdown 简明语法手册 EMOJI CHEAT SHEET python官方参考资料: Python 3.8.3 文档 NumPy 参考手册 Pandas: 强大的 Python 数据分析支持库 scikit-learn (sklearn) 官方文档中文版 Matplotlib 教程 https://pyecharts.org/#/ python其他参考资料: Python 标准库 Python 语言参考 Python文档内容 Python 标准库 python索引 PEP索引 python青南-炫技 背景、鼠标特效、2d动漫角色等参考鲸之声demo进行了自定义的修改 Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-12-01 00:57:11 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/30.Anaconda开发环境.html":{"url":"chapters/30.Anaconda开发环境.html","title":"Anaconda开发环境","keywords":"","body":"Anaconda介绍套件管理pip管理食用教程安装虚拟环境管理conda源管理其他管理 Anaconda介绍 Anaconda（官方网站）就是可以便捷获取包且对包能够进行管理，同时对环境可以统一管理的发行版本。Anaconda包含了conda、Python在内的超过180个科学包及其依赖项。 套件管理 显示已安装的套件 conda list pip list 套件安装 conda install packages # pip可以安装一些conda安装不了的包 pip install packages [-i https://pypi.douban.com/simple] # -i部分临时指定pip源 easy_install 参数 packages python setup.py install # 下载源码，进入到源码路径下 国内的pip源 阿里云 https://mirrors.aliyun.com/pypi/simple/ 中国科技大学 https://pypi.mirrors.ustc.edu.cn/simple/ 豆瓣(douban) https://pypi.douban.com/simple/ 清华大学 https://pypi.tuna.tsinghua.edu.cn/simple/ pip install xx -i http://pypi.douban.com/simple/ --trusted-host=pypi.douban.com/simple 从Anaconda.org安装套件 # 如果一个包不能使用conda安装，可在Anaconda.org查找 # 在左上角的叫“Search Anaconda Cloud”搜索框中输入“包名”并点击search按钮 conda install --channel https://conda.anaconda.ort/pandas bottleneck # 安装bottleneck包 删除套件 conda/pip uninstall packages easy_install -m packages 查找套件 conda search packages pip管理 升级 # 试用linux, win下可能会因为权限升级失败 pip install --upgrade pip # win或linux python -m pip install --upgrade pip 如果不小心删掉了pip，可以用以下命令安装 python -m ensurepip --default-pip 食用教程 安装 基础准备(centos7环境) 安装wget命令：yum -y install wget 安装git相关 # 安装git yum install git -y # 安装git lfs（大文件下载） curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.rpm.sh | sudo bash yum install git-lfs git lfs install 安装OpenSSL和libssl-dev yum install openssl yum install openssl-devel 安装gcc：yum install gcc 安装步骤 # 安装anaconda环境 wget https://repo.anaconda.com/archive/Anaconda3-2023.07-2-Linux-x86_64.sh chmod +x Anaconda3-2023.07-2-Linux-x86_64.sh ./Anaconda3-2023.07-2-Linux-x86_64.sh # 创建自己的python环境 conda create -n gpt310 python=3.10 anaconda 虚拟环境管理 使用Anaconda Prompt 管理虚拟环境 新建虚拟环境 添加后缀 “anaconda”或“biopython”可以创建一个基本科学计算功能完备的环境 conda create -n 环境名 python=3.10 anaconda # 如果需要指定虚拟环境的路径, 可以使用--prefix参数 conda create -n 环境名 --prefix='D:\\Program Files\\Anaconda4\\envs\\gpt310' python=3.10 anaconda 切换虚拟环境 # Linux，OS X source activate snowflakes # Windows conda activate snowflakes 关闭虚拟环境 # Linux，OS X source deactivate # Windows deactivate 查看所有的虚拟环境名称 conda info -e conda env list 移除虚拟环境 conda remove -n 虚拟环境名称 --all 复制虚拟环境 conda create -n 虚拟环境名称 --clone root IPython 交谈式命令窗口 cmd中python模式的加强版 观看先前输入的程序代码 history 查询使用说明 ?加在命令后面 简易智能输入 执行python档案 %run 档案路径 更新conda本身 conda update conda 更新anaconda 应用 conda update anaconda 更新python 假设当前python环境是3.6.1，而最新版本是3.6.2，那么就会升级到3.6.2 conda update python conda源管理 查看当前源信息 conda info 关注 channel URLs 字段内容 添加一个镜像 # 添加镜像 (base) C:\\Users\\hyc>conda config --add channels 'https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/' # 设置搜索时显示channel地址 (base) C:\\Users\\hyc>conda config --set show_channel_urls yes 删除一个镜像 # 注意链接要有引号，英文输入法 (base) C:\\Users\\hyc>conda config --remove channels 'https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/' 查看conda的镜像 conda config --show-sources conda config --show channels 删除所有镜像 conda config --remove-key channels 以上操作可以直接编辑.condarc文件完成 ssl_verify: False proxy_servers: http: http://192.168.4.80:3128 https: http://192.168.4.80:3128 channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ - defaults show_channel_urls: True 其他管理 运行环境分享 执行如下命令可以将当前环境下的 package 信息存入名为 environment 的 YAML 文件中 conda env export > environment.yaml 使用yaml文件创建环境 conda env create -f environment.yaml 国内常见源 anaconda conda 切换为国内源 Anaconda 镜像使用帮助 国内可用Anaconda 源的镜像站及换国内源方法 linux可行 conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ conda config --set show_channel_urls yes Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-12-01 00:57:11 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/32.Jupyter_Notebook介绍、安装及使用教程.html":{"url":"chapters/32.Jupyter_Notebook介绍、安装及使用教程.html","title":"Jupyter_Notebook介绍、安装及使用教程","keywords":"","body":"Jupyter Notebook使用教程简介安装与运行编辑界面FileEditViewCellKernelHelp快捷键命令模式下的快捷键编辑模式下的快捷键使用技巧魔法函数md生成目录md链接并定位加载指定网页源代码加载本地Python文件参考 Jupyter Notebook使用教程 简介 Jupyter Notebook是一个开源的Web应用程序，允许用户创建和共享包含代码、方程式、可视化和文本的文档。它的用途包括：数据清理和转换、数值模拟、统计建模、数据可视化、机器学习等等。它具有以下优势： 可选择语言：支持超过40种编程语言，包括Python、R、Julia、Scala等。 分享笔记本：可以使用电子邮件、Dropbox、GitHub和Jupyter Notebook Viewer与他人共享。 交互式输出：代码可以生成丰富的交互式输出，包括HTML、图像、视频、LaTeX等等。 大数据整合：通过Python、R、Scala编程语言使用Apache Spark等大数据框架工具。支持使用pandas、scikit-learn、ggplot2、TensorFlow来探索同一份数据。 Jupyter Notebook是基于网页的用于交互计算的应用程序。 其可被应用于全过程计算：开发、文档编写、运行代码和展示结果。 安装与运行 conda install jupyter notebook python -m pip install jupyter 运行Jupyter Notebook，首先cmd进入到自己的工作路径下，执行以下的命令 jupyter notebook [--port ] 默认端口8888 编辑界面 一个notebook的编辑界面主要由四部分组成：名称、菜单栏、工具条以及单元(Cell)，如下图所示： File File中的按钮选项，具体功能如下表： Edit Edit中的按钮选项如下图所示： View View中的按钮选项如下图所示 View中的功能可以让用户更好的展示自己的notebook，但对编写代码、实现功能没有影响。 Insert在当前单元上方/下方插入新的单元 Cell Cell 按ESC进入无编辑状态，ESC+Y和ESC+M切换cell type Kernel Kernel Help Help 快捷键 命令模式下的快捷键 编辑模式下的快捷键 使用技巧 魔法函数 使用魔法函数可以简单的实现一些单纯python要很麻烦才能实现的功能。 %：行魔法函数，只对本行代码生效。 %%：Cell魔法函数，在整个Cell中生效，必须放于Cell首行。 %lsmagic：列出所有的魔法函数 %magic查看各个魔法函数的说明 ?后面加上魔法函数名称，可以查看该函数的说明 一些常用魔法函数的示例： 注意这些命令是在Python kernel中适用的，其他 kernel 不一定适用。 md生成目录 不同于有道云笔记的Markdown编译器，Jupyter Notebook无法为Markdown文档通过特定语法添加目录，因此需要通过安装扩展来实现目录的添加。 conda install -c conda-forge jupyter_contrib_nbextensions 执行上述命令后，启动Jupyter Notebook，你会发现导航栏多了“Nbextensions”的类目，点击“Nbextensions”，勾选“Table of Contents” Nbextensions页面 之后再在Jupyter Notebook中使用Markdown，点击下图的图标即可使用啦。 md链接并定位 语法格式如下： [添加链接的正文](#自定义索引词) 跳转提示 加载指定网页源代码 %load URL 加载本地Python文件 %load Python文件的绝对路径 运行本地Python文件 %run Python文件的绝对路径 !python Python文件的绝对路径 获取当前位置 %pwd 使用shell命令 !shell命令 在Jupyter Notebook中的笔记本单元格中用英文感叹号“!”后接shell命令即可执行shell命令。 由于目前暂时用不到过多的魔术命令，因此暂时先参考官网的文档。 参考 Jupyter Notebook介绍、安装及使用教程 最详尽使用指南：超快上手Jupyter Notebook Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-12-01 00:57:11 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/15.Pythonic的几个办法.html":{"url":"chapters/15.Pythonic的几个办法.html","title":"Pythonic的几个办法","keywords":"","body":"Pythonic的几个办法确认python版本enumerate迭代列表递推式生成器表达式列表展平字典展平找list的最值索引list去重并保留顺序代码里面调用 pip使用f-Strings格式化字符串集合论使用字符串常量访问公共字符串组使用Itertools生成排列和组合漂亮的打印出JSONwith上下文管理with模块上下文装饰器for/else块合理使用列表序列解包链式比较操作assert用法slots优化内存暂留 Pythonic的几个办法 Python难点解析---高级篇2.Pythonic Python 实用冷门知识整理 这些年来，Python 开发者用Pythonic这个形容词来描述那种符合特定风格的代码。这种Pythonice风格，既不是非常严密的规范，也不是由编译器强加给开发者的规则，而是大家在使用Python语言协同工作的过程中逐渐形成的习惯。 确认python版本 python --version import sys print(sys.version) 3.7.6 (default, Jan 8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)] print(sys.version_info) sys.version_info(major=3, minor=7, micro=6, releaselevel='final', serial=0) 有很多种流行的Python运行时环境，例如，CPython、 Jython、 IronPython 以及PyPy等。 enumerate迭代 enumerate可以把各种迭代器包装为生成器，以便稍后产生输出值。 可以给enumerate提供第二个参数，以指定开始计数时所用的值(默认为0)。 >>> numbers = [45, 22, 14, 65, 97, 72] >>> for i, num in enumerate(numbers): ... if num % 3 == 0 and num % 5 == 0: ... numbers[i] = 'fizzbuzz' ... elif num % 3 == 0: ... numbers[i] = 'fizz' ... elif num % 5 == 0: ... numbers[i] = 'buzz' ... >>> numbers ['fizzbuzz', 22, 14, 'buzz', 97, 'fizz'] # 对于每个元素，enumerate()返回一个计数器和元素值。计数器默认为0，也是元素的索引。不想在0开始你的计数？只需使用可选的start参数来设置偏移量 >>> numbers = [45, 22, 14, 65, 97, 72] >>> for i, num in enumerate(numbers, start=52): ... print(i, num) ... 52 45 53 22 54 14 55 65 56 97 57 72 列表递推式 列表推导是构建列表（list）的快捷方式，而生成器表达式则可以用来创建其他任何类型的序列。 通常的原则是，只用列表推导来创建新的列表，并且尽量保持简短。如果列表推导的代码超过了两行，你可能就要考虑是不是得用 for 循环重写了。 在 Python 3 中都有了自己的局部作用域，就像函数似的。表达式内部的变量和赋值只在局部起作用，表达式的上下文里的同名变量还可以被正常引用，局部变量并不会影响到它们。 列表推导可以帮助我们把一个序列或是其他可迭代类型中的元素过滤或是加工，然后再新建一个列表。 列表推导的作用只有一个：生成列表。如果想生成其他类型的序列，生成器表达式就派上了用场。 >>> numbers = [4, 2, 1, 6, 9, 7] >>> def square(x): ... return x*x ... >>> list(map(square, numbers)) [16, 4, 1, 36, 81, 49] >>> [square(x) for x in numbers] [16, 4, 1, 36, 81, 49] # 使用map()和列表推导的两种方法都返回相同的值，但列表推导更容易阅读和理解。 >>> def is_odd(x): ... return bool(x % 2) ... >>> list(filter(is_odd, numbers)) [1, 9, 7] >>> [x for x in numbers if is_odd(x)] [1, 9, 7] # ，filter和列表推导方法返回相同的值，但列表推导更容易理解。 # 列表推导也支持多个if条件。处在同-循环级别中的多项条件，彼此之间默认形成and表达式。 # 例如，要从数字列表中选出大于4的偶数，那么下面这两种列表推导方式是等效的。 a = [i for i in range(10)] b = [x for x in a if x>4 if x%2==0] c = [x for x in a if x>4 and x%2==0] 生成器表达式 生成器表达式背后遵守了迭代器协议，可以逐个地产出元素，而不是先建立一个完整的列表，然后再把这个列表传递到某个构造函数里。 生成器表达式的语法跟列表推导差不多，只不过把方括号换成圆括号而已。 如果生成器表达式是一个函数调用过程中的唯一参数，那么不需要额外再用括号把它围起来。 生成器表达式就可以帮忙省掉运行 for 循环的开销 # 前面提到，列表推导是方便的工具，但有时会导致不必要的内存使用。 # 错误方式 >>> sum([i * i for i in range(1, 1001)]) 333833500 # 正确方式 >>> sum((i * i for i in range(1, 1001))) 333833500 # 换出括号会将列表推导更改为生成器表达式。当你知道要从序列中检索数据，但不需要同时访问所有数据的时候，生成器表达式非常适合。 # 生成器表达式返回生成器对象，而不是创建列表。该对象知道它在当前状态中的位置（例如，i = 49）并且仅在被要求时计算下一个值。 # 因此，当sum通过重复调用.__ next __()来迭代生成器对象时，生成器检查i等于多少，计算i * i，在内部递增i，并将正确的值返回到sum。该设计允许生成器用于大量数据序列，因为一次只有一个元素存在于内存中。 列表展平 一日一技：如何把多层嵌套的列表展平 def flat(deep_list, result): for element in deep_list: if isinstance(element, list): flat(element, result) else: result.append(element) a = [1, 2, [3, 4, [5, 6, 7], 8], 9, [10, 11]] result = [] flat(a, result) print(result) def list_flat(deep_list, ignore_types=(str, bytes)) -> List: \"\"\" 列表嵌套展平为一维列表 @param deep_list: 嵌套列表 @param ignore_types: 不做展平的类型 @rtype: 一维列表 \"\"\" for element in deep_list: if isinstance(element, list) and not isinstance(element, ignore_types): yield from list_flat(element, ignore_types=ignore_types) else: yield element a = [1, 2, [3, 4, [5, 6, 7], 8], 9, [10, 11]] result = [x for x in list_flat(a)] print(result) 所以，当代码运行到 [x for x in flat(a)] 的时候，每一次循环都会进入到 flat生成器里面。在 flat里面，对传入的参数使用for循环进行迭代，如果拿到的元素不是列表，那么就直接抛出，送到上一层。如果当前已经是最上层了，那么就再一次抛出给外面的列表推导式。如果当前元素是列表，那么继续生成一个生成器，并对这个新的生成器进行迭代，并把每一个结果继续往上层抛出。 最终，每一个数字都会被一层一层往上抛出给列表推导式，从而获得需要的结果。 字典展平 nest_dict = { 'a': 1, 'b': { 'c': 2, 'd': 3, 'e': {'f': 4} }, 'g': {'h': 5}, 'i': 6, 'j': {'k': {'l': {'m': 8}}} } 使用yield关键字来实现这个需求，在不炫技的情况下，只需要8行代码。在炫技的情况下，只需要3行代码。 要快速地把这个嵌套字典压扁，我们需要从下向上来处理字段。例如对于b->e->f->4这条路径，我们首先把最里面的{'f': 4}转换为一个元组('f', 4)。然后，把这个元组向上抛出，于是得到了元组('e', ('f', 4))。我们把 e拼接到f的前面，变为：('e_f', 4)，继续往上抛出，得到('b', ('e_f', 4))。再把b拼接到e_f上面，得到('b_e_f', 4)。完成一条线路的组装。 这个逻辑如果使用yield关键字来实现，就是： def map_flat(deep_map, full_key: bool = True) -> Dict: \"\"\" 字典嵌套展平为一维字典 @param deep_map: 嵌套字典 @param full_key: 是否使用完整的key @rtype: 一维字典 \"\"\" for key, value in deep_map.items(): if isinstance(value, dict): for k, v in map_flat(value, full_key): yield (f'{key}_{k}', v) if full_key else (k, v) else: yield key, value {k:v for k,v in map_flat(nest_dict)} { 'a': 1, 'b_c': 2, 'b_d': 3, 'b_e_f': 4, 'g_h': 5, 'i': 6, 'j_k_l_m': 8 } 通过使用 yield关键字，字典的key会像是在流水线上一样，一层一层从内向外进行组装，从而形成完整的路径。 找list的最值索引 def max_idx(lst): return max(range(len(lst)), key=lst.__getitem__) max_idx([1,5,7,2,3]) Out[62]: 2 list去重并保留顺序 from collections import OrderedDict a = ['heelo','world','world','a','hello'] list(OrderedDict.fromkeys(a).keys()) Out[65]: ['heelo', 'world', 'a', 'hello'] a = ['hello','world','world','a','hello'] list(OrderedDict.fromkeys(a).keys()) Out[67]: ['hello', 'world', 'a'] 代码里面调用 pip 说到安装 Python 的第三方库，会 Python 的同学都知道，在终端使用pip install xxx即可。 那么如果我想在代码里面安装第三方库怎么办呢？可能有人想到使用 os 模块： import os package_name = 'requests' os.system(f'pip install {package_name}') 这种方法确实可行，并且即使你在虚拟环境中使用这种方式安装，也确实不会安装到系统的 Python 环境中。 但是这种方式总感觉有点奇怪。而且如果这个package_name字符串经过精心构造，可以执行任意系统命令，例如： import os package_name = 'requests && rm -rf *' os.system(f'pip install {package_name}') 为了防止这种情况发生，我们可以直接调用pip这个 Python 包： from pip._internal import main main.main(['install', '第三方库名']) 命令行下面的参数都可以通过转换为列表的形式执行，例如： from pip._internal import main main.main(['install', '-r', 'requirements.txt']) 使用f-Strings格式化字符串 # f-strings支持使用字符串格式化迷你语言，以及强大的字符串插值。这些功能允许你添加变量甚至有效的Python表达式，并在添加到字符串之前在运行时对它们进行评估： >>> def get_name_and_decades(name, age): ... return f\"My name is {name} and I'm {age / 10:.5f} decades old.\" ... >>> get_name_and_decades(\"Maria\", 31) My name is Maria and I'm 3.10000 decades old. 集合论 集合的本质是许多唯一对象的聚集。因此，集合可以用于去重 集合中的元素必须是可散列的，set 类型本身是不可散列的，但是 frozenset 可以。因此可以创建一个包含不同 frozenset 的 set。 给定两个集合 a 和 b，a | b 返回的是它们的合集，a & b 得到的是交集，而 a - b 得到的是差集。 除空集之外，集合的字面量——{1}、{1, 2}，等等——看起来跟它的数学形式一模一样。如果是空集，那么必须写成 set() 的形式。 在 Python 3 里面，除了空集，集合的字符串表示形式总是以 {...} 的形式出现。 像 {1, 2, 3} 这种字面量句法相比于构造方法（set([1, 2, 3])）要更快且更易读。后者的速度要慢一些，因为 Python 必须先从 set 这个名字来查询构造方法，然后新建一个列表，最后再把这个列表传入到构造方法里。但是如果是像 {1, 2, 3} 这样的字面量，Python 会利用一个专门的叫作 BUILD_SET 的字节码来创建集合。 数学符号 python运算符 方法 描述 set的实现以及导致的结果 set 和 frozenset 的实现也依赖散列表，但在它们的散列表里存放的只有元素的引用（就像在字典里只存放键而没有相应的值）。在 set 加入到 Python 之前，我们都是把字典加上无意义的值当作集合来用的。 这些特点总结如下。 集合里的元素必须是可散列的。 集合很消耗内存。 可以很高效地判断元素是否存在于某个集合。 元素的次序取决于被添加到集合里的次序。 往集合里添加元素，可能会改变集合里已有元素的次序。 >>> import random >>> all_words = \"all the words in the world\".split() >>> def get_random_word(): ... return random.choice(all_words) >>> def get_unique_words(): ... words = set() ... for _ in range(1000): ... words.add(get_random_word()) ... return words >>> get_unique_words() {'world', 'all', 'the', 'words'} 使用字符串常量访问公共字符串组 # 可以使用is_upper()，它返回字符串中的所有字符是否都是大写字母： >>> import string >>> def is_upper(word): ... for letter in word: ... if letter not in string.ascii_uppercase: ... return False ... return True ... >>> is_upper('Thanks Geir') False >>> is_upper('LOL') True # is_upper()迭代word中的字母，并检查字母是否为string.ascii_大写字母的一部分。如果你打印出string.ascii_大写，你会发现它只是一个字符串，该值设置为文本“ABCDEFGHIJKLMNOPQRSTUVWXYZ”。 所有字符串常量都只是经常引用的字符串值的字符串。其中包括以下内容： string.ascii_letters string.ascii_uppercase string.ascii_lowercase string.digits string.hexdigits string.octdigits string.punctuation string.printable string.whitespace 使用Itertools生成排列和组合 # itertools.permutations()构建所有排列的列表，这意味着它是输入值的每个可能分组的列表，其长度与count参数匹配。r关键字参数允许我们指定每个分组中有多少值： >>> import itertools >>> friends = ['Monique', 'Ashish', 'Devon', 'Bernie'] >>> list(itertools.permutations(friends, r=2)) [('Monique', 'Ashish'), ('Monique', 'Devon'), ('Monique', 'Bernie'), ('Ashish', 'Monique'), ('Ashish', 'Devon'), ('Ashish', 'Bernie'), ('Devon', 'Monique'), ('Devon', 'Ashish'), ('Devon', 'Bernie'), ('Bernie', 'Monique'), ('Bernie', 'Ashish'), ('Bernie', 'Devon')] # itertools.combinations()生成组合。这些也是输入值的可能分组，但现在值的顺序无关紧要。 >>> list(itertools.combinations(friends, r=2)) [('Monique', 'Ashish'), ('Monique', 'Devon'), ('Monique', 'Bernie'), ('Ashish', 'Devon'), ('Ashish', 'Bernie'), ('Devon', 'Bernie')] 漂亮的打印出JSON >>> import json >>> print(json.dumps(data)) # No indention {\"status\": \"OK\", \"count\": 2, \"results\": [{\"age\": 27, \"name\": \"Oz\", \"lactose_intolerant\": true}, {\"age\": 29, \"name\": \"Joe\", \"lactose_intolerant\": false}]} >>> print(json.dumps(data, indent=2)) # With indention { \"status\": \"OK\", \"count\": 2, \"results\": [ { \"age\": 27, \"name\": \"Oz\", \"lactose_intolerant\": true }, { \"age\": 29, \"name\": \"Joe\", \"lactose_intolerant\": false } ] } with上下文管理 with模块 with 语句会设置一个临时的上下文，交给上下文管理器对象控制，并且负责清理上下文。这么做能避免错误并减少样板代码，因此 API 更安全，而且更易于使用。 上下文管理器对象存在的目的是管理 with 语句，就像迭代器的存在是为了管理 for 语句一样。 with 语句的目的是简化 try/finally 模式。finally 子句中的代码通常用于释放重要的资源，或者还原临时变更的状态。 上下文管理器协议包含 __enter__ 和 __exit__ 两个方法。 with 语句开始运行时，会在上下文管理器对象上调用__enter__ 方法。 with 语句运行结束后，会在上下文管理器对象上调用__exit__方法，以此扮演 finally 子句的角色。 with的行为 执行 with 后面的表达式得到的结果是上下文管理器对象，不过，把值绑定到目标变量上（as 子句）是在上下文管理器对象上调用 __enter__ 方法的结果。 __enter__ 方法除了返回上下文管理器之外，还可能返回其他对象。 不管控制流程以哪种方式退出 with 块，都会在上下文管理器对象上调用 __exit__ 方法，而不是在 __enter__ 方法返回的对象上调用。 with 语句的 as 子句是可选的。对 open 函数来说，必须加上 as 子句，以便获取文件的引用。 如果 `__exit__` 方法返回 `None`，或者 `True` 之外的值，`with` 块中的任何异常都会向上冒泡。 解释器调用 __enter__ 方法时，除了隐式的 self 之外，不会传入任何参数。传给 __exit__ 方法的三个参数列举如下。 exc_type 　　异常类（例如 ZeroDivisionError）。 exc_value 　　异常实例。有时会有参数传给异常构造方法，例如错误消息，这些参数可以使用 exc_value.args 获取。 traceback 　　traceback 对象。 可以手动调用 `__enter__` 和 `__exit__` 方法。 with 不仅能管理资源，还能用于去掉常规的设置和清理代码，或者在另一个过程前后执行的操作 with allocate_resource() as resource: resource.use() # 打开/关闭文件 with open('data.txt') as f: data = f.read() 上下文装饰器 contextlib 模块中的实用工具 closing: 如果对象提供了 close() 方法，但没有实现 __enter__/__exit__ 协议，那么可以使用这个函数构建上下文管理器。 suppress: 构建临时忽略指定异常的上下文管理器。 @contextmanager: 这个装饰器把简单的生成器函数变成上下文管理器，这样就不用创建类去实现管理器协议了。 ContextDecorator: 这是个基类，用于定义基于类的上下文管理器。这种上下文管理器也能用于装饰函数，在受管理的上下文中运行整个函数。 ExitStack: 这个上下文管理器能进入多个上下文管理器。with 块结束时，ExitStack 按照后进先出的顺序调用栈中各个上下文管理器的 __exit__ 方法。如果事先不知道 with 块要进入多少个上下文管理器，可以使用这个类。例如，同时打开任意一个文件列表中的所有文件。 显然，在这些实用工具中，使用最广泛的是 @contextmanager 装饰器，因此要格外留心。 这个装饰器也有迷惑人的一面，因为它与迭代无关，却要使用 yield 语句。 重点介绍下**@contextmanager** @contextmanager 装饰器能减少创建上下文管理器的样板代码量，因为不用编写一个完整的类，定义 __enter__ 和 __exit__ 方法，而只需实现有一个 yield 语句的生成器，生成想让 __enter__ 方法返回的值。 在使用 @contextmanager 装饰的生成器中，yield 语句的作用是把函数的定义体分成两部分：yield 语句前面的所有代码在 with 块开始时（即解释器调用 __enter__ 方法时）执行， yield 语句后面的代码在 with 块结束时（即调用 __exit__ 方法时）执行。 import contextlib @contextlib.contextmanager ➊ def looking_glass(): import sys original_write = sys.stdout.write ➋ def reverse_write(text): ➌ original_write(text[::-1]) sys.stdout.write = reverse_write ➍ yield 'JABBERWOCKY' ➎ sys.stdout.write = original_write ➏ # 这里相当于__exit__ 这个类的 __enter__ 方法有如下作用。 (1) 调用生成器函数，保存生成器对象（这里把它称为 gen）。 (2) 调用 next(gen)，执行到 yield 关键字所在的位置。 (3) 返回 next(gen) 产出的值，以便把产出的值绑定到 with/as 语句中的目标变量上。 with 块终止时，`__exit__` 方法会做以下几件事： 检查有没有把异常传给 exc_type；如果有，调用 gen.throw(exception)，在生成器函数定义体中包含 yield 关键字的那一行抛出异常。 否则，调用 next(gen)，继续执行生成器函数定义体中 yield 语句之后的代码。 上文示例有一个严重的错误：如果在 with 块中抛出了异常，Python 解释器会将其捕获，然后在 looking_glass 函数的 yield 表达式里再次抛出。 但是，那里没有处理错误的代码，因此 looking_glass 函数会中止，永远无法恢复成原来的 sys.stdout.write 方法，导致系统处于无效状态。 以下代码是基于生成器的上下文管理器，而且实现了异常处理——从外部看，行为与前文一样 import contextlib @contextlib.contextmanager def looking_glass(): import sys original_write = sys.stdout.write def reverse_write(text): original_write(text[::-1]) sys.stdout.write = reverse_write msg = '' ➊ try: yield 'JABBERWOCKY' except ZeroDivisionError: ➋ msg = 'Please DO NOT divide by zero!' finally: sys.stdout.write = original_write ➌ if msg: print(msg) ➍ 前面说过，为了告诉解释器异常已经处理了，__exit__ 方法会返回 True，此时解释器会压制异常。如果 __exit__ 方法没有显式返回一个值，那么解释器得到的是 None，然后向上冒泡异常。使用 @contextmanager 装饰器时，默认的行为是相反的：装饰器提供的__exit__ 方法假定发给生成器的所有异常都得到处理了，因此应该压制异常。6 如果不想让 @contextmanager 压制异常，必须在被装饰的函数中显式重新抛出异常。 在 @contextmanager 装饰器装饰的生成器中，yield 与迭代没有任何关系。 @contextmanager 装饰器能把包含一个 yield 语句的简单生成器变成上下文管理器——这比定义一个至少包含两个方法的类要更简洁。 for/else块 `else` 子句不仅能在 if 语句中使用，还能在 for、while 和 try 语句中使用。 `for/else`、`while/else` 和 `try/else` 的语义关系紧密，不过与 `if/else` 差别很大。 在循环中，else 的语义恰好相反：“运行这个循环，然后做那件事。” else 子句的行为如下。 for: 仅当 for 循环运行完毕时（即 for 循环没有被 break 语句中止）才运行 else 块。 while: 仅当 while 循环因为条件为假值而退出时（即 while 循环没有被 break 语句中止）才运行 else 块。 try: 仅当 try 块中没有异常抛出时才运行 else 块。官方文档还指出：“else 子句抛出的异常不会由前面的 except 子句处理。” 在所有情况下，如果异常或者 return、break 或 continue 语句导致控制权跳到了复合语句的主块之外，else 子句也会被跳过。 在这些语句中使用 else 子句通常能让代码更易于阅读，而且能省去一些麻烦，不用设置控制标志或者添加额外的 if 语句。 # for else 是 Python 中特有的语法格式，else 中的代码在 for 循环遍历完所有元素之后执行 # 如果for循环正常结束，else中语句执行。如果是break的，则不执行。 for i in mylist: if i == theflag: break process(i) else: raise ValueError(\"List argument missing terminal flag.\") # 方式一 flag = False for x in xx: if some condition: flag = True break if flag: print 'no break' # 方式二 for x in xx: if some condition: break else: print 'no break' # 判断质数/素数——我知道的最快的方法 # https://blog.csdn.net/songyunli1111/article/details/78690447 # Example def is_prime(n = 20): is_p = False for ii in range(2 if n>=2 else 1, int(math.sqrt(n))+1): if n%ii == 0: break else: is_p = True return is_p # 或者直接写成 def is_prime(n = 20): for ii in range(2 if n>=2 else 1, int(math.sqrt(n))+1): if n%ii == 0: return False else: return True # 获取n=50以内的素数列表 import math n = 50 data = [ii for ii in range(n)] res = itertools.compress(data,[is_prime(da) for da in data]) print([r for r in res]) Out[30]: [0, 2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47] 合理使用列表 from collections import deque names = deque(['raymond', 'rachel', 'matthew', 'roger', 'betty', 'melissa', 'judith', 'charlie']) names.popleft() Out[10]: 'raymond' 列表对象（list）是一个查询效率高于更新操作的数据结构，删除和插入需要对剩下的元素做移动操作 deque 是一个双向队列的数据结构，删除元素和插入元素会很快 序列解包 元组拆包可以应用到任何可迭代对象上，唯一的硬性要求是，被可迭代对象中的元素数量必须要跟接受这些元素的元组的空档数一致。除非我们用 * 来表示忽略多余的元素。 os.path.split() 函数就会返回以路径和最后一个文件名组成的元组 (path, last_part) 在平行赋值中，* 前缀只能用在一个变量名前面，但是这个变量可以出现在赋值表达式的任意位置 p = 'vttalk', 'female', 30, 'python@qq.com' name, gender, age, email = p name, gender, age, email Out[13]: ('vttalk', 'female', 30, 'python@qq.com') num_list = [100, 19, 20, 98] first, *left_num_list, last = num_list print(first, left_num_list, last) Out[14]: 100 [19, 20] 98 string = 'xuexiao 4 fuzhou daxue' tag,start_index,value = string.split(' ',2) 链式比较操作 if 18 assert用法 >>> assert mul(2, 3) == 7, 'This statement is wrong!!!!!!' Traceback (most recent call last): File \"\", line 1, in AssertionError: This statement is wrong!!!!!! slots优化内存 使用slots使用了100M内存，比使用dict存储属性值节省了2倍。 其实使用collection模块的namedtuple也可以实现slots相同的功能。namedtuple其实就是继承自tuple，同时也因为slots的值被设置成了一个空tuple以避免创建dict collection 和普通创建类方式相比，也节省了不少的内存。所在在确定类的属性值固定的情况下，可以使用slots方式对内存进行优化。但是这项技术不应该被滥用于静态类或者其他类似场合，那不是python程序的精神所在。 # 未使用__slots__ # -*- coding: utf-8 -* from memory_profiler import profile class Foobar(object): # __slots__ = ('x') def __init__(self, x): self.x = x @profile def main(): f = [Foobar(42) for i in range(1000000)] if __name__ == \"__main__\": main() Line # Mem usage Increment Line Contents ================================================ 137 45.7 MiB 45.7 MiB @profile 138 def main(): 139 215.9 MiB 0.9 MiB f = [Foobar(42) for i in range(1000000)] # 使用__slots__ # -*- coding: utf-8 -* from memory_profiler import profile class Foobar(object): __slots__ = ('x') def __init__(self, x): self.x = x @profile def main(): f = [Foobar(42) for i in range(1000000)] if __name__ == \"__main__\": main() Line # Mem usage Increment Line Contents ================================================ 132 45.7 MiB 45.7 MiB @profile 133 def main(): 134 99.8 MiB 0.4 MiB f = [Foobar(42) for i in range(1000000)] # 使用__slots__要注意，__slots__定义的属性仅对当前类起作用，对继承的子类是不起作用的 # 除非在子类中也定义__slots__，这样，子类允许定义的属性就是自身的__slots__加上父类的__slots__。 暂留 Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-12-01 00:57:11 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/19.Python增强提案PEP.html":{"url":"chapters/19.Python增强提案PEP.html","title":"Python增强提案PEP","keywords":"","body":"Python 增强提案PEPPEP 557 数据类（data class） Python 增强提案PEP Python中10个必读的PEP提案 理解Python数据类：Dataclass 的特征概述 （上） 理解Python数据类：Dataclass fields 的概述（下） Python3.7 dataclass使用指南小结 ​ PEP 是 Python 增强提案(Python Enhancement Proposal)的缩写。社区通过PEP来给 Python 语言建言献策，每个版本你所看到的新特性和一些变化都是通过PEP提案经过社区决策层讨论、投票决议的。 PEP 557 数据类（data class） dataclass的定义位于PEP-557，根据定义一个dataclass是指“一个带有默认值的可变的namedtuple”，广义的定义就是有一个类，它的属性均可公开访问，可以带有默认值并能被修改，而且类中含有与这些属性相关的类方法，那么这个类就可以称为dataclass，再通俗点讲，dataclass就是一个含有数据及操作数据方法的容器。 Dataclasses 是一些适合于存储数据对象(data object)的 Python 类。 下面是一个并不详尽的用于定义数据对象的特征列表： • 他们存储并表示特定的数据类型。例如：一个数字。对于那些熟悉对象关系映射(Object Relational Mapping，简称 ORM)的人来说，一个模型实例就是一个数据对象。它表示了一种特定类型的实体。它存储了用于定义或表示那种实体的属性。 • 他们能够被用于和同类型的其他对象进行比较。例如，一个数字可能大于，小于或等于另一个数字。 • 当然数据对象还有更多的特征，但上述内容足以帮助你理解关键部分。 • 为了理解 Dataclases，我们将实现一个简单的类。它能够存储一个数字，并允许我们执行上面提到的各种运算。 1.相比普通class，dataclass通常不包含私有属性，数据可以直接访问 2.dataclass的repr方法通常有固定格式，会打印出类型名以及属性名和它的值 3.dataclass拥有__eq__和__hash__魔法方法 4.dataclass有着模式单一固定的构造方式，或是需要重载运算符，而普通class通常无需这些工作 # -*- coding: utf-8 -* import random from typing import List from dataclasses import dataclass from dataclasses import field # dataclasses.field 接受了一个名为 default_factory 的参数，它的作用是：如果在创建对象时没有赋值，则使用该方法初始化该字段。 # default_factory 必须是一个可以调用的无参数方法(通常为一个函数) def get_random_marks(): return [random.randint(1, 10) for _ in range(5)] @dataclass(order=True) class Student: marks: List[int] = field(default_factory=get_random_marks) name: str = field(default='noname', compare=False) age: int = field(default=18, repr=False) # 在追踪一个对象的状态，并且希望它在初始化时一直被设为False # 更一般地，这个值在初始化时不能够被传递,init决定是否生成init verified: bool = field(repr=False, init=False, default=False) # 不好的方式：自定义比较方法 # 正确方式： # dataclass能够自动生成 , = 这些比较方法。但是这些比较方法的一个缺陷是， # 它们使用类中的所有字段进行比较, 并且是按定义顺序 # 这里使用(order=True)，把不需要的字段定义为 filed(compare=False) # def __eq__(self, other): # return (self.marks, self.name) == (other.marks, other.name) # 不好的方法：初始化一个变量为列表, 使用__post_init__方法, 或参数传递 # 正确方式是使用field # def __post_init__(self): # self.marks = get_random_marks() # 不要自己定义，会自动生成的__repr__方法使用所有的字段用于表示 # 不需要的元素使用field(repr=False)过滤 # def __repr__(self): # return self.name+' '+' '.join(self.marks) student_1 = Student([random.randint(1, 10) for _ in range(5)], 'tom', 17) student_2 = Student(age=16, name='Rick') # 这里会报异常 # student_3 = Student(verified=True) print(student_1) print(f'student_1 == student_2: {student_1 == student_2}') # 使用dataclasses.asdict和dataclasses.astuple我们可以把数据类实例中的数据转换成字典或者元组： from dataclasses import asdict, astuple print(asdict(student_2)) # 使用dataclasses.is_dataclass可以判断一个类或实例对象是否是数据类： print(f'isinstance(student_1, Student): {isinstance(student_1, Student)}') >>> Student(marks=[7, 6, 7, 2, 3], name='tom') student_1 == student_2: False {'marks': [6, 4, 3, 5, 1], 'name': 'Rick', 'age': 16, 'verified': False} isinstance(student_1, Student): True 装饰器的原型如下： dataclasses.dataclass(*, init=True, repr=True, eq=True, order=False, unsafe_hash=False, frozen=False) dataclass装饰器将根据类属性生成数据类和数据类需要的方法 key 含义 init 是否自动生成init，如果已经有定义同名方法则忽略这个值，也就是指定为True也不会自动生成 repr 是否自动生成repr；自动生成的打印格式为class_name(arrt1:value1, attr2:value2, ...) eq 是否生成eq；按属性在类内定义时的顺序逐个比较，全部的值相同才会返回True order 自动生成lt，le，gt，ge，比较方式与eq相同；如果order指定为True而eq指定为False，将引发ValueError；如果已经定义同名函数，将引发TypeError unsafehash 如果是False，将根据eq和frozen参数来生成hash:1. eq和frozen都为True，hash将会生成2. eq为True而frozen为False，hash被设为None3. eq为False，frozen为True，hash将使用超类（object）的同名属性（通常就是基于对象id的hash）当设置为True时将会根据类属性自动生成hash，然而这是不安全的，因为这些属性是默认可变的，这会导致hash的不一致，所以除非能保证对象属性不可随意改变，否则应该谨慎地设置该参数为True frozen 设为True时对field赋值将会引发错误，对象将是不可变的，如果已经定义了setattr和delattr将会引发TypeError field的原型： dataclasses.field(*, default=MISSING, default_factory=MISSING, repr=True, hash=None, init=True, compare=True, metadata=None) 通常我们无需直接使用，装饰器会根据我们给出的类型注解自动生成field，但有时候我们也需要定制这一过程，这时dataclasses.field就显得格外有用了。 default和default_factory参数将会影响默认值的产生，它们的默认值都是None，意思是调用时如果为指定则产生一个为None的值。其中default是field的默认值，而default_factory控制如何产生值，它接收一个无参数或者全是默认参数的callable对象，然后用调用这个对象获得field的初始值，之后再将default（如果值不是MISSING）复制给callable返回的这个对象。 init参数如果设置为False，表示不为这个field生成初始化操作，dataclass提供了hook—— __post_init__供我们利用这一特性： repr参数表示该field是否被包含进repr的输出，compare和hash参数表示field是否参与比较和计算hash值。metadata不被dataclass自身使用，通常让第三方组件从中获取某些元信息时才使用，所以我们不需要使用这一参数。 如果指定一个field的类型注解为dataclasses.InitVar，那么这个field将只会在初始化过程中（__init__和__post_init__）可以被使用，当初始化完成后访问该field会返回一个dataclasses.Field对象而不是field原本的值，也就是该field不再是一个可访问的数据对象。举个例子，比如一个由数据库对象，它只需要在初始化的过程中被访问： @dataclass class C: i: int j: int = None database: InitVar[DatabaseType] = None def __post_init__(self, database): if self.j is None and database is not None: self.j = database.lookup('j') c = C(10, database=my_database) dataclass继承 python3.7引入dataclass的一大原因就在于相比namedtuple，dataclass可以享受继承带来的便利。 dataclass装饰器会检查当前class的所有基类，如果发现一个dataclass，就会把它的字段按顺序添加进当前的class，随后再处理当前class的field。所有生成的方法也将按照这一过程处理，因此如果子类中的field与基类同名，那么子类将会无条件覆盖基类。子类将会根据所有的field重新生成一个构造函数，并在其中初始化基类。 几点注意事项： dataclass通常情况下是unhashable的，因为默认生成的hash是None，所以不能用来做字典的key，如果有这种需求，那么应该指定你的数据类为frozen dataclass 小心当你定义了和dataclass生成的同名方法时会引发的问题 当使用可变类型（如list）时，应该考虑使用field的default_factory 数据类的属性都是公开的，如果你有属性只需要初始化时使用而不需要在其他时候被访问，请使用dataclasses.InitVar Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-12-01 00:57:11 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/11.Regular_Expression_OP.html":{"url":"chapters/11.Regular_Expression_OP.html","title":"Regular_Expression_OP","summary":"python正则表达式的学习记录","keywords":"","body":"正则表达式概念正则元字符内容匹配个数匹配位置匹配分组匹配转义匹配基本使用预编译匹配模式忽略大小写re.I字符集本地化re.L多行模式re.M所有字符re.S冗余模式re.XUNICODE解析re.Ure.Match类匹配matchsearch检索findallfinditer替换和分割sub替换subn替换split分割正则缓存进阶用法非贪婪匹配分组匹配(捕获组)转义匹配替代函数re增强的包装 正则表达式概念 python正则表达式 深入理解正则表达式环视的概念与用法 资源 | 正则表达式的功法大全 在线正则表达式验证网站 概念定义 使用单个字符串来描述匹配某个句法规则的字符串，是对字符串操作的一种逻辑公式 应用场景 处理文本和数据，提高复杂文本分析的效率 正则表达式过程 依次拿出表达式和文本中的字符比较，如果每一个字符都能匹配，则匹配成功；否则匹配失败 # -*- coding: utf-8 -* import re from IPython.core.interactiveshell import InteractiveShell InteractiveShell.ast_node_interactivity = \"all\" # 可同时输出多个结果 pattern_s = r'imooc' # 定义正则表达式 pattern_r = re.compile(pattern_s) # 编译正则表达式 str1 = 'imooc book' # 需要查找的原始字符串 match_r = pattern_r.match(str1) print(f'match_r.group(): {match_r.group()}') match_r.group(): imooc 正则元字符 内容匹配 正则表达式 代表的匹配字符 . 匹配任意字符（不包括换行符） [0-9] 0-9的数字 [a-z] 小写字母 [A-Z] 大写字母 \\d 匹配数字，等同于[0-9] \\D 匹配非数字，等同于[\\^0-9] \\w 匹配大小写字母、数字和下划线，等同于[a-z0-9A-Z_] \\W 匹配非大小写字母、数字和下划线，等同于[\\^a-z0-9A-Z_] \\s 匹配空白 \\S 匹配非空白 \\u4e00-\\u9fff 匹配中文字符 个数匹配 正则表达式 代表的匹配字符 * 匹配前面的字符或者子表达式0次或多次 + 匹配前一个字符或子表达式一次或多次 ? 匹配前一个字符或子表达式0次或1次重复 {n} 匹配前一个字符或子表达式n次 {m,n} 匹配前一个字符或子表达式m至n次 {n,} 匹配前一个字符或者子表达式至少n次 *? / +? / ?? 惰性匹配上一个 位置匹配 正则表达式 代表的匹配字符 ^ 匹配字符串开头, 多行模式下匹配每一行的开始 $ 匹配字符串结尾, 多行模式下匹配每一行的结束 \\A / \\Z 指定字符串必须出现在开头/结尾 \\b 匹配位于单词开始或结束位置的空字符串 \\B 匹配不位于单词开始或结束位置的空字符串 分组匹配 正则表达式 代表的匹配字符 | 匹配左右任意一个表达式 (ab) 括号里的表达式作为一个分组 \\ 引用编号为num的分组匹配到的字符串 (?P) 分组起别名 (?P=name) 引用别名为name的分组匹配字符串 [ ] 可匹配其中任意一个字符 转义匹配 正则表达式 代表的匹配字符 \\ 转义字符，如\\.只能匹配.，不能再匹配任意字符 基本使用 预编译 compile 函数用于编译正则表达式，生成一个正则表达式(Pattern)对象，供 match() 和 search() 这两个函数使用 如果重复多次地使用正则表达式，最好是使用compile函数把正则表达式编译成对象re.Pattern，这样会大大地提高搜索的效率 re.compile(pattern, flags=0) 属性说明 flags：编译时指定的模式 groupindex：以正则表达式中有别名的组的别名为键、以该组对应的编号为值的字典，没有别名的组不包含在内。 groups：正则表达式中分组的数量 pattern：编译时用的正则表达式 方法说明 findall、finditer、match、search、split、sub、subn 等函数 例子 # -*- coding: utf-8 -* import re s = 'Hello, Mr.Gumby : 2016/10/26' pat = '''(?: # 构造一个不捕获分组 用于使用 | (?P\\w+\\.\\w+) # 匹配 Mr.Gumby | # 或 (?P\\s+\\.\\w+) # 一个匹配不到的命名分组 ) .*? # 匹配 : (\\d+) # 匹配 2016 ''' p = re.compile(pat, re.X) print(f'p.flags: {p.flags}') print(f'p.groupindex: {p.groupindex}') print(f'p.groups: {p.groups}') print(f'p.pattern: {p.pattern}') p.flags: 96 p.groupindex: {'name': 1, 'no': 2} p.groups: 3 p.pattern: (?: # 构造一个不捕获分组 用于使用 | (?P\\w+\\.\\w+) # 匹配 Mr.Gumby | # 或 (?P\\s+\\.\\w+) # 一个匹配不到的命名分组 ) .*? # 匹配 : (\\d+) # 匹配 2016 匹配模式 Python正则表达式，请不要再用re.compile了 在正则表达式中，采用预编译的优势就是可以节约时间，提高效率 re.compile(pattern, flags=0)给定一个正则表达式 pattern 指定使用的模式 flags 默认为0 即不使用任何模式，然后会返回一个 SRE_Pattern对象 参数说明 pattern: 一个字符串形式的正则表达式 flags: 可选，表示匹配模式，比如忽略大小写，多行模式等，使用按位或运算符 | 同时添加多个模式,具体参数为： re.I：数值2，忽略大小写 re.L：数值4，表示特殊字符集 \\w, \\W, \\b, \\B, \\s, \\S 依赖于当前环境 re.M：数值8，多行模式 re.S：数值16，即为 . 并且包括换行符在内的任意字符（. 不包括换行符） re.U：数值32，表示特殊字符集 \\w, \\W, \\b, \\B, \\d, \\D, \\s, \\S 依赖于 Unicode 字符属性数据库 re.X：数值64，为了增加可读性，忽略空格和 # 后面的注释 忽略大小写re.I # I: IGNORECASE， 忽略大小写的匹配模式 # -*- coding: utf-8 -* import re str1 = 'hello World!' pattern_s = \"hello world!\" pattern_r = re.compile(pattern_s, re.I) print(f'pattern_r.match(str1).group(): {pattern_r.match(str1).group()}') # 或在正则表达式中指定模式以及注释 pattern_s = \"(?#注释)(?i)hello world!\" pattern_r = re.compile(pattern_s) print(f'pattern_r.match(str1).group(): {pattern_r.match(str1).group()}') pattern_r.match(str1).group(): hello World! pattern_r.match(str1).group(): hello World! 字符集本地化re.L # L: LOCALE， 字符集本地化。这个功能是为了支持多语言版本的字符集使用环境的，比如在转义符\\w，在英文环境下，它代表[a-zA-Z0-9_]，即所以英文字符和数字。 # 如果在一个法语环境下使用，缺省设置下，不能匹配\"é\" 或\"ç\"。加上这L选项和就可以匹配了。不过这个对于中文环境似乎没有什么用，它仍然不能匹配中文字符。 多行模式re.M # M: MULTILINE，多行模式, 改变 ^ 和 $ 的行为 # -*- coding: utf-8 -* import re str1 = '''first line1 second line2 third line3''' # ^ regex_start = re.compile(\"^\\w+\") print(f'regex_start.findall(str1): {regex_start.findall(str1)}') regex_start_m = re.compile(\"^\\w+\", re.M) print(f'regex_start_m.findall(str1): {regex_start_m.findall(str1)}') # $ regex_end = re.compile(\"\\w+$\") print(f'regex_end.findall(str1): {regex_end.findall(str1)}') regex_end_m = re.compile(\"\\w+$\", re.M) print(f'regex_end_m.findall(str1): {regex_end_m.findall(str1)}') regex_start.findall(str1): ['first'] regex_start_m.findall(str1): ['first', 'second', 'third'] regex_end.findall(str1): ['line3'] regex_end_m.findall(str1): ['line1', 'line2', 'line3'] 所有字符re.S # S: DOTALL，此模式下 '.' 的匹配不受限制，可匹配任何字符，包括换行符 # -*- coding: utf-8 -* import re str2 = '''first line1 second line2 third line3''' # 此时.无法匹配换行符 regex = re.compile(\".+\") print(f'regex.findall(str2): {regex.findall(str2)}') # 此时.匹配换行符 regex_dotall = re.compile(\".+\", re.S) print(f'regex_dotall.findall(str2): {regex_dotall.findall(str2)}') regex.findall(str2): ['first line1', 'second line2', 'third line3'] regex_dotall.findall(str2): ['first line1\\nsecond line2\\nthird line3'] 冗余模式re.X # X: VERBOSE，冗余模式， 忽略正则表达式中的空白和#号注释，如写一个匹配邮箱的正则表达式 # -*- coding: utf-8 -* import re email_regex = re.compile(\"[\\w+\\.]+@[a-zA-Z\\d]+\\.(com|cn)\") email_regex = re.compile(\"\"\"[\\w+\\.]+ # 匹配@符前的部分 @ # @符 [a-zA-Z\\d]+ # 邮箱类别 \\.(com|cn) # 邮箱后缀 \"\"\", re.X) UNICODE解析re.U # U: UNICODE，使用 \\w, \\W, \\b, \\B 这些元字符时将按照 UNICODE 定义的属性. re.Match类 若匹配成功，match/search返回的是Match对象，finditer返回的也是Match对象的迭代器 获取匹配结果需要调用Match对象的group()、groups或group(index)方法 属性说明 endpos: 本次搜索结束位置索引 lastgroup: 本次搜索匹配到的最后一个分组的别名 lastindex: 本次搜索匹配到的最后一个分组的索引 pos: 本次搜索开始位置索引 re: 本次搜索使用的 SRE_Pattern 对象 regs: 列表，元素为元组，包含本次搜索匹配到的所有分组的起止位置 方法说明 end([group=0])：返回指定分组的结束位置，默认返回正则表达式所匹配到的最后一个字符的索引 expand(template)：根据模版返回相应的字符串，类似与 sub 函数里面的 repl， 可使用 \\1 或者 \\g 来选择分组 group([group1, ...])：根据提供的索引或名字返回响应分组的内容 默认返回 start() 到 end() 之间的字符串， 提供多个参数将返回一个元组 groupdict([default=None])：返回一个包含所有匹配到的命名分组的字典，没有命名的分组不包含在内 key 为组名， value 为匹配到的内容，参数 default 为没有参与本次匹配的命名分组提供默认值 groups([default=None])：以元组形式返回每一个分组匹配到的字符串，包括没有参与匹配的分组，其值为 default span([group])：返回指定分组的起止位置组成的元组，默认返回由 start() 和 end() 组成的元组 start([group])：返回指定分组的开始位置，默认返回正则表达式所匹配到的第一个字符的索引 # -*- coding: utf-8 -* import re s = 'Hello, Mr.Gumby : 2016/10/26' m = re.search(', (?P\\w+\\.\\w+).*?(\\d+)', s) # 本次搜索的结束位置索引 print(f'm.endpos: {m.endpos}') # 本次搜索匹配到的最后一个分组的别名 # 本次匹配最后一个分组没有别名 print(f'm.lastgroup: {m.lastgroup}') # 本次搜索匹配到的最后一个分组的索引 print(f'm.lastindex: {m.lastindex}') # 本次搜索开始位置索引 print(f'm.pos: {m.pos}') # 本次搜索使用的 SRE_Pattern 对象 print(f'm.re: {m.re}') # 列表，元素为元组，包含本次搜索匹配到的所有分组的起止位置 第一个元组为正则表达式匹配范围 print(f'm.regs: {m.regs}') # 本次搜索操作的字符串 print(f'm.string: {m.string}') m.endpos: 28 m.lastgroup: None m.lastindex: 2 m.pos: 0 m.re: re.compile(', (?P\\\\w+\\\\.\\\\w+).*?(\\\\d+)') m.regs: ((5, 22), (7, 15), (18, 22)) m.string: Hello, Mr.Gumby : 2016/10/26 # -*- coding: utf-8 -* import re s = 'Hello, Mr.Gumby : 2016/10/26' m = re.search('''(?: # 构造一个不捕获分组 用于使用 | (?P\\w+\\.\\w+) # 匹配 Mr.Gumby | # 或 (?P\\s+\\.\\w+) # 一个匹配不到的命名分组 ) .*? # 匹配 : (\\d+) # 匹配 2016 ''', s, re.X) # 返回指定分组的结束位置，默认返回正则表达式所匹配到的最后一个字符的索引 print(f'm.end(): {m.end()}') # 根据模版返回相应的字符串，类似sub()的 repl,可使用 \\1 或者 \\g 来选择分组 mExpend = m.expand('my name is \\\\1') print(f\"m.expand('my name is \\\\1'): {mExpend}\") # 根据提供的索引或名字返回响应分组的内容，默认返回 start() 到 end() 之间的字符串， # 提供多个参数将返回一个元组 print(f'm.group(): {m.group()}') print(f'm.group(1, 2): {m.group(1, 2)}') # 返回一个包含所有匹配到的命名分组的字典，没有命名的分组不包含在内，key 为组名， # value 为匹配到的内容，参数 default 为没有参与本次匹配的命名分组提供默认值 m.groupdict('default_string') # 以元组形式返回每一个分组匹配到的字符串，包括没有参与匹配的分组，其值为 default print(f\"m.groups('default_string'): {m.groups('default_string')}\") # 返回指定分组的起止未知组成的元组，默认返回由 start() 和 end() 组成的元组 print(f'm.span(3): {m.span(3)}') # 返回指定分组的开始位置，默认返回正则表达式所匹配到的第一个字符的索引 print(f'm.start(3): {m.start(3)}') m.end(): 22 m.expand('my name is \\1'): my name is Mr.Gumby m.group(): Mr.Gumby : 2016 m.group(1, 2): ('Mr.Gumby', None) m.groups('default_string'): ('Mr.Gumby', 'default_string', '2016') m.span(3): (18, 22) m.start(3): 18 匹配 match re.match(pattern, string[, flags]) 从首字母开始匹配，string如果包含pattern子串，则匹配成功，返回Match对象，失败则返回None，若要完全匹配，pattern要以$结尾 参数说明 pattern：匹配的正则表达式 string：要匹配的字符串。 flags：标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等。参见：正则表达式修饰符 - 可选标志 match函数在string的开始位置匹配，如果匹配则返回对象，否则返回None #! /usr/bin/env python # -*- coding=utf-8 -*- import re text = 'python world' m = re.match(r\"\\w+\", text) print(m.group(0)) if m else print('not match') text = '#python world' m = re.match(r\"\\w+\", text) print(m.group(0)) if m else print('not match') output: python output: not match # re.match()函数 # -*- coding: utf-8 -* import re str1 = '333STR1666STR299' pattern_s = r'([A-Z]+(\\d))' print(f're.match(pattern_s, str1): {re.match(pattern_s, str1)}') # str1的开头不符合正则，所以结果为None re.match(pattern_s, str1): None search re.search(pattern, string[, flags]) 若string中包含pattern子串，则返回Match对象，否则返回None，注意，如果string中存在多个pattern子串，只返回第一个 参数说明 pattern：匹配的正则表达式 string：要匹配的字符串 flags：标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等 search会扫描整个string查找匹配，如果找到则返回一个相应的匹配对象，否则返回None #! /usr/bin/env python # -*- coding=utf-8 -*- import re text = 'python world' m = re.search(r\"\\w+\", text) print(m.group(0)) if m else print('not match') text = '#python world' m = re.search(r\"\\w+\", text) print(m.group(0)) if m else print('not match') output: python output: python # re.search()函数 # -*- coding: utf-8 -* import re str1 = '333STR1666STR299' pattern_s = r'([A-Z]+(\\d))' match_r = re.search(pattern_s, str1) # match_r[0]是regex所代表的整个字符串 print(f'match_r.group(0): {match_r.group(0)}') # match_r.group() # match_r[1]是第一个()中的内容 print(f'match_r.group(1): {match_r.group(1)}') # match_r[2]是第二对()中的内容 print(f'match_r.group(2): {match_r.group(2)}') # 所有group组成的一个元组 print(f'match_r.groups(): {match_r.groups()}') match_r.group(0): STR1 match_r.group(1): STR1 match_r.group(2): 1 match_r.groups(): ('STR1', '1') 检索 findall re.findall(pattern, string[, flags]) 返回string中所有与pattern相匹配的全部字串，返回形式为数组 参数说明 string: 待匹配的字符串 pos: 可选参数，指定字符串的起始位置，默认为 0 endpos: 可选参数，指定字符串的结束位置，默认为字符串的长度 findall返回所有匹配的指定模式的文本子串到列表中，一个元素一个匹配串 # re.findall()函数 # -*- coding: utf-8 -* import re str1 = '333STR1666STR299' pattern_s = r'([A-Z]+(\\d))' match_r = re.findall(pattern_s, str1) for m in match_r: print(f'm[0], m[1]: {m[0], m[1]}') m[0], m[1]: ('STR1', '1') m[0], m[1]: ('STR2', '2') finditer re.finditer(pattern, string[, flags]) 返回string中所有与pattern相匹配的全部字串，返回形式为迭代器 参数说明 pattern：匹配的正则表达式 string：要匹配的字符串。 flags：标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等。参见：正则表达式修饰符 - 可选标志 finditer函数跟findall函数类似，但返回的是一个迭代器 # re.finditer()函数 # -*- coding: utf-8 -* import re str1 = '333STR1666STR299' pattern_s = r'([A-Z]+(\\d))' match_r = re.finditer(pattern_s, str1) for m in match_r: print(f'm.group(0), m.group(1), m.group(2): {m.group(0), m.group(1), m.group(2)}') # 字符串 m.group(0), m.group(1), m.group(2): ('STR1', 'STR1', '1') m.group(0), m.group(1), m.group(2): ('STR2', 'STR2', '2') 替换和分割 sub替换 re.sub(pattern, repl, string, count=0, flags=0) 将字符串中匹配正则表达式的部分替换为其他值 Return the string obtained by replacing the leftmost non-overlapping occurrences of the pattern in string by the replacement repl. repl can be either a string or a callable; if a string, backslash escapes in it are processed. If it is a callable, it's passed the Match object and must return a replacement string to be used. 参数说明 pattern: 正则中的模式字符串。 repl: 替换的字符串，也可为一个函数。 string: 要被查找替换的原始字符串。 count: 模式匹配后替换的最大次数，默认 0 表示替换所有的匹配。 代码 # -*- coding: utf-8 -* import re def add_one(match): val = match.group() num = int(val) + 1 return str(num) str2 = 'imooc videonum=200' reSub_1 = re.sub(r'\\d+', add_one, str2) print(f\"re.sub(r'\\d+', add_one, str2): {reSub_1}\") reSub_2 = re.sub(r'\\d+', '203', str2) print(f\"re.sub(r'\\d+', '203', str2): {reSub_2}\") sample_text = '2020-05-20 10:59:23 hello world 2020-05-21 10:59:24 hello kitty' sample_pattern = r'(?P\\d{4})-(?P\\d{2})-(?P\\d{2})' sample_repl = r'\\g/\\g/\\g' print(re.sub(sample_pattern, sample_repl, sample_text)) 输出 re.sub(r'\\d+', add_one, str2): imooc videonum=201 re.sub(r'\\d+', '203', str2): imooc videonum=203 05/20/2020 10:59:23 hello world 05/21/2020 10:59:24 hello kitty 高级用法 一日一技：如何正确使用 re.sub 的第二个参数 re.sub第二个参数可以是函数 设想有一个字符串abc18123456794xyz123，这个字符串中有两段数字，并且长短是不一样的 第一个数字是11位的手机号。我想把字符串替换为：`abc[隐藏手机号]xyz* 不是手机号的数字，每一位数字逐位替换为星号 import re def test(repl): if len(repl.group(0)) == 11: return '[隐藏手机号]' else: return '*' * len(repl.group(0)) a = 'abc18123456794xyz123' b = re.sub('\\d+', test, a) print(b) subn替换 subn(pattern, repl, string, count=0, flags=0) 作用与函数 sub 一样， 唯一不同之处在于返回值为一个元组，第一个值为替换后的字符串，第二个值为发生替换的次数 split分割 re.split(pattern, string, maxsplit=0, flags=0) 根据匹配分割字符串，返回分割字符串组成的列表 Split the source string by the occurrences of the pattern, returning a list containing the resulting substrings. If capturing parentheses are used in pattern, then the text of all groups in the pattern are also returned as part of the resulting list. If maxsplit is nonzero, at most maxsplit splits occur, and the remainder of the string is returned as the final element of the list. 参数说明 pattern：匹配的正则表达式 string：要匹配的字符串。 maxsplit：分隔次数，maxsplit=1 分隔一次，默认为 0，不限制次数。 flags：标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等。参见：正则表达式修饰符 - 可选标志 代码 # -*- coding: utf-8 -* import re str3 = 'imooc:C C++ Java Python,c#' resub_3 = re.split(r':| |,',str3) print(f\"re.split(r':| |,',str3):{resub_3}\") 输出 re.split(r':| |,',str3):['imooc', 'C', 'C++', 'Java', 'Python', 'c#'] 正则缓存 当你在程序中使用 re 模块，无论是先使用 compile 还是直接使用比如 findall 来使用正则表达式操作文本 re 模块都会将正则表达式先编译一下， 并且会将编译过后的正则表达式放到缓存中 这样下次使用同样的正则表达式的时候就不需要再次编译， 因为编译其实是很费时的，这样可以提升效率 而默认缓存的正则表达式的个数是 100，当你需要频繁使用少量正则表达式的时候，缓存可以提升效率 而使用的正则表达式过多时，缓存带来的优势就不明显了 这个re.purge()函数的作用是清除缓存中的正则表达式，可能在你需要优化占用内存的时候会用到 爬虫例子 代码 # -*- coding: utf-8 -* import urllib.request import re import os url = r'https://www.imooc.com/course/list?c=ai' res = urllib.request.urlopen(url) html = res.read().decode('utf-8') listurl = re.findall(r'src=\"//img\\d.+?jpg\"', html) lurl = ['http:%s' % x[5:-1] for x in listurl] print(lurl) basepath = './res' if not os.path.exists(basepath): os.makedirs(basepath) imgspath = os.path.join(basepath, 're_op') if not os.path.exists(imgspath): os.mkdir(imgspath) for ii, uu in enumerate(lurl): savepath = os.path.join(imgspath, '%d.jpg' % (ii)) res = urllib.request.urlretrieve(uu, savepath) 输出 ['http://img4.mukewang.com/5bd8157a0001a7a506000336-240-135.jpg', 'http://img3.mukewang.com/5bc6e6b80001434f06000338-240-135.jpg', 'http://img2.mukewang.com/5ba2386600013d3705980337-240-135.jpg', 'http://img3.mukewang.com/5b9105800001288905400300-240-135.jpg', 'http://img4.mukewang.com/5b7f737a0001cfb706000336-240-135.jpg', 'http://img4.mukewang.com/5abc6159000142f706000338-240-135.jpg', 'http://img1.mukewang.com/5a40c6370001d13c06000338-240-135.jpg'] 进阶用法 非贪婪匹配 匹配模式 贪婪匹配：贪婪匹配在匹配字符串时总是尝试匹配尽可能多的字符 非贪婪匹配：与贪婪匹配相反，非贪婪匹配在匹配字符串时总是尝试匹配尽可能少的字符 Python里数量词默认是贪婪模式的，使用*? / +? / ??可使贪婪模式变成非贪婪模式 # -*- coding: utf-8 -* import re str1 = '9abc' # 需要查找的原始字符串 pattern_s = r'[0-9][a-z]*?' # 定义正则表达式 pattern_r = re.compile(pattern_s) # 编译正则表达式 match_r = pattern_r.match(str1) print(f'match_r.group(): {match_r.group()}') pattern_s = r'[0-9][a-z]*' # 定义正则表达式 pattern_r = re.compile(pattern_s) # 编译正则表达式 match_r = pattern_r.match(str1) print(f'match_r.group(): {match_r.group()}') pattern_s = r'[0-9][a-z]+?' # 定义正则表达式 pattern_r = re.compile(pattern_s) # 编译正则表达式 match_r = pattern_r.match(str1) print(f'match_r.group(): {match_r.group()}') pattern_s = r'[0-9][a-z]+' # 定义正则表达式 pattern_r = re.compile(pattern_s) # 编译正则表达式 match_r = pattern_r.match(str1) print(f'match_r.group(): {match_r.group()}') pattern_s = r'[0-9][a-z]??' # 定义正则表达式 pattern_r = re.compile(pattern_s) # 编译正则表达式 match_r = pattern_r.match(str1) print(f'match_r.group(): {match_r.group()}') pattern_s = r'[0-9][a-z]?' # 定义正则表达式 pattern_r = re.compile(pattern_s) # 编译正则表达式 match_r = pattern_r.match(str1) print(f'match_r.group(): {match_r.group()}') match_r.group(): 9 match_r.group(): 9abc match_r.group(): 9a match_r.group(): 9abc match_r.group(): 9 match_r.group(): 9a 分组匹配(捕获组) import regex as re pat = re.compile(r\"有(?P(许可证[件]?|执照))的?(?P(暂扣|吊销).{0,4}(?P=zj))\") doc = \"有许可证的吊销许可证，并给予严厉惩罚\" res = list(re.finditer(pat, doc)) res[0].groupdict() Out[16]: {'zj': '许可证', 'xf_type': '吊销许可证'} # 分组匹配之 \\ # -*- coding: utf-8 -* import re str1 = 'python' pattern_s = r')[\\w]+python # 分组匹配之 别名 (?P) 和 (?P=name) # -*- coding: utf-8 -* import re str1 = 'python' pattern_s = r'[\\w]+>)[\\w]+python re.match(pattern_s, str1).groupdict() Out[13]: {'mark1': 'book>'} # 分组匹配之group()和groups() group() / group(0): 母串中与模式pattern匹配的子串 group(index): 第index个group匹配成功的子串 groups(): 所有group组成的一个元组，与pattern中的()有关 # -*- coding: utf-8 -* import re p = re.compile('\\d-\\d-\\d') m = p.match('2-3-1') print(f'm.group(): {m.group()}') print(f'm.group(0): {m.group(0)}') print(f'm.groups(): {m.groups()}') m.group(): 2-3-1 m.group(0): 2-3-1 m.groups(): () # -*- coding: utf-8 -* import re p = re.compile('(\\d)-(\\d)-(\\d)') m = p.match('2-3-1d5-4-3') print(f'm.group(): {m.group()}') print(f'm.group(0): {m.group(0)}') print(f'm.groups(): {m.groups()}') m.group(): 2-3-1 m.group(0): 2-3-1 m.groups(): ('2', '3', '1') # -*- coding: utf-8 -* import re p = re.compile('(\\d)-(\\d)-(\\d)') m = re.match(p,'2-3-1d5-4-3') print(f'm.group(): {m.group()}') print(f'm.group(0): {m.group(0)}') print(f'm.groups(): {m.groups()}') m.group(): 2-3-1 m.group(0): 2-3-1 m.groups(): ('2', '3', '1') 转义匹配替代函数 使用python的过程中，你肯定对转义字符的使用苦恼过，因为有的时候我们需要使用一些特殊符号如”$ * . ^”等的原意 有时候需要被转义后的功能，并且转义字符地使用很繁琐，容易出错 re.escape(pattern) 转义: 如果你需要操作的文本中含有正则的元字符，你在写正则的时候需要将元字符加上反斜扛 \\ 去匹配自身 而当这样的字符很多时，写出来的正则表达式就看起来很乱而且写起来也挺麻烦的，这个时候你可以使用这个函数 可以对字符串中所有可能被解释为正则运算符的字符进行转义的应用函数 # -*- coding: utf-8 -* import re str2 = \".+\\d123\" regex_str = re.escape(\".+\\d123\") # 查看转义后的字符 print(f'regex_str: {regex_str}') # 查看匹配到的结果 for g in re.findall(regex_str, str2): print(g) regex_str: \\.\\+\\\\d123 .+\\d123 re增强的包装 主要目的是，增强re的匹配能力，为match对象添加了start: int=0参数，start指定了当前字符串的起始位置，默认的起始位置是0 导入库，regex是re的增强版，需要pip安装，它支持更强的正则，比如一个表达式中可以出现两个一样的捕获组名字 from typing import Iterable, List import regex as re 包装match对象为ReMatch类 class ReMatch: def __init__(self, match, start: int = 0): \"\"\" 指定起始位置的Match类 :param match: re.Match对象 :param start: \"\"\" self.match = match self.start_idx = start @property def endpos(self): \"\"\" 本次搜索结束位置索引 \"\"\" return self.match.endpos + self.start_idx @property def lastgroup(self): \"\"\" 本次搜索匹配到的最后一个分组的别名 \"\"\" return self.match.lastgroup @property def lastindex(self): \"\"\" 本次搜索匹配到的最后一个分组的索引 \"\"\" return self.match.lastindex + self.start_idx if type(self.match.lastindex) == int else self.match.lastindex @property def pos(self): \"\"\" 本次搜索开始位置索引 \"\"\" return self.match.pos + self.start_idx @property def re(self): \"\"\" 本次搜索使用的SRE_Pattern对象 \"\"\" return self.match.re @property def regs(self): \"\"\" 列表，元素为元组，包含本次搜索匹配到的所有分组的起止位置 \"\"\" return tuple([(s + self.start_idx, e + self.start_idx) for s, e in self.match.regs]) def end(self, *args, **kwargs): \"\"\" 返回指定分组的结束位置，默认返回正则表达式所匹配到的最后一个字符的索引 \"\"\" return self.match.end(*args, **kwargs) + self.start_idx def expand(self, *args, **kwargs): \"\"\" 根据模版返回相应的字符串，类似与 sub 函数里面的 repl， 可使用 \\1 或者 \\g 来选择分组 \"\"\" return self.match.expand(*args, **kwargs) def group(self, *args, **kwargs): \"\"\" 根据提供的索引或名字返回响应分组的内容，默认返回 start() 到 end() 之间的字符串， 提供多个参数将返回一个元组 \"\"\" return self.match.group(*args, **kwargs) def groupdict(self, *args, **kwargs): \"\"\" 返回一个包含所有匹配到的命名分组的字典，没有命名的分组不包含在内，key 为组名， value 为匹配到的内容，参数 default 为没有参与本次匹配的命名分组提供默认值 \"\"\" return self.match.groupdict(*args, **kwargs) def _build_key_2_index(self): groups = self.match.groups() group_2_index = [] for idx, group in enumerate(groups, start=1): group_2_index.append((group, self.match.start(idx)+self.start_idx, self.match.end(idx)+self.start_idx)) return group_2_index def groupdict_with_index(self, *args, **kwargs): \"\"\" 同groupdict, 但匹配结果包装为带索引的ContentWithIndex对象 \"\"\" value = self.match.groupdict(*args, **kwargs) new_group_dict = dict() for k, v in value.items(): for group, s, e in self._build_key_2_index(): if v == group: new_group_dict[k] = ContentWithIndex(content=v, start=s, end=e) assert len(new_group_dict) == len(value) return new_group_dict def groups(self, *args, **kwargs): \"\"\" 以元组形式返回每一个分组匹配到的字符串，包括没有参与匹配的分组，其值为 default \"\"\" return self.match.groups(*args, **kwargs) def span(self, *args, **kwargs): \"\"\" 返回指定分组的起止位置组成的元组，默认返回由 start() 和 end() 组成的元组 \"\"\" return tuple([(s + self.start_idx, e + self.start_idx) for s, e in self.match.span(*args, **kwargs)]) def start(self, *args, **kwargs): \"\"\" 返回指定分组的开始位置，默认返回正则表达式所匹配到的第一个字符的索引 \"\"\" return self.match.start(*args, **kwargs) + self.start_idx def start_and_end(self): return self.start(), self.end() def __str__(self): return self.__repr__() def __repr__(self): return f\"\" 匹配结果类ContentWithIndex：主要是统一像split这样的方法返回的结果 class ContentWithIndex: def __init__(self, content: str, start: int, end: int): \"\"\" 带索引的切分结果 :param content: 切分结果 :param start: 起始位置 :param end: 结束位置 \"\"\" self.content = content self.start = start self.end = end @property def pos(self): return self.start @property def posend(self): return self.end def start_and_end(self): return self.start, self.end def __str__(self): return self.__repr__() def __repr__(self): return f\"{self.content} ({self.start}, {self.end})\" re包装的工具类ReUtils class ReUtils: def __init__(self): \"\"\" 正则匹配的工具类 \"\"\" pass @staticmethod def match(pattern, string, flags=0, start: int = 0): match = re.match(pattern=pattern, string=string, flags=flags) return ReMatch(match=match, start=start) @staticmethod def fullmatch(pattern, string, flags=0, start: int = 0): match = re.fullmatch(pattern=pattern, string=string, flags=flags) return ReMatch(match=match, start=start) @staticmethod def search(pattern, string, flags=0, start: int = 0): match = re.search(pattern=pattern, string=string, flags=flags) return ReMatch(match=match, start=start) @staticmethod def sub(pattern, repl, string, count=0, flags=0): return re.sub(pattern=pattern, repl=repl, string=string, count=count, flags=flags) @staticmethod def subn(pattern, repl, string, count=0, flags=0): return re.subn(pattern=pattern, repl=repl, string=string, count=count, flags=flags) @staticmethod def split(pattern, string, maxsplit=0, flags=0): return re.split(pattern=pattern, string=string, maxsplit=maxsplit, flags=flags) @staticmethod def split_with_index(pattern, string, maxsplit: int = 0, flags=0, start: int = 0) -> List[ContentWithIndex]: \"\"\" split带内容索引 :param pattern: :param string: :param maxsplit: :param flags: :param start: :return: \"\"\" matches = ReUtils.finditer(pattern, string, flags, start=start) # 使用re.finditer()获取匹配的位置和内容 content_with_indexs = [] start_idx = start for match in matches: end_idx = match.start() # 当前分隔符的起始位置 content_with_index = ContentWithIndex(content=string[start_idx - start:end_idx - start], start=start_idx, end=end_idx) content_with_indexs.append(content_with_index) # 添加分隔符之前的文本及其位置信息 start_idx = match.end() # 更新下一个分隔符的起始位置 if maxsplit and len(content_with_indexs) >= maxsplit: content_with_index = ContentWithIndex(content=string[start_idx - start:], start=start_idx, end=len(string)) content_with_indexs.append(content_with_index) # 添加分隔符之前的文本及其位置信息 break else: # 添加最后一个分隔符之后的文本 content_with_index = ContentWithIndex(content=string[start_idx - start:], start=start_idx, end=len(string) + start) content_with_indexs.append(content_with_index) # if maxsplit and len(content_with_indexs) Iterable[ReMatch]: for match in re.finditer(pattern=pattern, string=string, flags=flags): yield ReMatch(match=match, start=start) Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-12-01 00:57:11 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/8.collections模块.html":{"url":"chapters/8.collections模块.html","title":"collections模块","keywords":"","body":"Python的collections模块具名元组namedtuple概念使用定义访问字典介绍defaultdict默认值词典错误方式正确方式统计缺失多少键其它默认值有序词典OrderedDict概念使用例子2ChainMap计数器Counter定义遍历元素更新元素类集合操作子类化UserDict双向队列deque定义常用方法子类化UserDict Python的collections模块 collections --- 容器数据类型 Python3 collections模块使用详解 collections — Container datatypes namedtuple(): 生成可以使用名字来访问元素内容的tuple子类 deque: 双端队列，可以快速的从另外一侧追加和推出对象 Counter: 计数器，主要用来计数 OrderedDict: 有序字典 defaultdict: 带有默认值的字典 ChainMap 类似字典(dict)的容器类，将多个映射集合到一个视图里面 UserDict 封装了字典对象，简化了字典子类化 UserList 封装了列表对象，简化了列表子类化 UserString 封装了列表对象，简化了字符串子类化 collections.abc 抽象基类 tuple的几个特性： 不可变，iterable 拆包 tuple不可变不是绝对的 tuple比list好的地方 4.1 immutable的重要性：性能优化(元素全为immutable的会作为常量在编译时确定)、线程安全、可以作为dict的key，拆包特性 4.2 tuple类似struct，list类似array 容器序列 　　list、tuple 和 collections.deque 这些序列能存放不同类型的数据。 扁平序列 　　str、bytes、bytearray、memoryview 和 array.array，这类序列只能容纳一种类型。 容器序列存放的是它们所包含的任意类型的对象的引用，而扁平序列里存放的是值而不是引用。换句话说，扁平序列其实是一段连续的内存空间。由此可见扁平序列其实更加紧凑，但是它里面只能存放诸如字符、字节和数值这种基础类型。 序列类型还能按照能否被修改来分类。 可变序列 　　list、bytearray、array.array、collections.deque 和 memoryview。 不可变序列 　　tuple、str 和 bytes。 具名元组namedtuple 概念 命名的元组，意味给元组中的每个位置赋予含义，意味着代码可读性更强，namedtuple可以在任何常规元素使用的地方使用，而且它可以通过名称来获取字段信息而不仅仅是通过位置索引。用以构建只有少数属性但是没有方法的对象，比如数据库条目。 collections.namedtuple 是一个工厂函数，它可以用来构建一个带字段名的元组和一个有名字的 用 namedtuple 构建的类的实例所消耗的内存跟元组是一样的，因为字段名都被存在对应的类里面 创建一个具名元组需要两个参数，一个是类名，另一个是类的各个字段的名字。 几个最有用的：_fields 类属性、类方法 _make(iterable) 和实例方法 _asdict()。 ❶ _fields 属性是一个包含这个类所有字段名称的元组。 ❷ 用 _make() 通过接受一个可迭代对象来生成这个类的一个实例，它的作用跟 City(*delhi_data) 是一样的。 ❸ _asdict() 把具名元组以 collections.OrderedDict 的形式返回，我们可以利用它来把元组里的信息友好地呈现出来。 from collections import namedtuple User = namedtuple(\"USER\", [\"name\", \"age\", \"city\", \"height\"]) user_1 = User(name=\"雷鸣\", age=21, city=\"北京\", height=\"175\") print(\"user_1\", user_1.name, user_1.age, user_1.city, user_1.height) user_1 雷鸣 21 北京 175 user_1._fields Out[4]: ('name', 'age', 'city', 'height') 使用 定义 代码 # namedtuple： 生成可以使用名字来访问元素内容的tuple子类 # -*- coding: utf-8 -* from collections import namedtuple User = namedtuple(\"USER\", [\"name\", \"age\", \"city\", \"height\"]) user_1 = User(name=\"雷鸣\", age=21, city=\"北京\", height=\"175\") print(\"user_1\", user_1.name, user_1.age, user_1.city, user_1.height) user_tuple = (\"雷姆\", 17, '异世界') user_2 = User(*user_tuple, \"172\") print(\"user_2\", user_2.name, user_2.age, user_2.city, user_2.height) user_dict = { \"name\": \"雷玖\", \"age\": \"17\", \"city\": \"异世界\", } user_3 = User(**user_dict, height=\"172\") print(\"user_3\", user_3.name, user_3.age, user_3.city, user_3.height) user_tuple_2 = (\"雷姆\", 17, '异世界', \"172\") user_list_2 = [\"雷姆\", 17, '异世界', \"172\"] user_dict_2 = { \"name\": \"雷玖\", \"age\": \"17\", \"city\": \"异世界\", \"height\": \"172\" } user_4 = User._make(user_dict_2.values()) print(\"user_4\", user_4.name, user_4.age, user_4.city, user_4.height) user_5 = User._make(user_tuple_2) print(\"user_5\", user_5.name, user_5.age, user_5.city, user_5.height) user_6 = User._make(user_list_2) print(\"user_6\", user_6.name, user_6.age, user_6.city, user_6.height) 输出 user_1 雷鸣 21 北京 175 user_2 雷姆 17 异世界 172 user_3 雷玖 17 异世界 172 user_4 雷玖 17 异世界 172 user_5 雷姆 17 异世界 172 user_6 雷姆 17 异世界 172 访问 代码 from collections import namedtuple User = namedtuple(\"USER\", [\"name\", \"age\", \"city\", \"height\"]) user_tuple = (\"雷姆\", 17, '异世界') user_2 = User(*user_tuple, \"172\") print(user_2._asdict()) item = ('a', 'b', 'a', 'a', 5) for it in set(item): print(f'{it} 出现了{item.count(it)}次') 输出 OrderedDict([('name', '雷姆'), ('age', 17), ('city', '异世界'), ('height', '172')]) a 出现了3次 5 出现了1次 b 出现了1次 字典 介绍 dict 类型不但在各种程序里广泛使用，它也是 Python 语言的基石。 跟它有关的内置函数都在 __builtins__.__dict__模块中。 正是因为字典至关重要，Python 对它的实现做了高度优化，而散列表则是字典类型性能出众的根本原因。 collections.abc 模块中有 Mapping 和 MutableMapping 这两个抽象基类 然而，非抽象映射类型一般不会直接继承这些抽象基类，它们会直接对 dict 或是 collections.UserDict 进行扩展。 标准库里的所有映射类型都是利用 dict 来实现的，因此它们有个共同的限制，即只有可散列的数据类型才能用作这些映射里的键 关于可散列类型的定义有这样一段话： 如果一个对象是可散列的，那么在这个对象的生命周期中，它的散列值是不变的，而且这个对象需要实现 __hash__() 方法。另外可散列对象还要有 __qe__() 方法，这样才能跟其他键做比较。如果两个可散列对象是相等的，那么它们的散列值一定是一样的 一般来讲用户自定义的类型的对象都是可散列的，散列值就是它们的 id() 函数的返回值，所以所有这些对象在比较的时候都是不相等的 创建字典的不同方式： >>> a = dict(one=1, two=2, three=3) >>> b = {'one': 1, 'two': 2, 'three': 3} >>> c = dict(zip(['one', 'two', 'three'], [1, 2, 3])) >>> d = dict([('two', 2), ('one', 1), ('three', 3)]) >>> e = dict({'three': 3, 'one': 1, 'two': 2}) >>> a == b == c == d == e True 除了这些字面句法和灵活的构造方法之外，字典推导（dict comprehension）也可以用来建造新 dict 字典推导（dictcomp）可以从任何以键值对作为元素的可迭代对象中构建出字典。 映射类型的方法其实很丰富。表 3-1 为我们展示了 dict、defaultdict 和 OrderedDict 的常见方法，后面两个数据类型是 dict 的变种，位于 collections 模块内。 字典的变种： collections.OrderedDict： 这个类型在添加键的时候会保持顺序，因此键的迭代次序总是一致的。OrderedDict的 popitem 方法默认删除并返回的是字典里的最后一个元素，但是如果像 my_odict.popitem(last=False) 这样调用它，那么它删除并返回第一个被添加进去的元素。 collections.ChainMap： 该类型可以容纳数个不同的映射对象，然后在进行键查找操作的时候，这些对象会被当作一个整体被逐个查找，直到键被找到为止。 collections.Counter： 这个映射类型会给键准备一个整数计数器。每次更新一个键的时候都会增加这个计数器。所以这个类型可以用来给可散列表对象计数，或者是当成多重集来用——多重集合就是集合里的元素可以出现不止一次。 跟 OrderedDict、ChainMap 和 Counter 这些开箱即用的类型不同，UserDict 是让用户继承写子类的。 dict的实现及其导致的结果 键必须是可散列的 一个可散列的对象必须满足以下要求。 (1) 支持 hash() 函数，并且通过 __hash__() 方法所得到的散列值是不变的。 (2) 支持通过 __eq__() 方法来检测相等性。 (3) 若 a == b 为真，则 hash(a) == hash(b) 也为真。 所有由用户自定义的对象默认都是可散列的，因为它们的散列值由 id() 来获取，而且它们都是不相等的。 字典在内存上的开销巨大 由于字典使用了散列表，而散列表又必须是稀疏的，这导致它在空间上的效率低下。 在用户自定义的类型中，__slots__ 属性可以改变实例属性的存储方式，由 dict变成 tuple 键查询很快 dict 的实现是典型的空间换时间：字典类型有着巨大的内存开销，但它们提供了无视数据量大小的快速访问——只要字典能被装在内存里 键的次序取决于添加顺序 当往 dict 里添加新键而又发生散列冲突的时候，新键可能会被安排存放到另一个位置。 往字典里添加新键可能会改变已有键的顺序 无论何时往字典里添加新的键，Python 解释器都可能做出为字典扩容的决定。扩容导致的结果就是要新建一个更大的散列表，并把字典里已有的元素添加到新表里。 defaultdict默认值词典 错误方式 代码 # 错误方式一 student_grades = {} grades = [('elliot', 91), ('neelam', 98), ('bianca', 81), ('elliot', 88)] for name, grade in grades: if name not in student_grades: student_grades[name] = [] student_grades[name].append(grade) print(student_grades) # 错误方式二 student_grades = {} for name, grade in grades: student_grades.setdefault(name, []) student_grades[name].append(grade) print(student_grades) 输出 {'elliot': [91, 88], 'neelam': [98], 'bianca': [81]} {'elliot': [91, 88], 'neelam': [98], 'bianca': [81]} 正确方式 在这种情况下，你将创建一个defaultdict，它使用不带参数的list构造函数作为默认方法。没有参数的list返回一个空列表，因此如果名称不存在则defaultdict调用list()，然后再把学生成绩添加上。如果你想更炫一点，你也可以使用lambda函数作为值来返回任意常量。 在用户创建 defaultdict 对象的时候，就需要给它配置一个为找不到的键创造默认值的方法。 实例化一个 defaultdict 的时候，需要给构造方法提供一个可调用对象 把 list 构造方法作为 default_factory 来创建一个 defaultdict。 defaultdict 里的 default_factory 只会在 __getitem__ 里被调用，在其他的方法里完全不会发挥作用。比如，dd 是个 defaultdict，k 是个找不到的键， dd[k] 这个表达式会调用 default_factory 创造某个默认值，而 dd.get(k) 则会返回 None。 所有的映射类型在处理找不到的键的时候，都会牵扯到 __missing__ 方法。这也是这个方法称作“missing”的原因 如果有一个类继承了 dict，然后这个继承类提供了 __missing__ 方法，那么在 __getitem__ 碰到找不到的键的时候，Python 就会自动调用它 __missing__ 方法只会被 __getitem__ 调用 像 k in my_dict.keys() 这种操作在 Python 3 中是很快的，而且即便映射类型对象很庞大也没关系。这是因为 dict.keys() 的返回值是一个“视图”。视图就像一个集合，而且跟字典类似的是，在视图里查找一个元素的速度很快。 代码 # 正确方式 from collections import defaultdict student_grades = defaultdict(list) [student_grades[name].append(grade) for name, grade in grades] print(student_grades) 输出 defaultdict(, {'elliot': [91, 88], 'neelam': [98], 'bianca': [81]}) 统计缺失多少键 class CountMissing: ...: def __init__(self): ...: self.added = 0 ...: def missing(self): ...: self.added += 1 ...: return 0 ...: current = {'green':12, 'blue':3} increments = [('red',5),('blue',17),('orange',9)] counter = CountMissing() from collections import defaultdict result = defaultdict(counter.missing, current) for key,amount in increments: ...: result[key] += amount ...: print(counter.added) 2 其它默认值 代码 # 错误方式 cowboy = {'age': 32, 'horse': 'mustang', 'hat_size': 'large'} if 'name' in cowboy: name = cowboy['name'] else: name = 'The Man with No Name' print(name) # 如果你想在仍然访问name的key时使用默认值更新字典 # 错误方式 if 'name' not in cowboy: cowboy['name'] = 'The Man with No Name' name = cowboy['name'] print(name) # 正确方式 # get()执行与第一种方法相同的操作，但现在它们会自动处理。如果key存在，则返回适当的值。否则，将返回默认值 name = cowboy.get('name', 'The Man with No Name') print(name) # 正确方式 name = cowboy.setdefault('name', 'The Man with No Name') print(name) from collections import defaultdict dic = defaultdict(lambda :'0a') dic['a'] Out[41]: '0a' 输出 The Man with No Name The Man with No Name The Man with No Name The Man with No Name 有序词典OrderedDict 概念 OrderedDict类似于正常的词典，只是它记住了元素插入的顺序，当在有序的词典上迭代时，返回的元素就是它们第一次添加的顺序。 class collections.OrderedDict，返回已给dict的子类，支持常规的dict的方法，OrderedDict是一个记住元素首次插入顺序的词典，如果一个元素重写已经存在的元素，那么原始的插入位置保持不变，如果删除一个元素再重新插入，那么它就在末尾。 使用 使用dict时，Key是无序的。在对dict做迭代时，我们无法确定Key的顺序。如果要保持Key的顺序，可以用OrderedDict，OrderedDict的Key会按照插入的顺序排列，不是Key本身排序 例子1 代码 # 例子1 from collections import OrderedDict odd0 = {'banana': 3, 'apple': 4} od1 = OrderedDict({'banana': 3, 'apple': 4}) od2 = OrderedDict({'apple': 4, 'banana': 3}) print(od1 == od2) print(od1 == odd0) # 例子2 from collections import OrderedDict od1 = OrderedDict({'banana': 3, 'apple': 4}) # OrderedDict.popitem(last=True)，popitem方法返回和删除一个(key,value)对，如果last=True，就以LIFO方式执行，否则以FIFO方式执行。 od1.popitem(False) print(od1) od1.pop('apple') # 这里必须提供key print(od1) 输出 False True OrderedDict([('apple', 4)]) OrderedDict() 例子2 # 例子3 from collections import OrderedDict od1 = OrderedDict({'banana': 3, 'apple': 4}) od1.move_to_end('banana') print(od1) # 例子4 # OrderedDict可以实现一个FIFO（先进先出）的dict，当容量超出限制时，先删除最早添加的Key from collections import OrderedDict class LastUpdatedOrderedDict(OrderedDict): def __init__(self, capacity): super(LastUpdatedOrderedDict, self).__init__() self._capacity = capacity def __setitem__(self, key, value): containsKey = 1 if key in self else 0 if len(self) - containsKey >= self._capacity: last = self.popitem(last=False) print('remove:', last) if containsKey: del self[key] print('set:', (key, value)) else: print('add:', (key, value)) OrderedDict.__setitem__(self, key, value) OrderedDict([('apple', 4), ('banana', 3)]) ChainMap 原始的映射对象被存放在一个列表中构成一个字典序列self.maps 在 ChainMap 中查询某个键时，会对原始的映射对象依次查询，直至找到这个键，若未找到，则默认引发 KeyError 异常。 在 ChainMap 中进行插入、更新、删除时，只会对原始映射中的第一个映射进行操作 ChainMap 对象除了 maps 属性外，还具有一个 parents 属性和 new_child(m=None) 方法。 parents 属性返回了一个不包含原始映射中的第一个映射的 ChainMap 对象，对应源码中 self.class(self.maps[1:]) ，其效果和 ChainMap(d.maps[1:]) 相同 new_child(m=None) 方法返回一个包含指定映射 m（未指定时，为空字典）及其他原有映射的 ChainMap 对象，其中指定映射位于底层 maps 列表首位，在其他原有映射之前。 # ChainMap 类是为了将多个映射快速的链接到一起，这样它们就可以作为一个单元处理。它通常比创建一个新字典和多次调用 update() 要快很多 from collections import ChainMap a = {'a': 1, 'b': 2} b = {'c': 3} c = ChainMap(a, b) # 如果有重复的key，以第一次为准 print(c) # 将数据以列表形式展现，实际上是将链接指向原来的实现 print(c.maps) print(c.maps[0]['b']) ChainMap({'a': 1, 'b': 2}, {'c': 3}) [{'a': 1, 'b': 2}, {'c': 3}] 2 # 简单应用 一个Python命令中，如果在命令行中输入参数则使用该参数，没有则从OS环境变量中获取，如果还没有再取自定义默认值 import os import argparse from collections import ChainMap defaults = {'color': 'red', 'user': 'guest'} parser = argparse.ArgumentParser() parser.add_argument('-u', '--user') parser.add_argument('-c', '--color') namespace = parser.parse_args() command_line_args = {k: v for k, v in vars(namespace).items() if v} combined = ChainMap(command_line_args, os.environ, defaults) print(combined['color']) print(combined['user']) red guest 使用 ChainMap 对象作为嵌套上下文 我们在上面提到了 new_child(m=None) 可用于创建子上下文，这个是一个非常便捷的方法，可以使用一个空字典或其他指定的映射来创建一个新的 ChainMap 对象。针对这个新对象的修改不会对原有的 ChainMap 对象（可以将其理解为底层的数据结构或者基础上下文）产生影响。 >>> d_1 = {'name': 'bob', 'age': 25} >>> d_2 = {'height': '175', 'weight': 120} >>> c = ChainMap(d_1, d_2) # 创建一个基础上下文 >>> c_nc = c.new_child() # 创建一个嵌套的子上下文 >>> c_nc['skill'] = 'Python' # 在子上下文环境中进行赋值 >>> c ChainMap({'name': 'bob', 'age': 25}, {'height': '175', 'weight': 120}) >>> c_nc ChainMap({'skill': 'Python'}, {'name': 'bob', 'age': 25}, {'height': '175', 'weight': 120}) >>> list(c_nc) ['height', 'weight', 'name', 'age', 'skill'] >>> dict(c_nc) {'height': '175', 'weight': 120, 'name': 'bob', 'age': 25, 'skill': 'Python'} 计数器Counter Ps: Counter仅支持Hashable对象进行统计 定义 假如你有一长串没有标点符号或大写字母的单词，你想要计算每个单词出现的次数。 你可以使用字典或defaultdict增加计数，但collections.Counter提供了一种更清晰，更方便的方法。 Counter是dict的子类，它使用0作为任何缺失元素的默认值，并且更容易计算对象的出现次数： from collections import Counter words = \"if there was there was but if there was not there was not\".split() # 或者 counts = Counter(if=2,there=4,was=4,not=2,but=1) # 传进元组 counts = Counter(words) print(counts) >>> Counter({'there': 4, 'was': 4, 'if': 2, 'not': 2, 'but': 1}) 遍历元素 遍历所有元素 # 当你将单词列表传递给Counter时，它会存储每个单词以及该单词在列表中出现的次数。 # 如果你好奇两个最常见的词是什么？只需使用.most_common（）： print(counts.most_common(2)) # 遍历所有元素 for k, v in counts.items(): print(k, v) # 遍历打印所有元素 print(sorted(counts.elements())) [('there', 4), ('was', 4)] if 2 there 4 was 4 but 1 not 2 ['but', 'if', 'if', 'not', 'not', 'there', 'there', 'there', 'there', 'was', 'was', 'was', 'was'] 更新元素 # update(增加元素) counts.update(\"if you are here\".split()) print(counts) # subtract(原来的元素减去新传入的元素) counts.subtract(['if']) # if的次数减一 print(counts) # 删除元素 counts['but'] = 0 del counts['but'] print(counts) Counter({'there': 4, 'was': 4, 'if': 3, 'not': 2, 'but': 1, 'you': 1, 'are': 1, 'here': 1}) Counter({'there': 4, 'was': 4, 'if': 2, 'not': 2, 'but': 1, 'you': 1, 'are': 1, 'here': 1}) Counter({'there': 4, 'was': 4, 'if': 2, 'not': 2, 'you': 1, 'are': 1, 'here': 1}) 类集合操作 c = Counter(a=3, b=1) d = Counter(a=1, b=2) print(c + d) # 相加 print(c - d) # 相减，如果小于等于0，删去 print(c & d) # 求最小 print(c | d) # 求最大 Counter({'a': 4, 'b': 3}) Counter({'a': 2}) Counter({'a': 1, 'b': 1}) Counter({'a': 3, 'b': 2}) 子类化UserDict 就创造自定义映射类型来说，以 UserDict 为基类，总比以普通的 dict 为基类要来得方便。 而更倾向于从 UserDict 而不是从 dict 继承的主要原因是，后者有时会在某些方法的实现上走一些捷径，导致我们不得不在它的子类中重写这些方法，但是 UserDict 就不会带来这些问题。 值得注意的地方是，UserDict 并不是 dict 的子类，但是 UserDict 有一个叫作 data 的属性，是 dict 的实例，这个属性实际上是 UserDict 最终存储数据的地方。 双向队列deque 定义 deque是栈和队列的一种广义实现，deque是\"double-end queue\"的简称；deque支持线程安全、有效内存地以近似O(1)的性能在deque的两端插入和删除元素，尽管list也支持相似的操作，但是它主要在固定长度操作上的优化，从而在pop(0)和insert(0,v)（会改变数据的位置和大小）上有O(n)的时间复杂度。 常用方法 deque支持如下方法， append(x)， 将x添加到deque的右侧； appendleft(x)， 将x添加到deque的左侧； clear()， 将deque中的元素全部删除，最后长度为0； count(x)， 返回deque中元素等于x的个数； extend(iterable)， 将可迭代变量iterable中的元素添加至deque的右侧； extendleft(iterable)， 将变量iterable中的元素添加至deque的左侧，往左侧添加序列的顺序与可迭代变量iterable中的元素相反； pop()， 移除和返回deque中最右侧的元素，如果没有元素，将会报出IndexError； popleft()， 移除和返回deque中最左侧的元素，如果没有元素，将会报出IndexError； remove(value)， 移除第一次出现的value，如果没有找到，报出ValueError； reverse()， 反转deque中的元素，并返回None； rotate(n)， 从右侧反转n步，如果n为负数，则从左侧反转，d.rotate(1)等于d.appendleft(d.pop())； maxlen， 只读的属性，deque的最大长度，如果无解，就返回None； 除了以上的方法之外，deque还支持迭代、序列化、len(d)、reversed(d)、copy.copy(d)、copy.deepcopy(d)，通过in操作符进行成员测试和下标索引，索引的时间复杂度是在两端是O(1)，在中间是O(n)，为了快速获取，可以使用list代替。 ​ index（查找某个元素的索引位置） insert（在指定位置插入元素） >>> from collections import deque >>> d = deque('ghi')# 新建一个deque，有三个元素 >>> for ele in d:# 遍历deque ... print ele.upper() ... ... G H I >>> d.append('j')# deque右侧添加一个元素 >>> d.appendleft('f')# deque左侧添加一个元素 >>> d# 打印deque deque(['f', 'g', 'h', 'i', 'j']) >>> d.pop()# 返回和移除最右侧元素 'j' >>> d.popleft()# 返回和移除最左侧元素 'f' >>> list(d)# 以列表形式展示出deque的内容 ['g', 'h', 'i'] >>> d[0]# 获取最左侧的元素 'g' >>> d[-1]# 获取最右侧的元素 'i' >>> list(reversed(d))# 以列表形式展示出倒序的deque的内容 ['i', 'h', 'g'] >>> 'h' in d# 在deque中搜索 True >>> d.extend('jkl')# 一次添加多个元素 >>> d deque(['g', 'h', 'i', 'j', 'k', 'l']) >>> d.rotate(1)# 往右侧翻转 >>> d deque(['l', 'g', 'h', 'i', 'j', 'k']) >>> d.rotate(-1)# 往左侧翻转 >>> d deque(['g', 'h', 'i', 'j', 'k', 'l']) >>> deque(reversed(d))# 以逆序新建一个deque deque(['l', 'k', 'j', 'i', 'h', 'g']) >>> d.clear()# 清空deque >>> d.pop()# 不能在空的deque上pop Traceback (most recent call last): File \"\", line 1, in IndexError: pop from an empty deque >>> d.extendleft('abc')# 以输入的逆序向左扩展 >>> d deque(['c', 'b', 'a']) # deque是为了高效实现插入和删除操作的双向列表，适合用于队列和栈： # 使用list存储数据时，按索引访问元素很快，但是插入和删除元素就很慢了，因为list是线性存储，数据量大的时候，插入和删除效率很低。 >>> from collections import deque >>> q = deque(['a', 'b', 'c']) >>> q.append('x') >>> q.appendleft('y') >>> q deque(['y', 'a', 'b', 'c', 'x']) # deque除了实现list的append()和pop()外，还支持appendleft()和popleft()，这样就可以非常高效地往头部添加或删除元素 # deque是线程安全的，有GIL保护 # append和popleft都是原子操作 multiprocessing：这个包实现了自己的Queue，它和queue.Queue类似，是设计给进程间通信使用的。同时还有个multiprocessing.JoinableQueue类型，可以让任务管理变得更方便。 asyncio：python3.4新提供的包，里面有Queue、LifoQueue、PriorityQueue和JoinableQueue，这些类受到queue和mulitiprocessing模块的影响，但是为异步编程里的任务管理提供了便利。 子类化UserDict UserDict 封装了字典对象，简化了字典子类化 UserList 封装了列表对象，简化了列表子类化 UserString 封装了列表对象，简化了字符串子类化 Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-12-01 00:57:11 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/9.logging模块.html":{"url":"chapters/9.logging模块.html","title":"logging模块","summary":"python日志文件配置使用方法","keywords":"","body":"logging 模块logging 流程Logger 使用logging 配置python代码配置文件字典配置(推荐)Handler 子类自定义 Logger可能的问题 python的日志模块logging Python Logging 指南 Python日志库logging总结-可能是目前为止将logging库总结的最好的一篇文章 python logging日志模块以及多进程日志 Python配置日志的几种方式 logging 模块 在部署项目时，不可能直接将所有的信息都输出到控制台中，我们可以将这些信息记录到日志文件中 这样不仅方便我们查看程序运行时的情况，也可以在项目出现故障时根据运行时产生的日志快速定位问题出现的位置 logging框架组成： Loggers: 日志，暴露函数给应用程序，基于日志记录器和过滤器级别决定哪些日志有效。 LogRecord ：日志记录器，将日志传到相应的处理器处理。 Handlers: 处理器, 将(日志记录器产生的)日志记录发送至合适的目的地。 常用类型有StreamHandler、FileHandler、NullHandler Filters: 过滤器, 提供了更好的粒度控制,它可以决定输出哪些日志记录。 Formatters: 格式化器, 指明了最终输出中日志记录的布局。 logging日志级别： 每个logger都有一个日志的级别。logging中定义了如下级别 Level Numeric value 解释 NOTSET 0 意指不设置 所以按照父logger级别来过滤日志 DEBUG 10 详细信息，通常仅在诊断问题时才有意义 INFO 20 确认事情按预期工作 WARNING 30 表明发生了意外情况，或表明在不久的将来出现了一些问题（例如 “磁盘空间不足”）。但是该软件仍在按预期工作 ERROR 40 由于更严重的问题，该软件无法执行某些功能 CRITICAL 50 严重错误，表明程序本身可能无法继续运行 注意事项： 但是当发生异常时，直接使用无参数的 debug()、info()、warning()、error()、critical() 方法并不能记录异常信息 需要设置 exc_info 参数为 True 才可以，或者使用 exception() 方法，还可以使用 log() 方法，但还要设置日志级别和 exc_info 参数 import logging logging.basicConfig(filename=\"test.log\", filemode=\"w\", format=\"%(asctime)s %(name)s:%(levelname)s:%(message)s\", datefmt=\"%d-%M-%Y %H:%M:%S\", level=logging.DEBUG) a = 5 b = 0 try: c = a / b except Exception as e: # 下面三种方式三选一，推荐使用第一种 logging.exception(\"Exception occurred\") logging.error(\"Exception occurred\", exc_info=True) logging.log(level=logging.DEBUG, msg=\"Exception occurred\", exc_info=True) logging 流程 判断 Logger 对象对于设置的级别是否可用，如果可用，则往下执行，否则，流程结束。 创建 LogRecord 对象，如果注册到 Logger 对象中的 Filter 对象过滤后返回 False，则不记录日志，流程结束，否则，则向下执行。 LogRecord 对象将 Handler 对象传入当前的 Logger 对象，（图中的子流程）如果 Handler 对象的日志级别大于设置的日志级别，再判断注册到 Handler 对象中的 Filter 对象过滤后是否返回 True 而放行输出日志信息，否则不放行，流程结束。 如果传入的 Handler 大于 Logger 中设置的级别，也即 Handler 有效，则往下执行，否则，流程结束。 判断这个 Logger 对象是否还有父 Logger 对象，如果没有（代表当前 Logger 对象是最顶层的 Logger 对象 root Logger），流程结束。否则将 Logger 对象设置为它的父 Logger 对象，重复上面的 3、4 两步，输出父类 Logger 对象中的日志输出，直到是 root Logger 为止。 Logger 使用 logging 配置 python代码 使用Python代码显式的创建loggers，handlers和formatters并分别调用它们的配置函数 通过简单方式进行配置，使用 basicConfig() 函数直接进行配置； 代码 Logger 对象和 Handler 对象都可以设置级别，而默认 Logger 对象级别为 30 ，也即 WARNING，默认 Handler 对象级别为 0，也即 NOTSET。 logging 模块这样设计是为了更好的灵活性，比如有时候我们既想在控制台中输出DEBUG 级别的日志，又想在文件中输出WARNING级别的日志。 可以只设置一个最低级别的 Logger 对象，两个不同级别的 Handler 对象，示例代码如下： import logging import logging.handlers logger = logging.getLogger(\"logger\") handler1 = logging.StreamHandler() handler2 = logging.FileHandler(filename=\"test.log\", encoding=\"utf-8\") logger.setLevel(logging.DEBUG) handler1.setLevel(logging.WARNING) handler2.setLevel(logging.DEBUG) formatter = logging.Formatter(\"%(asctime)s %(name)s %(levelname)s %(message)s\") handler1.setFormatter(formatter) handler2.setFormatter(formatter) logger.addHandler(handler1) logger.addHandler(handler2) # 分别为 10、30、30 # print(handler1.level) # print(handler2.level) # print(logger.level) logger.debug('This is a customer debug message') logger.info('This is an customer info message') logger.warning('This is a customer warning message') logger.error('This is an customer error message') logger.critical('This is a customer critical message') 或者 logging.basicConfig(filename=\"config.log\", filemode=\"w\", format=\"%(asctime)s-%(name)s-%(levelname)s-%(message)s\", level=logging.INFO) logging.basicConfig(level=log_level, format='%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s', datefmt='%a, %d %b %Y %H:%M:%S', filename='parser_result.log', filemode='w') 参数说明 参数名称 参数描述 filename 日志输出到文件的文件名 filemode 文件模式，r[+]、w[+]、a[+] format 日志输出的格式 datefat 日志附带日期时间的格式 style 格式占位符，默认为 \"%\" 和 “{}” level 设置日志输出级别 stream 定义输出流，用来初始化 StreamHandler 对象不能 filename 参数一起使用，否则会ValueError 异常 handles 定义处理器，用来创建 Handler 对象，不能和 filename 、stream 参数一起使用，否则也会抛出 ValueError 异常 输出 2018-05-06 11:05:34,486 - simple_logger - DEBUG - debug message. 2018-05-06 11:05:34,487 - simple_logger - INFO - info message. 2018-05-06 11:05:34,487 - simple_logger - WARNING - warning message. 2018-05-06 11:05:34,487 - simple_logger - ERROR - error message. 2018-05-06 11:05:34,487 - simple_logger - CRITICAL - critical message. 配置文件 创建一个日志配置文件，然后使用fileConfig()函数来读取该文件的内容 相对于第一种配置方式的优点在于，它将配置信息和代码分离了，这一方面降低了日志的维护成本，同时还使得非开放人员也能够很容易的修改日志配置 通过配置文件进行配置，使用fileConfig()函数读取配置文件 常见的配置文件有 ini 格式、yaml 格式、JSON 格式，或者从网络中获取都是可以的 配置文件logging.conf 配置文件中一定要包含loggers、handlers、formatters这些section，它们通过keys这个option来指定该配置文件中已经定义好的loggers、handlers和formatters，多个值之间用逗号分隔；另外loggers这个section中的keys一定要包含root这个值； loggers、handlers、formatters中所所指定的日志器、处理器和格式器都需要在下面单独的section中进行定义。section的命名规则为[logger_loggerName]、[handler_handlerName]、[formatter_formatterName]； 定义logger的section必须指定level和handlers这两个option，level的可取值为DEBUG、INFO、WARNING、ERROR、CRITICAL、NOTSET，其中NOTSET表示所有级别的日志消息都要记录，包括用户定义级别；handlers的值是以逗号分隔的handler名字列表，这里出现的handler必须出现在[handlers]这个section中，并且相应的handler必须在配置文件中有对应的section定义； 对于非root logger来说，除了level和handlers这两个option之外，还需要一些额外的option，其中qualname是必须提供的option，它表示在logger层级中的名字，在应用代码中通过这个名字得到logger；propagate是可选的，其默认值为1，表示消息将会传递给高层次logger的handler，通常我们需要指定其值为0，这个可以看下面的例子；另外，对于非root logger的level如果设置为NOTSET，系统将会查找高层次的logger来决定此logger的有效level； 定义handler的section中必须指定class和args这两个option，level和formatter为可选option；class表示用于创建handler的类名，args表示传递给class所指定的handler类初始化方法参数，它必须是一个元组(tuple)的形式，即便只有一个参数值也需要是一个元组的形式；level与logger中的level一样，而formatter指定的是该处理器所使用的格式器，这里指定的格式器名称必须出现在formatters这个section总，且在配置文件中必须要有这个formatter的section定义；如果不指定formatter则该handler将会以消息本身作为日志消息进行记录，而不添加额外的时间、日志器名称等信息； 定义formatter的section中的option都是可选的，其中包括format用于指定格式字符串，默认为消息字符串本身；datefmt用于指定asctime的时间格式，默认为\"%Y-%m-%d %H:%M:%S\"；class用于指定格式器类名，默认为logging.Formatter； 每一个logger或者handler或者formatter都有一个key名字，以logger为例，首先需要在[loggers]配置中加上key名字代表了这个logger 然后用[loggers_xxxx]其中xxxx为key名来具体配置这个logger，在log02中我配置了level和一个handler名，当然你可以配置多个hander 根据这个handler名再去 [handlers]里面去找具体handler的配置，以此类推 ############################################## [loggers] keys=root, log02 [logger_root] level=INFO handlers=handler01 [logger_log02] level=DEBUG handler=handler02 qualname=log02 ############################################## [handlers] keys=handler01,handler02 [handler_handler01] class=FileHandler level=INFO formatter=form01 args=('../log/cv_parser_gm_server.log',\"a\") [handler_handler02] class=StreamHandler level=NOTSET formatter=form01 args=(sys.stdout,) ############################################## [formatters] keys=form01,form02 [formatter_form01] format=%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(process)d %(message)s datefmt=[%Y-%m-%d %H:%M:%S] [formatter_form02] format=(message)s ############################################## 代码 该函数实际上是对configparser模块的封装，函数定义：该函数定义在logging.config模块下 logging.config.fileConfig(fname, defaults=None, disable_existing_loggers=True) 参数说明： fname：表示配置文件的文件名或文件对象； defaults：指定传给ConfigParser的默认值； disable_existing_loggers：这是一个布尔值，默认值为True(为了向后兼容)表示禁用已经存在的logger，除非它们或它们的祖先明确的出现在日志配置中；如果该值为False，则对已存在的loggers保持启动状态 import logging.config # 读取日志配置文件内容 logging.config.fileConfig(\"logging.conf\") # 创建一个日志器logger logger = logging.getLogger(\"simpleExample\") # 日志输出 logger.debug(\"debug message.\") logger.info(\"info message.\") logger.warning(\"warning message.\") logger.error(\"error message.\") logger.critical(\"critical message.\") 输出 2018-05-06 12:29:24,849 - simpleExample - DEBUG - debug message. 2018-05-06 12:29:24,849 - simpleExample - INFO - info message. 2018-05-06 12:29:24,849 - simpleExample - WARNING - warning message. 2018-05-06 12:29:24,849 - simpleExample - ERROR - error message. 2018-05-06 12:29:24,849 - simpleExample - CRITICAL - critical message. 配置文件test.yaml version: 1 formatters: simple: format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s' handlers: console: class: logging.StreamHandler level: DEBUG formatter: simple loggers: simpleExample: level: DEBUG handlers: [console] propagate: no root: level: DEBUG handlers: [console] 代码 import logging.config # 需要安装 pyymal 库 import yaml with open('test.yaml', 'r') as f: config = yaml.safe_load(f.read()) logging.config.dictConfig(config) logger = logging.getLogger(\"sampleLogger\") 字典配置(推荐) 创建一个包含配置信息的dict，然后把它传递给dictConfig()函数 通过配置字典进行配置，使用 dictConfig() 函数读取配置信息 logging.FileHandler: 文件handle, 多线程下安全 logging.handlers.RotatingFileHandler: 轮循文件handle, 多线程下安全, 可以限制文件大小, 设置历史日志数量 concurrent_log_handler.ConcurrentRotatingFileHandler: 多进程多线程下安全, 可以限制文件大小, 设置历史日志数量 pip install redis==4.2.2 pip install concurrent-log-handler==0.9.20 logging_config.py #!/usr/bin/env Python # -- coding: utf-8 -- \"\"\" @version: v0.1 @author: narutohyc @file: logger_config.py @Description: 日志配置字典 + 定义logger句柄供项目使用 Python日志库logging总结-可能是目前为止将logging库总结的最好的一篇文章: https://www.jianshu.com/p/7b5e4752932e @time: 2020/5/23 21:34 \"\"\" import logging import os from enum import Enum, unique from logging import config as logging_config from os.path import join # 获取日志目录 from basic_support.data_access.config.project_config import project_config base_path = project_config.PROJECT_PATH base_log_path = join(base_path, 'logs') # 文件目录不存在时, 创建该目录 if not os.path.exists(base_log_path): os.makedirs(base_log_path) @unique class LogLevel(Enum): \"\"\" 日志等级枚举类 \"\"\" CRITICAL = '致命' # 严重错误,表明程序本身可能无法继续运行 ERROR = '错误' # 由于更严重的问题,该软件无法执行某些功能 WARNING = '警告' # 表明发生了意外情况,或表明在不久的将来出现了一些问题 (例如 '磁盘空间不足')。但是该软件仍在按预期工作 INFO = '普通' # 确认事情按预期工作 DEBUG = '详细' # 详细信息,通常仅在诊断问题时才有意义 NOTSET = '不设置' # 意指不设置,所以按照父logger级别来过滤日志 # 日志相关配置 LOGGING_CONFIG = { 'version': 1, 'loggers': { # 日志，暴露函数给应用程序，基于日志记录器和过滤器级别决定哪些日志有效 '': { # root logger 'level': LogLevel.INFO.name, # 日志等级 'handlers': ['console_handler', 'info_file_handler', 'error_file_handler'], } }, 'handlers': { # 处理器, 将(日志记录器产生的)日志记录发送至合适的目的地 'console_handler': { 'level': LogLevel.INFO.name, # 控制台日志等级 和 最终等级=max(当前等级,loggers) 'formatter': 'info', 'class': 'logging.StreamHandler', # 日志类 'stream': 'ext://sys.stdout', # 日志流 }, 'info_file_handler': { 'level': LogLevel.INFO.name, # 信息日志等级 'formatter': 'info', 'class': \"concurrent_log_handler.ConcurrentRotatingFileHandler\", # 多进程下多线程安全 'filename': os.path.join(base_log_path, 'info.log'), # 信息日志文件输出目录 'mode': 'a+', # 日志文件模型 a表示追加 w是覆盖写 'encoding': 'utf-8', 'backupCount': 4, # 4 = 自己+历史的3个 'maxBytes': 1024 * 1024 * 50, # 单个日志文件大小限制在 50MB内 'use_gzip': False, }, 'error_file_handler': { 'level': LogLevel.WARNING.name, # 错误日志等级 'formatter': 'error', 'class': 'logging.FileHandler', # 'class': 'logging.handlers.RotatingFileHandler', 'filename': os.path.join(base_log_path, 'error.log'), # 错误日志文件输出目录 'mode': 'a+', 'encoding': 'utf-8' } }, 'formatters': { # 格式化器, 指明了最终输出中日志记录的布局 'info': { 'format': '%(asctime)s %(module)s:%(lineno)d %(levelname)s: %(message)s', # 日志输出格式化 'datefmt': '%Y-%m-%d %H:%M:%S' # 日期格式化 }, 'error': { # 'format': '%(asctime)s-%(levelname)s-%(name)s-%(process)d::%(module)s|%(lineno)s:: %(message)s', 'format': '%(asctime)s %(module)s|%(lineno)s %(levelname)s|%(process)d: %(message)s', 'datefmt': '%Y-%m-%d %H:%M:%S' }, }, } # 获取日志实例 logging_config.dictConfig(LOGGING_CONFIG) logger = logging.getLogger(__name__) 使用 from comm.config.logger_config import logger # 日志输出 logger.debug(\"debug message.\") logger.info(\"info message.\") logger.warning(\"warning message.\") logger.error(\"error message.\") logger.critical(\"critical message.\") 输出 2020-05-24 10:29:30 text_augmentation_script DEBUG: debug message. 2020-05-24 10:29:30 text_augmentation_script INFO: info message. 2020-05-24 10:29:30 text_augmentation_script WARNING: warning message. 2020-05-24 10:29:30 text_augmentation_script ERROR: error message. 2020-05-24 10:29:30 text_augmentation_script CRITICAL: critical message. Handler 子类 StreamHandler 实例将消息发送到流（类文件对象）。 FileHandler 实例将消息发送到磁盘文件。 BaseRotatingHandler 是在某个点切割日志文件的处理器的基类。它并不意味着直接实例化。而是使用 RotatingFileHandler 或 TimedRotatingFileHandler。 RotatingFileHandler 实例将消息发送到磁盘文件，支持最大日志文件大小和日志文件切割。 TimedRotatingFileHandler 实例将消息发送到磁盘文件，以特定的时间间隔切割日志文件。 SocketHandler 实例将消息发送到 TCP/IP 套接字。从 3.4 开始，也支持 Unix 域套接字。 DatagramHandler 实例将消息发送到 UDP 套接字。从 3.4 开始，也支持 Unix 域套接字。 SMTPHandler 实例将消息发送到指定的电子邮件地址。 SysLogHandler 实例将消息发送到 Unix syslog 守护程序，可以是在远程计算机上。 NTEventLogHandler 实例将消息发送到 Windows NT/2000/XP 事件日志。 MemoryHandler 实例将消息发送到内存中的缓冲区，只要满足特定条件，就会刷新内存中的缓冲区。 HTTPHandler 实例使用 GET 或 POST 语义将消息发送到 HTTP 服务器。 WatchedFileHandler 实例监视他们要记录的文件。如果文件发生更改，则会关闭该文件并使用文件名重新打开。此处理程序仅在类 Unix 系统上有用; Windows 不支持使用的基础机制。 QueueHandler 实例将消息发送到队列，例如队列或多处理模块中实现的队列。 NullHandler 实例不会对错误消息执行任何操作。 NullHandler ，StreamHandler 和 FileHandler 类在核心日志包中定义。其他处理程序在子模块 logging.handlers 中定义。（还有另一个子模块 logging.config，用于配置功能。） 日志文件按照时间划分或者按照大小划分 如果将日志保存在一个文件中，那么时间一长，或者日志一多，单个日志文件就会很大，既不利于备份，也不利于查看。我们会想到能不能按照时间或者大小对日志文件进行划分呢？答案肯定是可以的，并且还很简单，logging 考虑到了我们这个需求。 logging.handlers 文件中提供了 TimedRotatingFileHandler 和 RotatingFileHandler 类分别可以实现按时间和大小划分。打开这个 handles 文件，可以看到还有其他功能的 Handler 类，它们都继承自基类 BaseRotatingHandler。 自定义 Logger 可能的问题 logging 库是线程安全的，但在多进程、多线程、多进程多线程环境中仍然还有值得考虑的问题，比如，如何将日志按照进程（或线程）划分为不同的日志文件，也即一个进程（或线程）对应一个文件。 可以使用多进程安全的日志类concurrent-log-handler Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-12-01 00:57:11 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/17.numpy小记.html":{"url":"chapters/17.numpy小记.html","title":"numpy小记","keywords":"","body":"numpy小记移动平均值第n个重复项的索引按列排序二维数组pearsonr相关系数唯一值的数量多个条件过滤numpy数组堆叠数组操作提取范围内的所有数字交换数组行列反转2维数组行列数组打印标准化一个数组至0到1之间缺失值相关数组交差集数组产生元素查找 numpy小记 70个NumPy分级练习题：用Python一举搞定机器学习矩阵运算 import numpy as np # 1. 替换满足条件的元素而不影响原始数组 arr = np.arange(10) out = np.where(arr%2==1, -1, arr) arr Out[4]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) out Out[5]: array([ 0, -1, 2, -1, 4, -1, 6, -1, 8, -1]) # 7. 获取两个数组元素匹配的索引号 a = np.array([1,2,3,2,3,4,3,4,5,6]) b = np.array([7,2,10,2,7,4,9,4,9,8]) np.where(a==b) Out[33]: (array([1, 3, 5, 7], dtype=int64),) # 9. 将处理标量的python函数在numpy数组上运行 maxx = lambda x,y: x if x>=y else y pair_max = np.vectorize(maxx, otypes=[float]) a = np.array([5,7,9,8,6,4,5]) b = np.array([6,3,4,8,9,7,1]) pair_max(a,b) Out[45]: array([6., 7., 9., 8., 9., 7., 5.]) # 18. 导入含有数字和文本的数据集，并保持的文本完整性 url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\" irirs_ld = np.genfromtxt(url, delimiter=',', dtype=None) # 21. 计算softmax值 def softmax(x): ex = np.exp(x-np.max(x)) return ex/ex.sum(axis=0) # 29. 概率抽样 # 暂定 # 33. 两个数组之间的欧氏距离 arr = np.arange(5) brr = np.arange(4,9) dist = np.linalg.norm(arr-brr) dist Out[176]: 8.94427190999916 # 38. 不连续的日期数组。通过填补缺失的日期，使其成为连续的日期序列 # 39. 一维数组arr，使用步长生成一个二维数组，窗口长度为4，步长为2 def gen_strides(arr, stride_len=5, windows_len=5): n_strides = ((arr.size-windows_len)//stride_len)+1 return np.array([arr[s:(s+windows_len)] for s in np.arange(0, n_strides*stride_len,stride_len)]) gen_strides(np.arange(15), stride_len=2, windows_len=4) Out[207]: array([[ 0, 1, 2, 3], [ 2, 3, 4, 5], [ 4, 5, 6, 7], [ 6, 7, 8, 9], [ 8, 9, 10, 11], [10, 11, 12, 13]]) 移动平均值 cumsum ：计算轴向元素累加和，返回由中间结果组成的数组 cumprod ：计算轴向元素累乘积，返回由中间结果组成的数组 data = np.array([1] * 10) acum = data.cumsum() cump = acum.cumprod() print(acum) [ 1 2 3 4 5 6 7 8 9 10] print(cump) array([ 1, 2, 6, 24, 120, 720, 5040, 40320, 362880, 3628800]) # 37. 窗口大小为3的移动平均值 def moving_average(a, n=3): ret = np.cumsum(a, dtype=float) ret[n:] = ret[n:]-ret[:-n] return ret[n-1:]/n np.random.seed(100) Z = np.random.randint(10, size=10) Z Out[191]: array([8, 8, 3, 7, 7, 0, 4, 2, 5, 2]) moving_average(Z, n=3).round(2) Out[192]: array([6.33, 6. , 5.67, 4.67, 3.67, 2. , 3.67, 3. ]) 第n个重复项的索引 # 36. 第n个重复项的索引 # 找出x中第1个重复n次的索引 arr = np.array([1,2,1,1,3,4,3,1,1,2,1,1,2]) n = 5 [ii for ii,kk in enumerate(arr) if kk==1][n-1] Out[186]: 8 np.where(arr==1)[0][n-1] Out[187]: 8 按列排序二维数组 # 31. 按列排序二维数组 arr = np.random.uniform(size=(3,4)) arr Out[146]: array([[0.54040458, 0.29679375, 0.1107879 , 0.3126403 ], [0.45697913, 0.65894007, 0.25425752, 0.64110126], [0.20012361, 0.65762481, 0.77828922, 0.7795984 ]]) arr[arr[:,1].argsort()] Out[147]: array([[0.54040458, 0.29679375, 0.1107879 , 0.3126403 ], [0.20012361, 0.65762481, 0.77828922, 0.7795984 ], [0.45697913, 0.65894007, 0.25425752, 0.64110126]]) pearsonr相关系数 # 25. 计算某两列的pearsonr相关系数 arr = np.random.uniform(size=(3,4)) arr Out[114]: array([[0.59884338, 0.60380454, 0.10514769, 0.38194344], [0.03647606, 0.89041156, 0.98092086, 0.05994199], [0.89054594, 0.5769015 , 0.74247969, 0.63018394]]) from scipy.stats.stats import pearsonr corr,p_value = pearsonr(arr[:,0], arr[:,2]) corr Out[117]: -0.432641494562719 唯一值的数量 # 27. 查找numpy数组中的唯一值的数量 arr = np.random.uniform(size=(3,4)) np.unique(arr, return_counts=True) Out[121]: (array([0.04486228, 0.14260031, 0.17808099, 0.23769421, 0.34019022, 0.35479561, 0.37625245, 0.50543143, 0.5928054 , 0.62994188, 0.9338413 , 0.94637988]), array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64)) # 28. 将数值转换为分类（文本）数组 # 将iris_2d的花瓣长度（第3列）组成一个文本数组 # '小' # 3-5 -> '中' # >=5 -> '大 from sklearn import datasets iris = datasets.load_iris().data petal_length_bin = np.digitize(iris[:,2].astype('float'), [0,3,5,10]) label_map = {1:'small', 2:'medium', 3:'large', 4:np.nan} petal_length_cat = [label_map[x] for x in petal_length_bin] petal_length_cat[:4] Out[130]: ['small', 'small', 'small', 'small'] 多个条件过滤numpy数组 # 24. 根据两个或多个条件过滤一个numpy数组 # 过滤具有arr（第3列）> 0.5和arr（第1列）0.5) & (arr[:,0] 堆叠数组操作 # 2. 垂直堆叠两个数组 a = np.arange(10).reshape(2,-1) b = np.repeat(1,10).reshape(2,-1) a Out[10]: array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]) b Out[11]: array([[1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]) np.concatenate([a,b], axis=0) Out[12]: array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]) np.vstack([a,b]) Out[13]: array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]) np.r_[a,b] Out[14]: array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]) # 3. 水平堆叠两个数组 np.concatenate([a,b], axis=1) Out[15]: array([[0, 1, 2, 3, 4, 1, 1, 1, 1, 1], [5, 6, 7, 8, 9, 1, 1, 1, 1, 1]]) np.hstack([a,b]) Out[16]: array([[0, 1, 2, 3, 4, 1, 1, 1, 1, 1], [5, 6, 7, 8, 9, 1, 1, 1, 1, 1]]) np.c_[a,b] Out[17]: array([[0, 1, 2, 3, 4, 1, 1, 1, 1, 1], [5, 6, 7, 8, 9, 1, 1, 1, 1, 1]]) 提取范围内的所有数字 # 8. 提取给定范围内的所有数字 a = np.arange(15) index = np.where((a>=5) & (a=5, a 交换数组行列 # 10. 交换2维numpy数组中的两个列 np.arange(9).reshape(3,3)[:,[1,0,2]] Out[46]: array([[1, 0, 2], [4, 3, 5], [7, 6, 8]]) # 11. 交换2维numpy数组中的两个行 np.arange(9).reshape(3,3)[[1,0,2], :] Out[47]: array([[3, 4, 5], [0, 1, 2], [6, 7, 8]]) 反转2维数组行列 # 12. 反转2维数组的行 np.arange(9).reshape(3,3)[[1,0,2], :] Out[51]: array([[3, 4, 5], [0, 1, 2], [6, 7, 8]]) # 13. 反转2维数组的列 np.arange(9).reshape(3,3)[:,::-1] Out[48]: array([[2, 1, 0], [5, 4, 3], [8, 7, 6]]) 数组打印 # 15. 打印三位小数的numpy数组 np.set_printoptions(3) np.random.uniform(5,10,(5,3)) Out[59]: array([[5.427, 6.768, 6.71 ], [7.243, 6.434, 7.314], [6.692, 8.417, 7.965], [8.718, 5.061, 5.936], [5.918, 8.064, 7.431]]) # 16. 使用科学记数法（如1e10）漂亮的打印数组rand_arr np.random.seed(100) np.set_printoptions(suppress=True) rand_arr = np.random.random([3,3])/1e3 rand_arr Out[68]: array([[0.000543, 0.000278, 0.000425], [0.000845, 0.000005, 0.000122], [0.000671, 0.000826, 0.000137]]) # 17. 限制数组输出中打印元素的数量 np.set_printoptions(threshold=6) arr = np.arange(15) arr Out[73]: array([ 0, 1, 2, ..., 12, 13, 14]) 标准化一个数组至0到1之间 # 20. 标准化一个数组至0到1之间 arr = np.random.uniform(size=(3,4)) arr Out[89]: array([[0.88959857, 0.64348368, 0.66692863, 0.41213651], [0.78437761, 0.75333385, 0.10293507, 0.7942196 ], [0.82432451, 0.11447181, 0.30328525, 0.41767367]]) s_max,s_min = arr.max(axis=0),arr.min(axis=0) (arr-s_min)/(s_max-s_min) Out[91]: array([[1. , 0.82805338, 1. , 0. ], [0. , 1. , 0. , 1. ], [0.37964781, 0. , 0.35523487, 0.01449204]]) 缺失值相关 # 26. 找出数组是否有缺失的值 arr = np.random.uniform(size=(3,4)) np.isnan(arr).any() Out[119]: False # 34. 删除所有nan值 arr = np.array([1,2,3,np.nan,5,6,7,np.nan]) arr[~np.isnan(arr)] Out[167]: array([1., 2., 3., 5., 6., 7.]) # 23. 数据集的5个随机位插入np.nan值 arr = np.random.uniform(size=(3,4)) i,j = np.where(arr) np.random.seed(100) arr[np.random.choice((i),5), np.random.choice((j),5)] = np.nan arr Out[103]: array([[ nan, 0.57138137, 0.27694632, 0.06905881], [0.25886229, nan, nan, 0.0125819 ], [ nan, 0.06872554, nan, 0.28179248]]) np.where(np.isnan(arr)) Out[105]: (array([0, 1, 1, 2, 2], dtype=int64), array([0, 1, 2, 0, 2], dtype=int64)) 数组交差集 # 5. 数组之间的共同元素 a = np.array([1,2,3,2,3,4,3,4,5,6]) b = np.array([7,2,10,2,7,4,9,4,9,8]) np.intersect1d(a,b) Out[27]: array([2, 4]) # 6. 删除存在于另一个数组中的元素 a = np.array([1,2,3,4,5]) b = np.array([5,6,7,8,9]) np.setdiff1d(a,b) Out[30]: array([1, 2, 3, 4]) 数组产生 # 4. 生成自定义序列 a = np.array([1,2,3]) np.r_[np.repeat(a,3), np.tile(a,3)].reshape(2,-1) Out[24]: array([[1, 1, 1, 2, 2, 2, 3, 3, 3], [1, 2, 3, 1, 2, 3, 1, 2, 3]]) # 14. 二维数组，以包含5到10之间的随机浮点数 rand_arr = np.random.randint(low=5, high=10, size=(5,3)) + np.random.random((5, 3)) rand_arr Out[56]: array([[5.37441948, 8.61709329, 8.24544657], [6.79517054, 6.43953972, 5.01129723], [9.662239 , 6.24575781, 6.51341089], [8.01997704, 6.66118956, 8.18283188], [8.02228762, 6.9609364 , 7.4676691 ]]) rand_arr = np.random.uniform(5,10, size=(5,3)) rand_arr Out[58]: array([[7.13046251, 7.4977751 , 8.55210656], [7.9222628 , 5.7828763 , 9.57743131], [9.13648012, 5.71533963, 9.18622945], [5.94241285, 8.08918632, 9.32716147], [5.75027771, 8.83210586, 8.3991677 ]]) 元素查找 # 19. 从一维元组数组中提取特定的列 iris_2d = np.array([row.tolist()[:4] for row in irirs_ld]) iris_2d[:4] Out[78]: array([[5.1, 3.5, 1.4, 0.2], [4.9, 3. , 1.4, 0.2], [4.7, 3.2, 1.3, 0.2], [4.6, 3.1, 1.5, 0.2]]) # 22. 找到数组的百分位的值 # 第5位和第95百分位的值 arr = np.random.uniform(size=(3,4)) np.percentile(arr, q=[5,95]) Out[94]: array([0.31394255, 0.87443495]) # 30. 获得数组中第二大的元素值 arr = np.random.uniform(size=(3,4)) np.unique(np.sort(arr))[-2] Out[132]: 0.9570126003527981 # 32. 首次出现的值大于给定值的位置 arr = np.random.uniform(size=(3,4)) arr Out[150]: array([[0.61032815, 0.30900035, 0.69773491, 0.8596183 ], [0.62532376, 0.98240783, 0.97650013, 0.16669413], [0.02317814, 0.16074455, 0.92349683, 0.95354985]]) np.argmax(arr[:,2].astype('float') > 0.7) Out[151]: 1 # 35. 一维数组所有局部最大值（或峰值） arr = np.array([1,3,7,1,2,6,0,1]) doublediff = np.diff(np.sign(np.diff(arr))) peak_locations = np.where(doublediff==-2)[0]+1 peak_locations Out[180]: array([2, 5], dtype=int64) Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-12-01 00:57:11 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/16.pandas小记.html":{"url":"chapters/16.pandas小记.html","title":"pandas小记","keywords":"","body":"pandas学习 pandas学习 Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-12-01 00:57:11 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/18.python_pipe包管道包学习.html":{"url":"chapters/18.python_pipe包管道包学习.html","title":"python_pipe包管道包学习","keywords":"","body":"python pipe包管道包学习构造你自己的pipe管道函数删除重复值groupby() 对列表实现分组计算求和和平均数select函数，相当于map映射操作聚合函数筛选和过滤take前几个元素和count生成器的长度flatmap操作两个列表实现zip函数指定函数排序并选出max索引截取数组元素any()和all()实现 python pipe包管道包学习 参考: https://github.com/JulienPalard/Pipe from pipe import * # 此处不是太理解 [1, 2, 3, 4, 5] | tee | as_list 1 2 3 4 5 Out[32]: [1, 2, 3, 4, 5] # 将生成器转换成list列表 [1, 2, 3, 4, 5, 6] | as_list Out[34]: [1, 2, 3, 4, 5, 6] # Like Python's built-in \"reversed\" primitive. [1, 2, 3] | reverse | concat Out[52]: '3, 2, 1' # Returns index of value in iterable 检索值 # 输入value start stop [1,2,3,2,1] | index(value=2,start=2,stop=4) Out[53]: 3 [1,2,3,2,1] | index(3) Out[54]: 2 构造你自己的pipe管道函数 # 构造你自己的pipe管道函数 stdout = Pipe(lambda x: sys.stdout.write(str(x))) select = Pipe(lambda iterable, pred: (pred(x) for x in iterable)) # 或者 @Pipe def stdout(x): sys.stdout.write(str(x)) 删除重复值 # dedup() Deduplicate values删除重复的值 [1,1,2,2,3,3,1,2,3] | dedup |as_list Out[50]: [1, 2, 3] # 删除连续的且重复的值 [1,1,2,2,3,3,1,2,3] | uniq | as_list Out[51]: [1, 2, 3, 1, 2, 3] groupby() 对列表实现分组计算 # groupby() 对列表实现分组计算 (1, 2, 3, 4, 5, 6, 7, 8, 9) | groupby(lambda x: x % 2 and \"Even\" or \"Odd\") | select(lambda x: \"%s : %s\" % (x[0], (x[1] | concat(', ')))) | concat(' / ') Out[46]: 'Even : 1, 3, 5, 7, 9 / Odd : 2, 4, 6, 8' [5, -4, 3, -2, 1] | sort(key=abs) | concat Out[49]: '1, -2, 3, -4, 5' 求和和平均数 # 加法 [1,2,3] | add Out[17]: 6 # 平均数，使用自定义函数，必须在函数前面加上@pipe装饰 [1, 2, 3] | select(lambda x: x * x) |average Out[30]: 4.666666666666667 # 平均数 [1, 2, 3, 4, 5, 6] | average Out[35]: 3.5 select函数，相当于map映射操作 # select函数，相当于map映射操作 [1,3,5] | select(lambda x:float(x +1)) |as_list Out[20]: [2.0, 4.0, 6.0] 聚合函数 # 聚合函数，可以使用lambda函数自定义 (1, 2, 3, 4, 5, 6, 7, 8, 9) | aggregate(lambda x, y: x + y) Out[21]: 45 筛选和过滤 # 筛选和过滤 [1, 2, 3] | where(lambda x: x % 2 == 0) | as_list Out[24]: [2] [1, 2, 3, 4] | take_while(lambda x: x take前几个元素和count生成器的长度 # take前几个元素和count生成器的长度 [1,2,3]|take(2) |as_list |where(lambda x : x ==2) |as_list |count Out[27]: 1 [1,2,3]|count Out[28]: 3 flatmap操作 # 相当于flatmap [[1, 2], [3, 4], [5]] | chain |as_list Out[31]: [1, 2, 3, 4, 5] 两个列表实现zip函数 # 两个列表实现zip函数 (1, 2, 3, 4, 5, 6, 7, 8, 9) | izip([9, 8, 7, 6, 5, 4, 3, 2, 1]) | concat Out[38]: '(1, 9), (2, 8), (3, 7), (4, 6), (5, 5), (6, 4), (7, 3), (8, 2), (9, 1)' 指定函数排序并选出max # max()按照key中的指定的函数来排序，然后筛选出max的函数 ('aa', 'b', 'fosdfdfo', 'qwerty', 'bar', 'zoog') | max(key=len) Out[43]: 'fosdfdfo' ('aa', 'b', 'foo', 'qwerty', 'bar', 'zoog') | max() Out[44]: 'zoog' ('aa', 'b', 'foo', 'qwerty', 'bar', 'zoog') | max Out[45]: 'zoog' 索引截取数组元素 # 使用islice截取从下表2到7,按照索引截取数组中的元素 (1, 2, 3, 4, 5, 6, 7, 8, 9) | islice(2, 8) | concat Out[36]: '3, 4, 5, 6, 7, 8' (1, 2, 3, 4, 5, 6, 7, 8, 9) | islice(0,1) |as_list Out[37]: [1] any()和all()实现 # any()，只要存在一个就返回true (1, 3, 5, 6, 7) | any(lambda x: x >= 7) Out[39]: True (1, 3, 5, 6, 7) | any(lambda x: x > 7) Out[40]: False # all(),必须全部满足才会返回true (1, 3, 5, 6, 7) | all(lambda x: x Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-12-01 00:57:11 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/7.python元编程.html":{"url":"chapters/7.python元编程.html","title":"python元编程","keywords":"","body":"动态属性属性描述符(get/set/delete)基本概念专有名词例子要点模拟ORM成绩管理成绩管理2.0成绩管理2.1进阶添加回调实现底层 @classmethod实现底层 @staticmethod实现底层 @property按需生成属性实例属性查找四个魔法函数__getattr____setattr____delattr____getattribute__要点例子元类基本概念元类的定义元类的本质小结验证子类注册子类获取__init__的默认参数注解类的属性ORM例子要点 动态属性 在 Python 中，数据的属性和处理数据的方法统称属性（attribute）。其实，方法只是可调用的属性。 属性描述符(get/set/delete) python 使用特性管理实例属性 (转)Python描述符（descriptor）解密 python理解描述符(descriptor) python 描述符总结 Python描述符 (descriptor) 详解 在不可散列的类中使用描述符-python python高级编程——描述符Descriptor详解（下篇） 基本概念 描述符是对多个属性运用相同存取逻辑的一种方式。例如，Django ORM 和 SQL Alchemy 等 ORM 中的字段类型是描述符，把数据库记录中字段里的数据与 Python 对象的属性对应起来。 为什么需要描述符：对property来说，最大的缺点就是它们不能重复使用。虽然property可以让类从外部看起来接口整洁漂亮，但是却做不到内部同样整洁漂亮。 描述符是property的升级版，允许你为重复的property逻辑编写单独的类来处理。 基本要求：描述符是实现了特定协议的类，这个协议包括 __get__、__set__ 和 __delete__ 方法。property 类实现了完整的描述符协议。通常，可以只实现部分协议。其实，我们在真实的代码中见到的大多数描述符只实现了 __get__ 和 __set__ 方法，还有很多只实现了其中的一个。 实现了 __get__、__set__ 或 __delete__ 方法的类是描述符。 用法：描述符的用法是，创建一个描述符类，它的实例对象作为另一个类的属性。 为了让描述符能够正常工作，它们必须定义在类的层次上。如果你不这么做，那么Python无法自动为你调用__get__和__set__方法。 大致流程： 定义一个描述符类D，其内包含一个或多个__get__()、__set__()、__delete__()方法 将描述符类D的实例对象d赋值给另一个要代理的类中某个属性attr，即attr=D() 之后访问、赋值、删除attr属性，将会自动触发描述符类中的__get__()、__set__()、__delete__()方法 实现： 要定义描述符类很简单，只要某个类中包含了下面一个或多个方法，就算是满足描述符协议，就是描述符类，就可以作为属性操作的代理器。 class Descriptor(): def __get__(self, instance, owner):... def __set__(self, instance, value):... def __delete__(self, instance):... 需要注意的是，__get__的返回值需要是属性值或抛异常，另外两个方法要返回None。 类属性描述符对象和实例属性同名时：描述符针对的是类属性，但是当一个类中，如果类属性是描述符对象，而实例属性由于这个描述符属性同名 class Person: character = CharacterDescriptor('乐观的') weight = WeightDescriptor(150) def __init__(self, character,weight): self.character = character self.weight = weight p = Person('悲观的', 200) print(p.character) #属性的访问 print(p.weight) # 从上面的运行结果可以看出，首先是访问了描述符的__set__方法，这是因为在构建对象的时候，相当于为character和weight赋值，然后再调用__get__方法，这是因为访问了类属性character和weight，但是最终打印出来值却并不是类属性的值，这是因为，实例属性实际上是在“描述符类属性”后面访问的，所以覆盖掉了。 专有名词 描述符类: 实现了描述符协议的类，描述符类的一些协议(__get__、__set__或__delete__ )。 实现了__get__、__set__、__delete__ 方法的类是描述符，只要实现了其中一个就是。 托管类： 将描述符实例作为类属性的类，比如Fruits 类，他有 weight、price 两个类属性，且都被赋予了描述符类的实例。 描述符实例: 描述符类创建出描述符实例，通常来讲，描述符类的实例会被赋给托管类的类属性。 托管实例: 托管类创建出来的实例 托管属性: 托管类中由描述符实例处理的公开属性 存储属性: 可以粗略的理解为、托管实例的属性、在上例中使用 vars(apple) 得到的结果中 price 和 weight 实例属性就是存储属性，它们实际存储着*实例的*属性值 非数据描述符：一个类，如果只定义了__get__() 或者是__delete__()方法，而没有定义__set__()方法，则认为是非数据描述符(即没有定义__set__) 数据描述符：一个类，不仅定义了__get__() 方法，还定义__set__(), __delete__() 方法，则认为是数据描述符(即定义了__get__和__set__) ps： 托管属性是类(Fruits)属性、存储属性是实例(apple)的属性。 > Quantity 实例是描述符，因此有个放大镜，用于获取值（__get__），以及一个手抓，用于设置值（__set__）。 例子 要点 定义位置：为了让描述符能够正常工作，它们必须定义在类的层次上。如果你不这么做，那么Python无法自动为你调用__get__和__set__方法。 独立实例：类使用了一个字典来单独保存专属于实例的数据。这个一般来说是没问题的，除非你用到了不可哈希（unhashable）的对象 不可哈希处理：list的子类是不可哈希的，因此它们不能为描述符类用做数据字典的key。有一些方法可以规避这个问题，但是都不完美。最好的方法可能就是给你的描述符加标签了。描述符可以安全的在这里存储数据。只是要记住，不要在别的地方也给这个描述符添加标签。这样的代码很脆弱也有很多微妙之处。但这个方法的确很普遍，可以用在不可哈希的所有者类上。 class Descriptor(object): def __init__(self, label): self.label = label def __get__(self, instance, owner): print('__get__', instance, owner) return instance.__dict__.get(self.label) def __set__(self, instance, value): print('__set__') instance.__dict__[self.label] = value class Foo(list): x = Descriptor('x') y = Descriptor('y') 泄漏内存问题：WeakKeyDictionary可以保证描述符类不会泄漏内存：WeakKeyDictionary的特殊之处在于：如果运行期系统发现这种字典所持有的引用，是整个程序里面指向Exam实例的最后一份引用，那么，系统就会自动将该实例从字典的键中移除。 模拟ORM 代码-描述符类 #!/usr/bin/env Python # -- coding: utf-8 -- import weakref import numbers class Field: pass class IntField(Field): # 数据描述符 def __init__(self, db_column, min_value=None, max_value=None): self.min_value = min_value self.max_value = max_value self.db_column = db_column if min_value is not None: if not isinstance(min_value, numbers.Integral): raise ValueError(\"min_value must be int\") elif min_value max_value: raise ValueError(\"min_value must be smaller than max_value\") self._value = weakref.WeakKeyDictionary() def __get__(self, instance, owner): if instance is None: return self return self._value.get(instance, 0) def __set__(self, instance, value): if not isinstance(value, numbers.Integral): raise ValueError(\"int value need\") if value self.max_value: raise ValueError(\"value must between min_value and max_value\") self._value[instance] = value class CharField(Field): def __init__(self, db_column, max_length=None): # self._value = None self.db_column = db_column if max_length is None: raise ValueError(\"you must spcify max_lenth for charfiled\") self.max_length = max_length self._value = weakref.WeakKeyDictionary() def __get__(self, instance, owner): if instance is None: return self return self._value.get(instance, '0') def __set__(self, instance, value): if not isinstance(value, str): raise ValueError(\"string value need\") if len(value) > self.max_length: raise ValueError(\"value len excess len of max_length\") self._value[instance] = value 代码-元类 class ModelMetaClass(type): def __new__(cls, name, bases, attrs, **kwargs): if name == \"BaseModel\": return super().__new__(cls, name, bases, attrs, **kwargs) fields = {} for key, value in attrs.items(): if isinstance(value, Field): fields[key] = value attrs_meta = attrs.get(\"Meta\", None) _meta = {} db_table = name.lower() if attrs_meta is not None: table = getattr(attrs_meta, \"db_table\", None) if table is not None: db_table = table _meta[\"db_table\"] = db_table attrs[\"_meta\"] = _meta attrs[\"fields\"] = fields del attrs[\"Meta\"] return super().__new__(cls, name, bases, attrs, **kwargs) class BaseModel(metaclass=ModelMetaClass): def __init__(self, *args, **kwargs): for key, value in kwargs.items(): setattr(self, key, value) return super(BaseModel, self).__init__() def save(self): fields = [] values = [] for key, value in self.fields.items(): db_column = value.db_column if db_column is None: db_column = key.lower() fields.append(db_column) value = getattr(self, key) values.append(str(value)) sql = \"insert {db_table}({fields}) value({values})\".format(db_table=self._meta[\"db_table\"], fields=\",\".join(fields), values=\",\".join(values)) print(sql) 代码-测试 class User(BaseModel): name = CharField(db_column=\"name\", max_length=10) age = IntField(db_column=\"age\", min_value=1, max_value=100) class Meta: db_table = \"user\" if __name__ == '__main__': first_user = User(name=\"bobby\", age=28) first_user.name = \"bobby\" first_user.age = 28 second_user = User(name=\"bobby\", age=23) print(first_user.name is second_user.name) second_user.name = 'okay' print(first_user.name is second_user.name) second_user.name = 'sec_boddy' print(first_user.name) print(second_user.name) print(first_user.name is second_user.name) 输出 True False bobby sec_boddy False 成绩管理 代码 #!/usr/bin/env Python # -- coding: utf-8 -- import weakref class Grade: def __init__(self): self._values = weakref.WeakKeyDictionary() def __get__(self, instance, owner): if instance is None: return self return self._values.get(instance, 0) def __set__(self, instance, value): if not (0 输出 2020-06-25 20:18:23 lazy_db INFO: first 82 2020-06-25 20:18:23 lazy_db INFO: second 75 成绩管理2.0 代码 class Grade: def __init__(self, name): self.name = name self.internal_name = '_' + self.name def __get__(self, instance, owner): if instance is None: return self return getattr(instance, self.internal_name, '') def __set__(self, instance, value): if not (0 输出 2020-06-25 20:18:23 lazy_db INFO: first 82 2020-06-25 20:18:23 lazy_db INFO: second 75 成绩管理2.1 代码 class Grade: def __init__(self): self.name = None self.internal_name = None def __get__(self, instance, owner): if instance is None: return self return getattr(instance, self.internal_name, '') def __set__(self, instance, value): if not (0 进阶 添加回调 描述符仅仅是类，也许你想要为它们增加一些方法。举个例子，描述符是一个用来回调property的很好的手段。比如我们想要一个类的某个部分的状态发生变化时就立刻通知我们。下面的大部分代码是用来做这个的： class CallbackProperty(object): \"\"\"A property that will alert observers when upon updates\"\"\" def __init__(self, default=None): self.data = WeakKeyDictionary() self.default = default self.callbacks = WeakKeyDictionary() def __get__(self, instance, owner): return self.data.get(instance, self.default) def __set__(self, instance, value): for callback in self.callbacks.get(instance, []): # alert callback function of new value callback(value) self.data[instance] = value def add_callback(self, instance, callback): \"\"\"Add a new function to call everytime the descriptor updates\"\"\" #but how do we get here?!?! if instance not in self.callbacks: self.callbacks[instance] = [] self.callbacks[instance].append(callback) class BankAccount(object): balance = CallbackProperty(0) def low_balance_warning(value): if value 这是一个很有吸引力的模式——我们可以自定义回调函数用来响应一个类中的状态变化，而且完全无需修改这个类的代码。这样做可真是替人分忧解难呀。现在，我们所要做的就是调用ba.balance.add_callback(ba, low_balance_warning)，以使得每次balance变化时low_balance_warning都会被调用。 但是我们是如何做到的呢？当我们试图访问它们时，描述符总是会调用__get__。就好像addcallback方法是无法触及的一样！其实关键在于利用了一种特殊的情况，即，当从类的层次访问时，`_get`方法的第一个参数是None。 class CallbackProperty(object): \"\"\"A property that will alert observers when upon updates\"\"\" def __init__(self, default=None): self.data = WeakKeyDictionary() self.default = default self.callbacks = WeakKeyDictionary() def __get__(self, instance, owner): if instance is None: return self return self.data.get(instance, self.default) def __set__(self, instance, value): for callback in self.callbacks.get(instance, []): # alert callback function of new value callback(value) self.data[instance] = value def add_callback(self, instance, callback): \"\"\"Add a new function to call everytime the descriptor within instance updates\"\"\" if instance not in self.callbacks: self.callbacks[instance] = [] self.callbacks[instance].append(callback) class BankAccount(object): balance = CallbackProperty(0) def low_balance_warning(value): if value 实现底层 @classmethod class NewDefine_classmethod: \"\"\" 使用“描述符”和“装饰器”结合起来，模拟@classmethod \"\"\" def __init__(self, function): self.function = function def __get__(self, instance, owner): #对传进函数进行加工,最后返回该函数 def wrapper(*args, **kwargs): #使用不定参数是为了匹配需要修饰的函数参数 print(\"给函数添加额外功能\") self.function(owner, *args, **kwargs) return wrapper class Person: name='我有姓名' def __init__(self): pass @NewDefine_classmethod def study_1(cls): print(f'我的名字是：{cls.name},我会搞学习！') @NewDefine_classmethod def study_2(cls,score): print(f'我的名字是：{cls.name},我会搞学习！,而且这次考试考了 {score} 分') print(Person.study_1()) print(Person.study_2(99)) 可以分这样几步分析： 第一步：@NewDefine_classmethod本质上是一个“类装饰器”，从它的定义可知，它的定义为 class NewDefine_classmethod(function).我们发现，python系统定义的@classmethod其实它的定义也是一样的，如下， class classmethod(function) .怎么样？它们二者的定义是不是一样？ 第二步：NewDefineclassmethod本质上又是一个描述符，因为在它的内部实现了\\_get__协议，由此可见，NewDefine_classmethod是“集装饰器-描述符”于一身的。 第三步：运行过程分析，因为study_1=NewDefine_classmethod（study_1）,所以，study_1本质上是一个NewDefine_classmethod的对象，又因为NewDefine_classmethod本质上是实现了描述符的，所以，study_1本质上是一个定义在类中的描述符属性。 第四步：因为study1本质上是一个定义在类中的描述符属性。所以在执行Person.study_1的时候，相当于是访问类的描述符属性，所以会进入到描述符的\\_get__方法。 现在是不是觉得原来python描述符还有这样神奇的使用呢？ 注意：如果修饰的函数本身是具有返回值的，在__get__里面所定义的wrapper里面一定要返回，即return self.function(owner, args, *kwargs)。 还有一个地方需要注意的是，因为这是自定义的底层实现，所以一些集成IDE可能会显示有语法错误，但是这没有关系，这正是python灵活多变的地方，运行并不会出现错误。 实现底层 @staticmethod staticmethod方法与classmethod方法的区别在于classmethod方法在使用需要传进一个类的引用作为参数。而staticmethod则不用。 class NewDefine_staticmethod: \"\"\" 使用“描述符”和“装饰器”结合起来，模拟@classmethod \"\"\" def __init__(self, function): self.function = function def __get__(self, instance, owner): #对传进函数进行加工,最后返回该函数 def wrapper(*args, **kwargs): #使用不定参数是为了匹配需要修饰的函数参数 print(\"给函数添加额外功能\") self.function(*args, **kwargs) return wrapper class Person: name='我有姓名' def __init__(self): pass @NewDefine_staticmethod def study_1(math,english): print(f'我数学考了 {math} 分,英语考了 {english} 分,我会搞学习！') @NewDefine_staticmethod def study_2(history,science): print(f'我历史考了 {history} 分,科学考了 {science} 分,我会搞学习！') print(Person.study_1(99,98)) print(Person.study_2(88,89)) 类方法classmethod必须第一个参数是cls，这个实际上就是判断所属的那个类，因此在__get__里面的function在调用的时候，第一个参数需要传递为owner，因为所属的“类cls等价于Person等价于owner”，但是因为静态方法不需要任何参数cls或者是self都不需要，因此在__get__实现的时候不能再传递owner参数，否则会显示参数错误。 实现底层 @property class NewDefine_property: \"\"\" 使用“描述符”和“装饰器”结合起来，模拟@classmethod \"\"\" def __init__(self, function): self.function = function def __get__(self, instance, owner): print(\"给函数添加额外功能\") return self.function(instance) class Person: name='我有姓名' def __init__(self): self.__study=100 @NewDefine_property def study_1(self): #使用property装饰的函数一般不要用“参数”，因为它的主要功能是对属性的封装 return self.__study p=Person() print(p.study_1) 基本思想和前面分析的还是一样的，但是有几个地方有所区别，需要注意： 第一：@property的目的是封装一个方法，是这个方法可以被当做属性访问 第二：调用的方式与前面有所不同，__get__里面不能再定义wrapper了，否则不会调用wrapper。得不到想要的结果，为什么呢？ 因为调用的方式不一样，根据前面的分析，study_1的本质是描述符属性，但是前面的调用均是使用的 Person.study_1()或者是p.study_1()的形式，还是当成方法去使用的。但是此处不一样了，直接就是当成属性去使用， p.study1 ，不再是方法调用，因此wrapper函数得不到调用。所以\\_get__方法得到了进一步简化。 按需生成属性 Python 魔法方法（三） __getattr__，__setattr__， __delattr__ Python高级用法之动态属性 使用__getattr__、__setattr__和__getattribute__来动态生成属性 Python 语言提供了一些挂钩，使得开发者很容易就能编写出通用的代码，以便将多个系统黏合起来。例如，我们要把数据库的行（row）表示为 Python 对象。由于数据库有自己的一套结构（schema），也称架构、模式、纲要、概要、大纲，所以在操作与行相对应的对象时，我们必须知道这个数据库的结构。然而，把 Python 对象与数据库相连接的这些代码，却不需要知道行的结构，所以，这部分代码应该写得通用一些。 那么，如何实现这种通用的代码呢？普通的实例属性、@property 方法和描述符，都不能完成此功能，因为它们都必须预先定义好，而像这样的动态行为，则可以通过 Python 的__getattr__特殊方法来做。如果某个类定义了__getattr__，同时系统在该类对象的实例字典中又找不到待查询的属性，那么，系统就会调用这个方法。 实例属性查找 python属性查找（attribute lookup） 首先需要明白的是实例属性查找的过程： 如果obj是某个类的实例，那么obj.name（以及等价的getattr(obj,'name')）首先调用__getattribute__。如果类定义了__getattr__方法，那么在__getattribute__抛出 AttributeError 的时候就会调用到__getattr__，而对于描述符(__get__）的调用，则是发生在__getattribute__内部的。官网文档是这么描述的 The implementation works through a precedence chain that gives data descriptors priority over instance variables, instance variables priority over non-data descriptors, and assigns lowest priority to __getattr__() if provided. obj = Clz(), 那么obj.attr 顺序如下： （1）如果“attr”是出现在Clz或其基类的__dict__中， 且attr是data descriptor， 那么调用其__get__方法, 否则 （2）如果“attr”出现在obj的__dict__中， 那么直接返回 obj.__dict__['attr']， 否则 （3）如果“attr”出现在Clz或其基类的__dict__中 （3.1）如果attr是non-data descriptor，那么调用其__get__方法， 否则 （3.2）返回 __dict__['attr'] （4）如果Clz有__getattr__方法，调用__getattr__方法，否则 （5）抛出AttributeError 程序每次访问对象的属性时，Python 系统都会调用这个特殊方法，即使属性字典里面已经有了该属性，也依然会触发 __getattribute__ 方法。这样就可以在程序每次访问属性时，检查全局事务状态。 按照 Python 处理缺失属性的标准流程，如果程序动态地访问了一个不应该有的属性，那么可以在 __getattr__ 和 __getattribute__ 里面抛出 AttributeError 异常。 实现通用的功能时，我们经常会在 Python 代码里使用内置的 hasattr 函数来判断对象是否已经拥有了相关的属性，并用内置的 __getattr__ 函数来获取属性值。这些函数会先在实例字典中搜索待查询的属性，然后再调用 __getattr__。 四个魔法函数 访问时机 如果某个类定义了__getattr__ ，同时系统在该类对象的实例字典中又找不到待查询的属性，那么，系统就会调用这个方法。 程序每次访问对象的属性时，Python 系统都会调用这个特殊方法，即使属性字典里面已经有了该属性，也依然会触发__getattribute__方法。这样就可以在程序每次访问属性 按照Python处理缺失属性的标准流程，如果程序动态地访问了一个不应该有的属性，那么可以在__getattr__ 和__getattribute__ 里面抛出AttributeError异常。 只要对实例的属性赋值，无论是直接赋值，还是通过内置的setattr函数赋值，都会触发__setattr__方法 。 __getattr__ 当我们访问一个不存在的属性的时候，会抛出异常，提示我们不存在这个属性。而这个异常就是__getattr__方法抛出的，其原因在于他是访问一个不存在的属性的最后落脚点，作为异常抛出的地方提示出错再适合不过了。 看例子，我们找一个存在的属性和不存在的属性。 class A(object): def __init__(self, value): self.value = value def __getattr__(self, item): print \"into __getattr__\" return \"can not find\" a = A(10) print a.value # 10 print a.name # into __getattr__ # can not find 可以看出，访问存在的属性时，会正常返回值，若该值不存在，则会进入最后的兜底函数__getattr__。 __setattr__ 在对一个属性设置值的时候，会调用到这个函数，每个设置值的方式都会进入这个方法。 class A(object): def __init__(self, value): print \"into __init__\" self.value = value def __setattr__(self, name, value): print \"into __setattr__\" if value == 10: print \"from __init__\" object.__setattr__(self, name, value) a = A(10) # into __init__ # into __setattr__ # from __init__ print a.value # 10 a.value = 100 # into __setattr__ print a.value # 100 在实例化的时候，会进行初始化，在__init__里，对value的属性值进行了设置，这时候会调用__setattr__方法。 在对a.value重新设置值100的时候，会再次进入__setattr__方法。 需要注意的地方是，在重写__setattr__方法的时候千万不要重复调用造成死循环。 class A(object): def __init__(self, value): self.value = value def __setattr__(self, name, value): self.name = value 这是个死循环。当我们实例化这个类的时候，会进入__init__，然后对value进行设置值，设置值会进入__setattr__方法，而__setattr__方法里面又有一个self.name=value设置值的操作，会再次调用自身__setattr__，造成死循环。 除了上面调用object类的__setattr__避开死循环，还可以如下重写__setattr__避开循环。 class A(object): def __init__(self, value): self.value = value def __setattr__(self, name, value): self.__dict__[name] = value a = A(10) print a.value # 10 __delattr__ __delattr__是个删除属性的方法 class A(object): def __init__(self, value): self.value = value def __delattr__(self, item): object.__delattr__(self, item) def __getattr__(self, item): return \"when can not find attribute into __getattr__\" a = A(10) print a.value # 10 del a.value print a.value # when can not find attribute into __getattr__ __delattr__也要避免死循环的问题，就如__setattr__一样，在重写__delattr__，避免重复调用。 __getattribute__ 使用__getattribute__对属性的访问做额外处理 假设我们需要在数据库中实现事物（transaction）处理，即每次在访问属性时，需要额外调用特殊方法检查数据库中对应的行是否有效，以及相关的事务是否依然开放。此时使用__getattr__无法实现这种功能，因为第二次访问属性时，Python会直接返回上首次调用时存储在__dict__中的属性值，而不会再次调用__getattr__插寻属性的状态。此种情况下我们需要使用__getattribute__，该方法在用户每次访问属性是都会被调用。 class LazyDB(object): def __init__(self): self.exist = 1 def __getattribute__(self, item): print('__getattribute__ (%s) called' % item) try: return super().__getattribute__(item) except AttributeError: value = ' '.join(['default value: ', item]) setattr(self, item, value) return value data = LazyDB() print(data.foo) ##每次访问类属性时都会被调用，此处是第1次调用 print(data.foo) ##每次访问类属性时都会被调用，此处是第2次调用 print(data.__dict__) ##每次访问类属性时都会被调用，此处是第3次待用 ###输出如下： __getattribute__ (foo) called default value: foo __getattribute__ (foo) called default value: foo __getattribute__ (__dict__) called {'exist': 1, 'foo': 'default value: foo'} 要点 __getattr__ 和 __setatr__，我们可以用惰性的方式来加载并保存对象的属性。 要理解 __getattr__ 与 __getattribute__ 的区别：前者只会在待访问的属性缺失时触发，而后者则会在每次访问属性时触发。 如果要在 __getattribute__ 和 __setattr__ 方法中访问实例属性，那么应该直接通过super()（也就是object类的同名方法）来做，以避免无限递归。 总结： （1）对于类装饰器属性，只要出现属性访问（不管是通过对象访问还是类名访问），都会优先调用装饰器的__get__方法; （2）对于类装饰器属性，若出现属性修改（不管是通过对象访问还是类名访问），都会优先调用装饰器的__set__方法; （3）对于类装饰器属性，若出现属性删除（不管是通过对象访问还是类名访问），都会优先调用装饰器的__delete__方法 例子 元类 第33/34条用元类核实和登记子类,3334,验证,注册 基本概念 类元编程是指在运行时创建或定制类的技艺。在 Python 中，类是一等对象，因此任何时候都可以使用函数新建类，而无需使用 class 关键字。类装饰器也是函数，不过能够审查、修改，甚至把被装饰的类替换成其他类。最后，元类是类元编程最高级的工具：使用元类可以创建具有某种特质的全新类种，例如我们见过的抽象基类。 元类的定义 Python定义元类时，需要从 type类中继承，然后重写 __new__方法，便可以实现意想不到的功能。 class Meta(type): def __new__(meta,name,bases,class_dict): #...各种逻辑实现1 cls = type.__new__(meta,name,bases,class_dict) print('当前类名',name) print('父类',bases) print('全部类属性',class_dict) #...各种逻辑实现2 return cls class MyClass(object,metaclass=Meta): stuff = 33 def foo(self): pass 当前类名 MyClass 父类 (,) 全部类属性 {'__module__': '__main__', '__qualname__': 'MyClass', 'stuff': 33, 'foo': } 元类可以获知那个类的名称、其所继承的父类，以及定义在class语句体中的全部类属性 元类的本质 在Python当中万物皆对象，我们用 class关键字定义的类本身也是一个对象， 负责产生该对象的类称之为元类 ，元类可以简称为类的类， 元类的主要目的是为了控制类的创建行为 。 type是Python的一个内建元类，用来直接控制生成类，在Python中任何 class定义的类其实都是 type类实例化的结果。 只有继承了 type类才能称之为一个元类，否则就是一个普通的自定义类，自定义元类可以控制类的产生过程，类的产生过程其实就是元类的调用过程。 小结 元类的各种操作可以实现类的验证和注册逻辑，均可以在元类的 __new__方法中实现，主要原因是当子类对象构建时，会先调用元类的 __new__方法，产生一个空对象，然后再调用子类的 __init__方法给对象属性进行赋值。 验证子类 元类是python比较高级的用法，简而言之，元类就是创建类的类。 而type就是一个元类，是用来创建类对象的类。 因此，要定义元类就要使其继承type类。 通常情况下，开发者在使用OOP的方式编程时，往往会用到__init__方法，即构造函数。 该方法会在类初始化时运行。但是我们可以将验证的时机提前，以至于提前到类创建之时，因此就会用到__new__方法。 class Base(type): def __new__(cls, name, param, dicts): print(cls) print(name) print(param) print(dicts) return super().__new__(cls, name, param, dicts) class Meta(metaclass=Base): name = 'yang' def person(self): pass Meta() Meta () {'__module__': '__main__', '__qualname__': 'Meta', 'name': 'yang', 'person': } 元类中所编写的验证逻辑，针对的是该基类的子类，而不是基类本身。 __new__()方法接收到的参数依次是： 当前准备创建的类的对象； 类的名字； 类继承的父类集合； 类的方法集合。 案例1 ：编写一个多边形类，当边数小于 3时，其类报错，实现其验证逻辑。 class ValidatePolygon(type): ## __new__当中放入验证逻辑 def __new__(meta,name,bases,class_dict): if bases!=(object,): ##针对子类而不对基类 if class_dict['sides'] 注册子类 元类还有一个用途，就是在程序中 自动注册类型。开发者每次从基类中继承子类时，基类的元类都可以自动运行注册代码。 案例2 ：实现对象的序列化与反序列化 ###建立类名与该类对象的映射关系，维护registry字典。 registry = {} def register_class(target_class): registry[target_class.__name__] = target_class def deserialize(data): params = json.loads(data) name = params['class'] target_class = registry[name] return target_class class Meta(type): def __new__(meta,name,bases,class_dict): cls = type.__new__(meta,name,bases,class_dict) register_class(cls) ##注册子类 return cls class BetterSerializable(object): def __init__(self,*args): self.args = args def serialize(self): return json.dumps({'class':self.__class__.__name__, 'args':self.args,}) def __repr__(self): pass class RegisterSerializabel(BetterSerializable,metaclass=Meta): pass 通过元类来实现类的注册，可以确保所有的子类都不会遗漏，从而避免后续的错误。 获取__init__的默认参数 获取__init__的默认参数，并在classmethod方法中为没有给定的属性赋默认值，提升代码的健壮性 元类定义： #!/usr/bin/env Python # -- coding: utf-8 -- \"\"\" @version: v0.1 @author: narutohyc @file: meta_interface.py @Description: @time: 2020/6/15 20:29 \"\"\" import collections from abc import (ABC, abstractmethod, ABCMeta) import inspect class DicMetaClass(ABCMeta): def __new__(cls, name, bases, attrs, **kwargs): if name == 'DicMeta': return super().__new__(cls, name, bases, attrs, **kwargs) # 获取__init__函数的 默认值 argspec = inspect.getfullargspec(attrs[\"__init__\"]) init_defaults = dict(zip(argspec.args[-len(argspec.defaults):], argspec.defaults)) cls.__init_defaults = init_defaults attrs['__init_defaults__'] = init_defaults return super().__new__(cls, name, bases, attrs, **kwargs) 抽象父类： #!/usr/bin/env Python # -- coding: utf-8 -- \"\"\" @version: v0.1 @author: narutohyc @file: meta_interface.py @Description: @time: 2020/6/15 20:29 \"\"\" from abc import (ABC, abstractmethod, ABCMeta) class DicMeta(ABC, metaclass=DicMetaClass): def __init__(self): pass @abstractmethod def to_dict(self): ''' 返回字典 ''' pass @classmethod def load_from_mapping(cls, mapping_datas): ''' 用字典来构建实例对象 ''' assert isinstance(mapping_datas, collections.abc.Mapping) obj = cls.__new__(cls) [setattr(obj, k, v) for k, v in mapping_datas.items()] return obj 子类实现： #!/usr/bin/env Python # -- coding: utf-8 -- \"\"\" @version: v0.1 @author: narutohyc @file: text_meta.py @Description: @time: 2020/5/22 14:55 \"\"\" from augmentation.meta_class.meta_interface import DicMeta from utils.utils_func import gen_md5, str2bool import re class TaskMeta(DicMeta): ''' 数据包装类的bean结构 ''' def __init__(self, text, doc_id, sentence_id, reg_lst, has_reg=True, flag=None, dataset='train', text_source=\"primitive\"): super(TaskMeta, self).__init__() self.text = text self.doc_id = doc_id self.sentence_id = sentence_id if reg_lst and isinstance(reg_lst[0], list): reg_lst = ['%s %s %s' % (tag, start_idx, value) for tag, start_idx, value in reg_lst] self.reg_lst = sorted(reg_lst, key=lambda reg: int(re.sub(' +', ' ', reg).split(\" \", 2)[1])) if reg_lst else [] self.flag = list(set(i.split(' ', 2)[0] for i in self.reg_lst)) if flag is None else flag self.has_reg = str2bool(has_reg) self.dataset = dataset self.text_source = text_source self._id = gen_md5(self.text) @classmethod def load_from_mapping(cls, mapping_datas): ''' 用字典来构建 TaskMeta实例 ''' obj = super(TaskMeta, cls).load_from_mapping(mapping_datas) obj._id = gen_md5(obj.text) [setattr(obj, k, v) for k, v in obj.__init_defaults__.items() if not hasattr(obj, k)] if obj.flag is None: obj.flag = list(set(i.split(' ', 2)[0] for i in obj.reg_lst)) obj.has_reg = str2bool(obj.has_reg) return obj @property def to_dict(self): ''' 当该类没有其他多余属性时 可以直接返回self.__dict__的副本 ''' return {\"text\": self.text, \"doc_id\": self.doc_id, \"sentence_id\": self.sentence_id, \"reg_lst\": self.reg_lst, \"flag\": list(self.flag), \"has_reg\": self.has_reg, \"dataset\": self.dataset, \"text_source\": self.text_source, \"_id\": self._id} 测试类： task_meta_0 = TaskMeta.load_from_mapping({'text': '斯坦福大学开发的基于条件随机场的命名实体识别系统，该系统参数是基于CoNLL、MUC-6、MUC-7和ACE命名实体语料训练出来的。', 'doc_id': 'id1', 'sentence_id': 'id1', 'reg_lst': ['学校 0 斯坦福大学', '标注 33 CoNLL', '标注 39 MUC-6', '标注 45 MUC-7', '标注 51 ACE']}) task_meta_1 = TaskMeta.load_from_mapping({'text': '斯坦福大学开发的基于条件随机场的命名实体识别系统，该系统参数是基于CoNLL、MUC-6、MUC-7和ACE命名实体语料训练出来的。', 'doc_id': 'id1', 'sentence_id': 'id1', 'reg_lst': ['学校 0 斯坦福大学', '标注 33 CoNLL', '标注 39 MUC-6', '标注 45 MUC-7', '标注 51 ACE'], 'flag': ['学校', '标注'], 'has_reg': True, 'dataset': 'train', 'text_source': 'primitive', '_id': '3b895befc659345be8686bd7de4d7693'}) task_meta_0.to_dict == task_meta_1.to_dict Out[33]: True 可以看出，taskmeta_0和task_meta_1两者的 值是完全相同的，这里就可以做到共享\\_init__默认参数的效果 注解类的属性 元类还有一个更有用处的功能，那就是可以在某个类刚定义好但是尚未使用的时候，提前修改或注解该类的属性。这种写法通常会与描述符(descriptor) 搭配起来(参见本书第31条)，令这些属性可以更加详细地了解自己在外围类中的使用方式。 例如，要定义新的类，用来表示客户数据库里的某- -行。同时，我们还希望在该类的相关属性与数据库表的每一列之间， 建立对应关系。于是，用下面这个描述符类，把属性与列名联系起来。 class Field: def __init__(self, name): self.name = name self.internal_name = '_' + self.name def __get__(self, instance, owner): if instance is None: return self return getattr(instance, self.internal_name, '') def __set__(self, instance, value): setattr(instance, self.internal_name, value) 由于列的名称已经保存到了Field描述符中，所以我们可以通过内置的setattr和getattr函数，把每个实例的所有状态都作为protected字段，存放在该实例的字典里面。 在本书前面的例子中，为了避免内存泄漏，我们曾经用weakref字典来构建描述符，而刚才的那段代码，目前看来，似乎要比weakref方案便捷得多。 接下来定义表示数据行的Customer类，定义该类的时候，我们要为每个类属性指定对应的列名。 class Customer: first_name = Field('first_name') last_name = Field('last_name') prefix = Field('prefix') suffix = Field('suffix') 问题在于，上面这种写法显得有些重复。在Customer类的class语句体中，我们既然要将构建好的Field对象赋给Customer.first name, 那为什么还要把这个字段名(本例中是'first name') 再传给Field的构造器呢? 之所以还要把字段名传给Field构造器，是因为定义Customer类的时候，Python 会以从右向左的顺序解读赋值语句，这与从左至右的阅读顺序恰好相反。首先，Python 会以Field(first name') 的形式来调用Field 构造器，然后，它把调用构造器所得的返回值，赋给Customer.field name。 从这个顺序来看，Field 对象没有办法提前知道自己会赋给Customer类里的哪一个属性。 为了消除这种重复代码，我们现在用元类来改写它。使用元类，就相当于直接在class语句上面放置挂钩，只要class语句体处理完毕，这个挂钩就会立刻触发。于是，我们可以借助元类，为Field描述符自动设置其Field.name和Field.internal_ name, 而不用再像刚才那样，把列的名称手工传给Field 构造器。 class Meta(type): def __new__(meta, name, bases, class_dict): for key, value in class_dict.items(): if isinstance(value, Field): value.name = key value.internal_name = '_' + key cls = type.__new__(meta, name, bases, class_dict) return cls 下面定义一一个基类，该基类使用刚才定义好的Meta作为其元类。凡是代表数据库里面某一行的类，都应该从这个基类中继承，以确保它们能够利用元类所提供的功能: class DatabaseRow(object, metaclass=Meta): pass 采用元类来实现这套方案时，Field 描述符类基本上是无需修改的。唯一 要调整的地方就在于:现在不需要再给构造器传人参数了，因为刚才编写的Meta.__new__ 方法会自动把相关的属性设置好。 class Field: def __init__(self): self.name = None self.internal_name = None 有了元类、新的DatabaseRow基类以及新的Field描述符之后，我们在为数据行定义DatabaseRow子类时，就不用再像原来那样，编写重复的代码了。 class BetterCustomer(DatabaseRow): first_name = Field() last_name = Field() prefix = Field() suffix = Field() ORM例子 ORM 是 python编程语言后端web框架 Django的核心思想，“Object Relational Mapping”，即对象-关系映射，简称ORM。 一个句话理解就是：创建一个实例对象，用创建它的类名当做数据表名，用创建它的类属性对应数据表的字段，当对这个实例对象操作时，能够对应MySQL语句 代码 class ModelMetaclass(type): def __new__(cls, name, bases, attrs): mappings = dict() for k, v in attrs.items(): if isinstance(v, tuple): print('Found mapping :%s ==> %s' % (k, v)) mappings[k] = v for k in mappings.keys(): attrs.pop(k) attrs['__mappings__'] = mappings attrs['__table__'] = name return type.__new__(cls, name, bases, attrs) class Model(object, metaclass=ModelMetaclass): def __init__(self, **kwargs): for name, value in kwargs.items(): setattr(self, name, value) def save(self): fields, args = [], [] for k, v in self.__mappings__.items(): fields.append(v[0]) args.append(getattr(self, k, None)) args_temp = list() for temp in args: if isinstance(temp, int): args_temp.append(str(temp)) elif isinstance(temp, str): args_temp.append(\"\"\"'%s'\"\"\" % temp) sql = 'insert into %s (%s) values (%s)' % (self.__table__, ','.join(fields), ','.join(args_temp)) print(sql) class User(Model): uid = ('uid', 'int unsigned') name = ('username', 'varchar(30)') email = ('email', 'varchar(30)') password = ('password', 'varchar(30)') user = User(uid=1234, name='naruto', email='1832044042@qq.mail', password='hycpass') user.save() 输出 Found mapping :uid ==> ('uid', 'int unsigned') Found mapping :name ==> ('username', 'varchar(30)') Found mapping :email ==> ('email', 'varchar(30)') Found mapping :password ==> ('password', 'varchar(30)') insert into User (uid,username,email,password) values (1234,'naruto','1832044042@qq.mail','hycpass') 要点 借助元类，我们可以在某个类完全定义好之前，率先修改该类的属性。 描述符与元类能够有效地组合起来，以便对某种行为做出修饰，或在程序运行时探查相关信息。 如果把元类与描述符相结合，那就可以在不使用weakref模块的前提下避免内存泄漏。 Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-12-01 00:57:11 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/3.python协程.html":{"url":"chapters/3.python协程.html","title":"python协程","keywords":"","body":"python迭代对象可迭代的对象迭代器规约函数生成器itertools模块无限迭代器有限迭代器组合生成器其他方法使用现有扩展功能自定义扩展python协程生成器和协程协程状态yield fromyield from 获取返回值greenlet 和 geventgreenletgeventasyncio.coroutineasync 和 await协程与异步应用生产者和消费者多进程、多线程与协程 python迭代对象 Coroutines Are Faster To Start Than Threads in Python 扫描内存中放不下的数据集时，我们要找到一种惰性获取数据项的方式，即按需一次获取一个数据项 这就是迭代器模式(Iterator pattern)，迭代是数据处理的基石 Python 2.2(2001 年)加入了 yield 关键字，这个关键字用于构建生成器(generator)，其作用与迭代器一样 所有生成器都是迭代器，因为生成器完全实现了迭代器接口 迭代器用于从集合中取出元素；而生成器用于\"凭空\"生成元素 Python 社区中，大多数时候都把迭代器和生成器视作同一概念 内置的 range() 函数也返回一个类似生成器的对象，而以前则返回完整的列表 如果一定要让 range() 函数返回列表，那么必须明确指明(例如，list(range(100))) 序列可以迭代的原因：iter函数 解释器需要迭代对象 x 时，会自动调用 iter(x) 内置的 iter 函数有以下作用 检查对象是否实现了 __iter__ 方法，如果实现了就调用它，获取一个迭代器 如果没有实现 __iter__ 方法，但是实现了 __getitem__ 方法，Python 会创建一个迭代器，尝试按顺序(从索引 0 开始)获取元素 如果尝试失败，Python 抛出 TypeError 异常，通常会提示\"C object is not iterable\"(C 对象不可迭代)，其中 C 是目标对象所属的类 任何 Python 序列都可迭代的原因是，它们都实现了 __getitem__ 方法 鸭子类型（duck typing）的极端形式 不仅要实现特殊的 __iter__ 方法，还要实现 __getitem__ 方法 而且 __getitem__ 方法的参数是从 0 开始的整数（int），这样才认为对象是可迭代的 白鹅类型（goose-typing）理论中，可迭代对象的定义简单一些，不过没那么灵活 如果实现了 __iter__ 方法，那么就认为对象是可迭代的 检查对象 x 能否迭代，最准确的方法是：调用 iter(x)函数，如果不可迭代，再处理 TypeError 异常 这比使用 isinstance(x, abc.Iterable) 更准确 因为 iter(x) 函数会考虑到遗留的 __getitem__ 方法，而 abc.Iterable 类则不考虑 可迭代的对象 使用 iter 内置函数可以获取迭代器的对象 如果对象实现了能返回迭代器的 __iter__ 方法，那么对象就是可迭代的。序列都可以迭代是实现了 __getitem__方法，而且其参数是从零开始的索引，这种对象也可以迭代 遍历方式 下面是一个简单的 for 循环，迭代一个字符串。这里，字符串 'ABC' 是可迭代的对象。背后是有迭代器的，只不过我们看不到： >>> s = 'ABC' >>> for char in s: ... print(char) ... A B C 如果没有 for 语句，不得不使用 while 循环模拟，要像下面这样写： >>> s = 'ABC' >>> it = iter(s) # ➊ >>> while True: ... try: ... print(next(it)) # ➋ ... except StopIteration: # ➌ ... del it # ➍ ... break # ➎ ... A B C ❶ 使用可迭代的对象构建迭代器 it。 ❷ 不断在迭代器上调用 next 函数，获取下一个字符。 ❸ 如果没有字符了，迭代器会抛出 StopIteration 异常。 ❹ 释放对 it 的引用，即废弃迭代器对象。 ❺ 退出循环。 StopIteration 异常表明迭代器到头了 Python 语言内部会处理 for 循环和其他迭代上下文(如列表推导、元组拆包等等)中的 StopIteration 异常 明确可迭代的对象和迭代器之间的关系：Python 从可迭代的对象中获取迭代器 迭代器 标准的迭代器接口有两个方法 __next__: 返回下一个可用的元素，如果没有元素了，抛出 StopIteration 异常 __iter__: 返回 self，以便在应该使用可迭代对象的地方使用迭代器，例如在 for 循环中 Iterator 抽象基类实现 __iter__ 方法的方式是返回实例本身(return self) 如果想再次迭代，要重新构建迭代器，在需要可迭代对象的地方可以使用迭代器 检查迭代器遗留对象 因为迭代器只需 __next__ 和 __iter__ 两个方法，所以除了调用 next() 方法，以及捕获 StopIteration 异常之外，没有办法检查是否还有遗留的元素 Python 中的迭代器还实现了 __iter__ 方法，因此迭代器也可以迭代 使用迭代器模式实现 Sentence 类 import re import reprlib RE_WORD = re.compile('\\w+') class Sentence: def __init__(self, text): self.text = text self.words = RE_WORD.findall(text) def __repr__(self): return 'Sentence(%s)' % reprlib.repr(self.text) def __iter__(self): ➊ return SentenceIterator(self.words) ➋ class SentenceIterator: def __init__(self, words): self.words = words ➌ self.index = 0 ➍ def __next__(self): try: word = self.words[self.index] ➎ except IndexError: raise StopIteration() ➏ self.index += 1 ➐ return word ➑ def __iter__(self): ➒ return self 对这个示例来说，其实没必要在 SentenceIterator 类中实现 __iter__方法，不过这么做是对的，因为迭代器应该实现 __next__ 和 __iter__ 两个方法，而且这么做能让迭代器通过 issubclass(SentenceIterator, abc.Iterator) 测试 可迭代的对象有个 __iter__ 方法，每次都实例化一个新的迭代器 而迭代器要实现 __next__ 方法，返回单个元素，此外还要实现 __iter__ 方法，返回迭代器本身 为了\"支持多种遍历\"，必须能从同一个可迭代的实例中获取多个独立的迭代器，而且各个迭代器要能维护自身的内部状态，因此这一模式正确的实现方式是，每次调用 iter(my_iterable) 都新建一个独立的迭代器。这就是为什么这个示例需要定义 SentenceIterator 类 规约函数 接受一个可迭代的对象，然后返回单个结果，这些函数叫归约函数 对 all 和 any 函数来说，有一项重要的优化措施是 reduce 函数做不到的，这两个函数会短路(即一旦确定了结果就立即停止使用迭代器) 模块 函数 说明 内置 all(it) it 中的所有元素都为真值时返回 True，否则返回 Falseall([]) 返回 True 内置 any(it) 只要 it 中有元素为真值就返回 True，否则返回 Falseany([]) 返回 False 内置 max(it, [key=,] [default=]) 返回 it 中值最大的元素；*key 是排序函数，与 sorted 函数中的一样如果可迭代的对象为空，返回 default 内置 min(it, [key=,] [default=]) 返回 it 中值最小的元素；#key 是排序函数，与 sorted 函数中的一样如果可迭代的对象为空，返回 default functools reduce(func, it, [initial]) 把前两个元素传给 func，然后把计算结果和第三个元素传给 func，以此类推，返回最后的结果如果提供了 initial，把它当作第一个元素传入 内置 sum(it, start=0) it 中所有元素的总和，如果提供可选的 start，会把它加上（计算浮点数的加法时，可以使用 math.fsum 函数提高精度） 在 Python 中迭代对象 x 时会调用 iter(x) iter 函数还有一个鲜为人知的用法：传入两个参数，使用常规的函数或任何可调用的对象创建迭代器 这样使用时，第一个参数必须是可调用的对象，用于不断调用(没有参数)，产出各个值；第二个值是哨符，这是个标记值，当可调用的对象返回这个值时，触发迭代器抛出 StopIteration 异常，而不产出哨符 生成器 只要 Python 函数的定义体中有 yield 关键字，该函数就是生成器函数 生成器函数会创建一个生成器对象，包装生成器函数的定义体 把生成器传给 next(...) 函数时，生成器函数会向前，执行函数定义体中的下一个 yield 语句 返回产出的值，并在函数定义体的当前位置暂停 最终，函数的定义体返回时，外层的生成器对象会抛出 StopIteration 异常——这一点与迭代器协议一致 如果一个类只是为了构建生成器而去实现 __iter__ 方法，那还不如使用生成器函数 惰性是好的特质，至少在编程语言和 API 中是如此，惰性实现是指尽可能延后生成值 这样做能节省内存，而且或许还可以避免做无用的处理 import re import reprlib RE_WORD = re.compile('\\w+') class Sentence: def __init__(self, text): self.text = text ➊ def __repr__(self): return 'Sentence(%s)' % reprlib.repr(self.text) def __iter__(self): for match in RE_WORD.finditer(self.text): ➋ yield match.group() ➌ ❶ 不再需要 words 列表。 ❷ finditer 函数构建一个迭代器，包含 self.text 中匹配 RE_WORD 的单词，产出 MatchObject 实例。 ❸ match.group() 方法从 MatchObject 实例中提取匹配正则表达式的具体文本 生成器表达式可以理解为列表推导的惰性版本：不会迫切地构建列表，而是返回一个生成器，按需惰性生成元素 也就是说，如果列表推导是制造列表的工厂，那么生成器表达式就是制造生成器的工厂 import re import reprlib RE_WORD = re.compile('\\w+') class Sentence: def __init__(self, text): self.text = text def __repr__(self): return 'Sentence(%s)' % reprlib.repr(self.text) def __iter__(self): return (match.group() for match in RE_WORD.finditer(self.text)) 最终的效果一样：调用 __iter__ 方法会得到一个生成器对象 生成器表达式是语法糖：完全可以替换成生成器函数，不过有时使用生成器表达式更便利 生成器表达式是创建生成器的简洁句法，这样无需先定义函数再调用 生成器函数灵活得多，可以使用多个语句实现复杂的逻辑，也可以作为协程使用 遇到简单的情况时，可以使用生成器表达式 如果生成器表达式要分成多行写，我倾向于定义生成器函数，以便提高可读性 如果函数或构造方法只有一个参数，传入生成器表达式时不用写一对调用函数的括号 再写一对括号围住生成器表达式，只写一对括号就行了 迭代器有两个特点 接口 Python 的迭代器协议定义了两个方法：__next__ 和 __iter__ 生成器对象实现了这两个方法，因此从这方面来看，所有生成器都是迭代器 实现方式 生成器这种 Python 语言结构可以使用两种方式编写：含有 yield 关键字的函数，或者生成器表达式 调用生成器函数或者执行生成器表达式得到的生成器对象属于语言内部的 GeneratorType 类型 迭代器可以节约大量的空间 当需要生成的数列数量非常大时，由于代码中将数字存储在 list 中，会导致巨大内存占用 # 生成器 class Fab(object): def __init__(self, max): self.max = max self.n, self.a, self.b = 0, 0, 1 def __iter__(self): return self def next(self): if self.n 上述代码通过类的形式将函数封装为一个可迭代对象 通过next方法在循环的时候每次去取一个数，只有在需要使用的时候才会生成，内存占用很小 但是，上述代码较为繁琐，在Python中，有一种语法糖能简化，那就是 yield 如果这个元素可以通过某种方式推算出来切可以进行循环操作，就避免了大的内存占用 只需要函数在循环时计算得下一个数字并返回，这样就不必创建完整的 list ，从而节省大量空间 在Python中，这种一边循环一边计算的机制，称为生成器：generator # yield 语法糖 def fab(max): n, a, b = 0, 0, 1 while n 调用和 Fab 版的完全一致，也可以使用 next 方法等。简单的说， yield 的作用就是把一个函数变为一个 generator ，带有 yield 的函数不再是一个普通函数， Python 解释器会将器视为 generator 在 for 循环执行时，每次循环都会执行 fab 函数内部的代码，执行到 yield 时 函数就返回一个迭代值，下次迭代时，就从 yield 的下一句继续执行，调用next也是同理 当函数执行结束时，会抛出 StopIteration 异常，表示迭代完成 itertools模块 itertools --- 为高效循环而创建迭代器的函数 Python-进阶-itertools模块小结 itertools 迭代器的特点是：惰性求值（Lazy evaluation），即只有当迭代至某个值时，它才会被计算，这个特点使得迭代器特别适合于遍历大文件或无限集合等，因为我们不用一次性将它们存储在内存中 Python 内置的 itertools 模块包含了一系列用来产生不同类型迭代器的函数或类，这些函数的返回都是一个迭代器，我们可以通过 for 循环来遍历取值，也可以使用 next() 来取值 itertools 模块提供的迭代器函数有以下几种类型： 无限迭代器：生成一个无限序列，比如自然数序列 1, 2, 3, 4, ... 有限迭代器：接收一个或多个序列（sequence）作为参数，进行组合、分组和过滤等 组合生成器：序列的排列、组合，求序列的笛卡儿积等 无限迭代器 itertools 模块提供了三个函数（事实上，它们是类）用于生成一个无限序列迭代器 count(firstval=0, step=1) 创建一个从 firstval (默认值为 0) 开始，以 step (默认值为 1) 为步长的的无限整数迭代器 cycle(iterable) 对 iterable 中的元素反复执行循环，返回迭代器 repeat(object [,times] 反复生成 object，如果给定 times，则重复次数为 times，否则为无限 Iterator Arguments Results Example count() start, [step] start, start+step, start+2*step, … count(10) --> 10 11 12 13 14 ... cycle() p p0, p1, … plast, p0, p1, … cycle('ABCD') --> A B C D A B C D ... repeat() elem [,n] elem, elem, elem, … endlessly or up to n times repeat(10, 3) --> 10 10 10 import itertools nums = itertools.count(start=2,step=3) for i in nums: if i>15: break print(i, end=' ') Out[17]: 2 5 8 11 14 [i for i in itertools.repeat(['a','b'],3)] Out[18]: [['a', 'b'], ['a', 'b'], ['a', 'b']] 有限迭代器 itertools 模块提供了多个函数（类），接收一个或多个迭代对象作为参数，对它们进行组合、分组和过滤等 chain() compress() dropwhile() groupby() ifilter() ifilterfalse() islice() imap() starmap() tee() takewhile() izip() izip_longest() Iterator Arguments Results Example chain() p, q, … p0, p1, … plast, q0, q1, … chain('ABC', 'DEF') --> A B C D E F compress() data, selectors (d[0] if s[0]), (d[1] if s[1]), … compress('ABCDEF', [1,0,1,0,1,1]) --> A C E F dropwhile() pred, seq seq[n], seq[n+1], starting when pred fails dropwhile(lambda x: x 6 4 1 groupby() iterable[, keyfunc] sub-iterators grouped by value of keyfunc(v) ifilter() pred, seq elements of seq where pred(elem) is true ifilter(lambda x: x%2, range(10)) --> 1 3 5 7 9 ifilterfalse() pred, seq elements of seq where pred(elem) is false ifilterfalse(lambda x: x%2, range(10)) --> 0 2 4 6 8 islice() seq, [start,] stop [, step] elements from seq[start:stop:step] islice('ABCDEFG', 2, None) --> C D E F G starmap() func, seq func(seq[0]), func(seq[1]), … starmap(pow, [(2,5), (3,2), (10,3)]) --> 32 9 1000 tee() it, n it1, it2, … itn splits one iterator into n takewhile() pred, seq seq[0], seq[1], until pred fails takewhile(lambda x: x 1 4 zip_longest() p, q, … (p[0], q[0]), (p[1], q[1]), … zip_longest('ABCD', 'xy', fillvalue='-') --> Ax By C- D- [item for item in itertools.chain([1, 2, 3], ['a', 'b', 'c'])] Out[19]: [1, 2, 3, 'a', 'b', 'c'] # 接收一个可迭代对象作为参数，返回一个迭代器 string = itertools.chain.from_iterable('ABCD') # compress 可用于对数据进行筛选，当selectors的某个元素为true时，则保留data对应位置的元素，否则去除： # compress(data, selectors) list(compress('ABCDEF', [1, 1, 0, 1, 0, 1])) Out[23]:['A', 'B', 'D', 'F'] # dropwhile(predicate, iterable) # 其中，predicate 是函数，iterable 是可迭代对象。对于 iterable 中的元素，如果 predicate(item) 为 true，则丢弃该元素，否则返回该项及所有后续项。 >>> list(dropwhile(lambda x: x >> >>> list(dropwhile(lambda x: x > 3, [2, 1, 6, 5, 4])) [2, 1, 6, 5, 4] # groupby(iterable[, keyfunc]) # 相邻相同元素分组 # 其中，iterable 是一个可迭代对象，keyfunc 是分组函数，用于对 iterable 的连续项进行分组，如果不指定，则默认对 iterable 中的连续相同项进行分组，返回一个 (key, sub-iterator) 的迭代器。 >>> for key, value_iter in groupby('aaabbbaaccd'): ... print key, ':', list(value_iter) ... a : ['a', 'a', 'a'] b : ['b', 'b', 'b'] a : ['a', 'a'] c : ['c', 'c'] d : ['d'] # ifilter(function or None, sequence) # 将 iterable 中 function(item) 为 True 的元素组成一个迭代器返回，如果 function 是 None，则返回 iterable 中所有计算为 True 的项。 >>> list(ifilter(lambda x: x >> >>> list(ifilter(None, [0, 1, 2, 0, 3, 4])) [1, 2, 3, 4] # ifilterfalse 的使用形式和 ifilter 类似，它将 iterable 中 function(item) 为 False 的元素组成一个迭代器返回，如果 function 是 None，则返回 iterable 中所有计算为 False 的项。 >>> list(ifilterfalse(lambda x: x >> >>> list(ifilter(None, [0, 1, 2, 0, 3, 4])) [0, 0] # islice(iterable, [start,] stop [, step]) # 其中，iterable 是可迭代对象，start 是开始索引，stop 是结束索引，step 是步长，start 和 step 可选。 >>> list(islice([10, 6, 2, 8, 1, 3, 9], 5)) [10, 6, 2, 8, 1] >>> >>> list(islice(count(), 6)) [0, 1, 2, 3, 4, 5] >>> >>> list(islice(count(), 3, 10)) [3, 4, 5, 6, 7, 8, 9] >>> list(islice(count(), 3, 10 ,2)) [3, 5, 7, 9] # tee 用于从 iterable 创建 n 个独立的迭代器，以元组的形式返回，n 的默认值是 2。 iter1, iter2 = tee('abcde') # n 默认为 2，创建两个独立的迭代器 # takewhile(predicate, iterable) # 其中，predicate 是函数，iterable 是可迭代对象。对于 iterable 中的元素，如果 predicate(item) 为 true，则保留该元素，只要 predicate(item) 为 false，则立即停止迭代。 >>> list(takewhile(lambda x: x >> list(takewhile(lambda x: x > 3, [2, 1, 6, 5, 4])) [] from itertools import zip_longest [item for item in zip_longest('ABCD', 'xy')] Out[41]: [('A', 'x'), ('B', 'y'), ('C', None), ('D', None)] [item for item in zip_longest('ABCD', 'xy',fillvalue='-')] Out[42]: [('A', 'x'), ('B', 'y'), ('C', '-'), ('D', '-')] itertools.groupby用法 # 分组统计，并获取每组的具体元素 import re from itertools import groupby sentence = '我是 哈哈 一直在 我 三连击 陈飒飒 阿豆腐干 阿苏打水丢阿萨德' res= itertools.groupby(sorted(re.sub(' +',' ',sentence).split(' '),key=len,reverse=True),len) for k,v in res: print('%s->%s'%(k,list(v))) 8->['阿苏打水丢阿萨德'] 4->['阿豆腐干'] 3->['一直在', '三连击', '陈飒飒'] 2->['我是', '哈哈'] 1->['我'] itertools.tee用法 itertools.tee分裂出来的多个生成器不是线程安全的，不能在多线程里面运行，否则会导致报错，这里给出一个报错的例子： import itertools import threading def generator(): for i in range(1000000): yield i g = generator() g_1, g_2 = itertools.tee(g, 2) for x in [g_1, g_2]: threading.Thread(target=sum, args=(x,)).start() 多线程安全版本 class ThreadingTee: def __init__(self, tee_obj, lock): self.tee_obj = tee_obj self.lock = lock def __iter__(self): return self def __next__(self): with self.lock: return next(self.tee_obj) def __copy__(self): return ThreadingTee(self.tee_obj.__copy__(), self.lock) def threading_tee(iterable, n=2): \"\"\"tuple of n independent thread-safe iterators\"\"\" lock = Lock() return tuple(ThreadingTee(tee_obj, lock) for tee_obj in itertools.tee(iterable, n)) 组合生成器 itertools 模块还提供了多个组合生成器函数，用于求序列的排列、组合等 product permutations combinations combinations_with_replacement Iterator Arguments Results product() p, q, … [repeat=1] cartesian product, equivalent to a nested for-loop permutations() p[, r] r-length tuples, all possible orderings, no repeated elements combinations() p, r r-length tuples, in sorted order, no repeated elements combinations_with_replacement() p, r r-length tuples, in sorted order, with repeated elements product('ABCD', repeat=2) AA AB AC AD BA BB BC BD CA CB CC CD DA DB DC DD permutations('ABCD', 2) AB AC AD BA BC BD CA CB CD DA DB DC combinations('ABCD', 2) AB AC AD BC BD CD combinations_with_replacement('ABCD', 2) AA AB AC AD BB BC BD CC CD DD # product 用于求多个可迭代对象的笛卡尔积，它跟嵌套的 for 循环等价。它的一般使用形式如下： # product(iter1, iter2, ... iterN, [repeat=1]) # 其中，repeat 是一个关键字参数，用于指定重复生成序列的次数， >>> for item in product('ABCD', 'xy'): ... print(item) ('A', 'x') ('A', 'y') ('B', 'x') ('B', 'y') ('C', 'x') ('C', 'y') ('D', 'x') ('D', 'y') >>> list(product('ABC', repeat=2)) [('A', 'A'), ('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'B'), ('B', 'C'), ('C', 'A'), ('C', 'B'), ('C', 'C')] # permutations 用于生成一个排列，它的一般使用形式如下： # permutations(iterable[, r]) # 其中，r 指定生成排列的元素的长度，如果不指定，则默认为可迭代对象的元素长度。 >>> list(permutations('ABC', 2)) [('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'C'), ('C', 'A'), ('C', 'B')] >>> list(permutations('ABC')) [('A', 'B', 'C'), ('A', 'C', 'B'), ('B', 'A', 'C'), ('B', 'C', 'A'), ('C', 'A', 'B'), ('C', 'B', 'A')] # combinations 用于求序列的组合，它的使用形式如下： # combinations(iterable, r) # 其中，r 指定生成组合的元素的长度。 >>> from itertools import combinations >>> list(combinations('ABC', 2)) [('A', 'B'), ('A', 'C'), ('B', 'C')] # combinations_with_replacement 和 combinations 类似，但它生成的组合包含自身元素。 >>> from itertools import combinations_with_replacement >>> list(combinations_with_replacement('ABC', 2)) [('A', 'A'), ('A', 'B'), ('A', 'C'), ('B', 'B'), ('B', 'C'), ('C', 'C')] 其他方法 accumulate对iterable对象逐个进行func操作(默认是累加) itertools.accumulate(iterable[, func, *, initial=None]) >>> from itertools import accumulate >>> import operator # operator --- 标准运算符替代函数 >>> a = [1,2,3,4,5] >>> b = accumulate(a) # 默认是累加 这里返回的是一个可迭代对象 >>> list(b) # 强制转化 [1, 3, 6, 10, 15] 使用现有扩展功能 使用内置的itertools可以组合扩展出更多更强大功能的方法 def take(n, iterable): \"Return first n items of the iterable as a list\" return list(islice(iterable, n)) def tabulate(function, start=0): \"Return function(0), function(1), ...\" return imap(function, count(start)) def consume(iterator, n): \"Advance the iterator n-steps ahead. If n is none, consume entirely.\" # Use functions that consume iterators at C speed. if n is None: # feed the entire iterator into a zero-length deque collections.deque(iterator, maxlen=0) else: # advance to the empty slice starting at position n next(islice(iterator, n, n), None) def nth(iterable, n, default=None): \"Returns the nth item or a default value\" return next(islice(iterable, n, None), default) def quantify(iterable, pred=bool): \"Count how many times the predicate is true\" return sum(imap(pred, iterable)) def padnone(iterable): \"\"\"Returns the sequence elements and then returns None indefinitely. Useful for emulating the behavior of the built-in map() function. \"\"\" return chain(iterable, repeat(None)) def ncycles(iterable, n): \"Returns the sequence elements n times\" return chain.from_iterable(repeat(tuple(iterable), n)) def dotproduct(vec1, vec2): return sum(imap(operator.mul, vec1, vec2)) def flatten(listOfLists): \"Flatten one level of nesting\" return chain.from_iterable(listOfLists) def repeatfunc(func, times=None, *args): \"\"\"Repeat calls to func with specified arguments. Example: repeatfunc(random.random) \"\"\" if times is None: return starmap(func, repeat(args)) return starmap(func, repeat(args, times)) def pairwise(iterable): \"s -> (s0,s1), (s1,s2), (s2, s3), ...\" a, b = tee(iterable) next(b, None) return izip(a, b) def grouper(iterable, n, fillvalue=None): \"Collect data into fixed-length chunks or blocks\" # grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx args = [iter(iterable)] * n return izip_longest(fillvalue=fillvalue, *args) def roundrobin(*iterables): \"roundrobin('ABC', 'D', 'EF') --> A D E B F C\" # Recipe credited to George Sakkis pending = len(iterables) nexts = cycle(iter(it).next for it in iterables) while pending: try: for next in nexts: yield next() except StopIteration: pending -= 1 nexts = cycle(islice(nexts, pending)) def powerset(iterable): \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\" s = list(iterable) return chain.from_iterable(combinations(s, r) for r in range(len(s)+1)) def unique_everseen(iterable, key=None): \"List unique elements, preserving order. Remember all elements ever seen.\" # unique_everseen('AAAABBBCCDAABBB') --> A B C D # unique_everseen('ABBCcAD', str.lower) --> A B C D seen = set() seen_add = seen.add if key is None: for element in ifilterfalse(seen.__contains__, iterable): seen_add(element) yield element else: for element in iterable: k = key(element) if k not in seen: seen_add(k) yield element def unique_justseen(iterable, key=None): \"List unique elements, preserving order. Remember only the element just seen.\" # unique_justseen('AAAABBBCCDAABBB') --> A B C D A B # unique_justseen('ABBCcAD', str.lower) --> A B C A D return imap(next, imap(itemgetter(1), groupby(iterable, key))) def iter_except(func, exception, first=None): \"\"\" Call a function repeatedly until an exception is raised. Converts a call-until-exception interface to an iterator interface. Like __builtin__.iter(func, sentinel) but uses an exception instead of a sentinel to end the loop. Examples: bsddbiter = iter_except(db.next, bsddb.error, db.first) heapiter = iter_except(functools.partial(heappop, h), IndexError) dictiter = iter_except(d.popitem, KeyError) dequeiter = iter_except(d.popleft, IndexError) queueiter = iter_except(q.get_nowait, Queue.Empty) setiter = iter_except(s.pop, KeyError) \"\"\" try: if first is not None: yield first() while 1: yield func() except exception: pass def random_product(*args, **kwds): \"Random selection from itertools.product(*args, **kwds)\" pools = map(tuple, args) * kwds.get('repeat', 1) return tuple(random.choice(pool) for pool in pools) def random_permutation(iterable, r=None): \"Random selection from itertools.permutations(iterable, r)\" pool = tuple(iterable) r = len(pool) if r is None else r return tuple(random.sample(pool, r)) def random_combination(iterable, r): \"Random selection from itertools.combinations(iterable, r)\" pool = tuple(iterable) n = len(pool) indices = sorted(random.sample(xrange(n), r)) return tuple(pool[i] for i in indices) def random_combination_with_replacement(iterable, r): \"Random selection from itertools.combinations_with_replacement(iterable, r)\" pool = tuple(iterable) n = len(pool) indices = sorted(random.randrange(n) for i in xrange(r)) return tuple(pool[i] for i in indices) def tee_lookahead(t, i): \"\"\"Inspect the i-th upcomping value from a tee object while leaving the tee object at its current position. Raise an IndexError if the underlying iterator doesn't have enough values. \"\"\" for value in islice(t.__copy__(), i, None): return value raise IndexError(i) 自定义扩展 # 将序列按大小切分,更好的性能 from itertools import chain, islice def chunks(iterable, size, format=iter): it = iter(iterable) while True: yield format(chain((it.next(),), islice(it, size - 1))) >>> l = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\"] >>> for chunk in chunks(l, 3, tuple): print(chunk) (\"a\", \"b\", \"c\") (\"d\", \"e\", \"f\") (\"g\",) python协程 谈谈Python协程技术的演进谈谈Python协程技术的演进 Python协程从入门到放弃到死亡到重生 Python协程的演化-从yield/send到async/await 协程的实现方式有很多，这里我们来列举三种基本方式 利用 yield 来实现协程 利用 greenlet 模块实现 利用 gevent 模块实现 并行与并发 并行是指两个或者多个事件在同一时刻发生，通常是当系统有一个以上CPU或CPU核心时，才有可能并行 两个或者多个事件、线程并步抢占CPU资源 并发是指两个或者多个事件在同一时间间隔内发生 在一定时间间隔内，有多个程序在执行，但在同一时刻，只有一个程序在执行 生成器和协程 协程的概念 协程通过允许多个入口点在某些位置暂停和恢复执行来概括用于非抢占式多任务的子程序 yield 关键字已经可以暂停执行，如果在暂停后有方法把一些值发送回到暂停窒息的函数中，那么便就可以理解为协程 添加了把东西发回已经暂停的生成器中的方法，这个方法就是 send() 生成器和协程 从语法上讲，生成器是一个带yield语句的函数。协程，又称微线程，纤程，英文Coroutine 从某些角度来理解，协程其实就是一个可以暂停执行的函数，并且可以恢复继续执行 生成器和协程都是通过 python 中 yield 的关键字实现的 next()和send() 生成器只调用 next 来不断的生成数据，而协程会调用 next 和 send 来返回结果和接收参数 与.__next__() 方法一样，.send() 方法致使生成器前进到下一个 yield 语句 不过.send() 方法还允许使用生成器的客户把数据发给自己，即不管传给 .send() 方法什么参数，那个参数都会成为生成器函数定义体中对应的 yield 表达式的值 也就是说，.send() 方法允许在客户代码和生成器之间双向交换数据 而 .__next__() 方法只允许客户从生成器中获取数据 Python 新引入的 yield from 句法允许生成器或协程把工作委托给第三方完成，这样就无需嵌套 for 循环作为变通了 import random,time def stupid_fib(n): index = 0 a = 0 b = 1 while index 其中 next(sfib) 相当于 sfib.send(None)，可以使得 sfib 运行至第一个 yield 处返回 后续将一个随机的秒数发送给 sfib ，作为当前中断的 yield 表达式的返回值 yield 表达式的作用包含了三个步骤： 向函数外抛出值 暂停，等待 next() 或 send() 恢复 赋值，接受 send 发送过来的数据 需要注意的是，在使用 send(None) 或者 next() 预激生成器函数，并执行到第一个 yield 语句结束的位置时，实际程序只执行了1、2两步，程序返回了值，并暂停，并没有执行第三步去赋值。在后续的循环中，才会进行赋值。 生成器对象有 send 、throw 和 close 方法，这三个方法的作用分别是： 发送数据给生成器并赋值给 yield 语句 向生成器中抛入异常由生成器内部处理 终止生成器 这三个方法使得生成器进化成协程 协程状态 协程有四种存在状态： GEN_CREATED 创建完成，等待执行 GEN_RUNNING 解释器正在执行（这个状态在下面的示例程序中无法看到） GEN_SUSPENDED 在 yield 表达式处暂停 GEN_CLOSE 执行结束，生成器停止 可以使用 inspect.getgeneratorstate 方法查看协程的当前状态 In [202]: import inspect In [203]: def generator(): ...: i = '激活生成器' ...: while True: ...: try: ...: value = yield i ...: except ValueError: ...: print('OVER') ...: i = value ...: In [204]: g = generator() # 1 In [205]: inspect.getgeneratorstate(g) # 2 Out[205]: 'GEN_CREATED' In [206]: next(g) # 3 Out[206]: '激活生成器' In [207]: inspect.getgeneratorstate(g) Out[207]: 'GEN_SUSPENDED' In [208]: g.send('Hello Shiyanlou') # 4 Out[208]: 'Hello Shiyanlou' In [209]: g.throw(ValueError) # 5 OVER Out[209]: 'Hello Shiyanlou' In [210]: g.close() # 6 In [211]: inspect.getgeneratorstate(g) Out[211]: 'GEN_CLOSED' 代码说明 创建生成器 查看生成器状态 这步操作叫做预激生成器（或协程），这是必须做的。在生成器创建完成后，需要将其第一次运行到 yield 语句处暂停 暂停状态的生成器可以使用 send 方法发送数据，此方法的参数就是 yield 表达式的值 也就是 yield 表达式等号前面的 value 变量的值变成 'Hello Shiyanlou'，继续向下执行完一次 while 循环，变量 i 被赋值，继续运行下一次循环，yield 表达式弹出变量 i 向生成器抛入异常，异常会被 try except 捕获，作进一步处理 close 方法终止生成器，异常不会被抛出 因为生成器的调用方也就是程序员自己可以控制生成器的启动、暂停、终止，而且可以向生成器内部传入数据 所以这种生成器又叫做协程，generator 函数既可以叫做生成器函数，也可以叫协程函数，这是生成器向协程的过渡阶段 yield from 在 Python3.3 出现了 yield from 语法, yield from item 表达式从 item 中获得迭代器 yield from 可以代替 for 循环，使得代码更为精炼，yield from 后面需要加的是可迭代对象 yield from i 完全代替了内层的 for 循环。而且代码读起来更顺畅，不过感觉更像是语法糖 除了代替循环之外，yield from 还会创建通道，把内层生成器直接与外层生成器的客户端联系起来 把生成器当成协程使用时，这个通道特别重要，不仅能为客户端代码生成值，还能使用客户端代码提供的值 # yield from def first_gen(): for c in \"AB\": yield c for i in range(0, 3): yield i print(list(first_gen())) def second_gen(): yield from \"AB\" yield from range(0, 3) print(list(second_gen())) ['A', 'B', 0, 1, 2] ['A', 'B', 0, 1, 2] 当 yiled from 后面加上一个生成器之后，就实现了生成的嵌套。实现生成器的嵌套，不一定要使用 yield from 但它可以让我们避免让自己处理各种料想不到的异常，如果自己去实现，会加大编码的难度。 yield from 的主要功能是打开双向通道，把最外层的调用与最内层的子生成器连接起来 这样二者就可以直接发送和产出值，还可以直接穿入异常 委派生成器在 yied from 表达式处暂停时，调用方可以直接把数据发给子生成器 子生成器再把产出值发给调用方，子生成器返回之后，解释器会抛出 StopIteration 异常 委托生成器的作用就是：在调用方与子生成器之间建立一个双向通道 为什么一定要使用 yield from 语句呢：在使用 yiled from 语句时，语句为我们已经处理了很多的异常 yield from 获取返回值 python协程系列（三）——yield from原理详解 在使用yield生成器的时候，如果使用for语句去迭代生成器，则不会显式的出发StopIteration异常，而是自动捕获StopIteration异常 所以如果遇到return，只是会终止迭代，而不会触发异常，故而也就没办法获取return的值 def my_generator(): for i in range(5): if i==2: return '我被迫中断了' else: yield i def main(generator): try: for i in generator: #不会显式触发异常，故而无法获取到return的值 print(i) except StopIteration as exc: print(exc.value) g=my_generator() #调用 main(g) ''' 运行结果为： 0 1 ''' 从上面的例子可以看出，for迭代语句不会显式触发异常，故而无法获取到return的值 迭代到2的时候遇到return语句，隐式的触发了StopIteration异常，就终止迭代了，但是在程序中不会显示出来 def my_generator(): for i in range(5): if i==2: return '我被迫中断了' else: yield i def main(generator): try: print(next(generator)) #每次迭代一个值，则会显式出发StopIteration print(next(generator)) print(next(generator)) print(next(generator)) print(next(generator)) except StopIteration as exc: print(exc.value) #获取返回的值 g=my_generator() main(g) ''' 运行结果为： 0 1 我被迫中断了 ''' 现在我们使用yield from来完成上面的同样的功能 def my_generator(): for i in range(5): if i==2: return '我被迫中断了' else: yield i def wrap_my_generator(generator): #定义一个包装“生成器”的生成器，它的本质还是生成器 result = yield from generator #自动触发StopIteration异常，并且将return的返回值赋值给yield from表达式的结果 print(result) def main(generator): for j in generator: print(j) g = my_generator() wrap_g = wrap_my_generator(g) main(wrap_g) #调用 ''' 运行结果为： 0 1 我被迫中断了 ''' 从上面的比较可以看出，yield from具有以下几个特点： 调用方—>生成器函数(协程函数) 调用方—>生成器包装函数—>生成器函数(协程函数) return返回的值或者是StopIteration的value 属性的值变成 yield from 表达式的值，即上面的result greenlet 和 gevent greenlet可以实现协程，不过每一次都要人为去指向下一个该执行的协程 greenlet 可以从一个协程切换到任意其他协程，但必须保证 greenlet 的正常结束，在协程之间的任意切换很容易出现问题 greelet 是 Stackless 发展来的 Cpython 扩展包， greelet 是底层实现了原生协程的C扩展库 迭代器(即可指子生成器)产生的值直接返还给调用者 任何使用send()方法发给委派生产器(即外部生产器)的值被直接传递给迭代器。如果send值是None，则调用迭代器next()方法；如果不为None，则调用迭代器的send()方法。如果对迭代器的调用产生Stoplteration异常，委派生产器恢复继续执行yield from后面的语句；若迭代器产生其他任何异常，则都传递给委派生产器 子生成器可能只是一个迭代器，并不是一个作为协程的生成器，所以它不支持.throw()和.close()方法，即可能会产生AttributeError异常 除了GeneratorExit异常外的其他抛给委派生产器的异常，将会被传递到迭代器的throw()方法，如果迭代器throw()调用产生了Stoplteration异常，委派生产器恢复并继续执行，其他异常则传递给委派生产器 如果GeneratorExit异常被抛给委派生产器，或者委派生产器的close()方法被调用，如果迭代器有close()的话也将被调用。如果close()调用产生异常，异常将传递给委派生产器。否则，委派生产器将抛出GeneratorExit 异常 当迭代器结束并抛出异常时，yield from表达式的值是其Stoplteration异常中的第一个参数 一个生成器中的return expr语句将会从生成器退出并抛出 Stoplteration(expr)异常 greenlet # 使用 greenlet 实现的 生产者-消费者 模型： # greenlet 的价值在于高性能的原生协程，且语义更加明确、显式切换，执行到 switch 时就切换程序 # 直接将函数包装成协程，可以保留原代码的风格 # 基于greenlet的生产者消费者协程 from greenlet import greenlet import random import time def Producer(): while True: item = random.randint(1, 10) print(\"生产中...\".format(item)) time.sleep(1) c.switch(item) # 切换到消费者，并将item传入。 def Consumer(): while True: item = p.switch() # 切换到生产者。等待生产者传递参数item print(\"消费中..\".format(item)) c = greenlet(Consumer) # 将普通函数编程协程 p = greenlet(Producer) # 同理 c.switch() # 启动协程，Consumer先执行 \"\"\" 从consumer开始执行，执行到item=p.switch()时，程序切换到producer，并等待传参 producer得到执行权后，生成一个item,并往下执行代码 当producer执行到c.switch(item)时，程序携带传递的item切换到consumer, consumer继续往下执行，直到下一次运行到p.switch时，交出执行权，切换到producer，重复以上过程 \"\"\" gevent gevent 是实现协程的第三方库，通过封装 greenlet，epoll 回调编程模式，生成器协程实现 当遇到 IO 操作时，就自动切换到其他协程，等到 IO 操作完成，再在适当的时候切换回来继续执行 gevent 会自动切换协程，就保证总有协程在执行，而不是等待 IO 由于切换实在 IO 操作时自动完成，所以 gevent 需要修改 Python 的自带的一些保准库，这一过程在启动时通过 monkey patch 完成 gevent的价值在于它的使用基于epoll的libev来避开阻塞；使用基于gevent的高效协程，来切换执行 只在遇到阻塞的时候切换，没有轮询和线程开销 \"\"\" gevent: 通过greenlet实现协程，核心就是遇到IO操作，会自动切换到其他协程 \"\"\" # 将python标准库中的一些阻塞操作变为非阻塞 from gevent import monkey;monkey.patch_all() # 使用猴子补丁要写在第一行 import gevent def test1(): print(\"test1\") gevent.sleep(0) # 模拟耗时操作 print(\"test11\") def test2(): print(\"test2\") gevent.sleep(0) # 模拟耗时操作 print(\"test22\") # g1 = gevent.spawn(test1) # 将函数封装成协程，并启动 # g2 = gevent.spawn(test2) # gevent.joinall([g1, g2]) \"\"\" # joinall() 阻塞当前流程，执行给定的greenlet(列表中的对象),等待程序执行完 # spawn是启动协程，参数为函数名及其参数 test1 test2 test11 test22 代码执行test1,打印test1，遇到gevent.sleep(0)时切换程序，执行test2 test()执行，打印test2，执行到gevent.sleep(0)时切换程序 执行test1在gevent.sleep(0)后面的代码，直到再次遇到gevent时，切换程序 然后在test2中，继续执行gevent后的代码，直到遇到gevent时，再次切换 直到程序执行完毕 \"\"\" asyncio.coroutine 在 Python3.4 中加入了 asyncio 库，使得 Python 获得了事件循环的特性，但这个还是以生成器对象为基础 yield from 在 asyncio 模块中很常用，通过 asnyncio+生成器 ，我们可以实现这样一个异步的模型： import asyncio @asyncio.coroutine def counttdown(number, n): while n > 0: print(\"T-minus\", n, \"({})\".format(number)) yield from asyncio.sleep(1) n -= 1 loop = asyncio.get_event_loop() tasks = [ asyncio.ensure_future(counttdown(\"A\", 2)), asyncio.ensure_future(counttdown(\"B\", 5)), ] loop.run_until_complete(asyncio.wait(tasks)) loop.close() 【out】 Traceback (most recent call last): File \"\", line 15, in loop.run_until_complete(asyncio.wait(tasks)) File \"S:\\Anaconda3\\lib\\asyncio\\base_events.py\", line 555, in run_until_complete self.run_forever() File \"S:\\Anaconda3\\lib\\asyncio\\base_events.py\", line 510, in run_forever raise RuntimeError('This event loop is already running') RuntimeError: This event loop is already running T-minus 2 (A) T-minus 5 (B) T-minus 1 (A) T-minus 4 (B) T-minus 3 (B) T-minus 2 (B) T-minus 1 (B) 这里 asyncio.coroutine 装饰器是用来标记这个函数是一个协程，因为 asyncio 要求所有用作协程的生成器必须由 asyncio.coroutine 装饰 这段代码中，事件循环会启动两个 countdown() 协程，它们会一直执行，知道遇到 yield from asyncio.sleep() ，暂停执行，并将一个 async.Future 对象返回给事件循环 事件循环会监控这个 asyncio.Future 对象，一旦执行完成后，会将这个 Future 的执行结果返回给刚刚因为这个 Futur e暂停的协程，并且继续执行原协程 event_loop 事件循环： 程序开启一个无限的循环，程序员会把一些函数注册到事件循环上。当满足事件发生的时候，调用相应的协程函数 coroutine 协程：协程对象，指一个使用async关键字定义的函数，它的调用不会立即执行函数，而是会返回一个协程对象 协程对象需要注册到事件循环，由事件循环调用 task 任务：一个协程对象就是一个原生可以挂起的函数，任务则是对协程进一步封装，其中包含任务的各种状态 协程对象不能直接运行，在注册事件循环的时候，其实是 run_until_complete 方法将协程包装成一个任务（ task ）对象 task 对象是 Future 的子类，保存了协程运行后的状态，用于未来获取协程的结果 在上面的代码中， asyncio.sleep 中，创建了一个 Futrure 对象，作为更内层的协程对象，通过 yield from 交给了事件循环，而 Future 是一个实现了 __iter__ 对象的生成器。 @coroutine def sleep(delay, result=None, *, loop=None): \"\"\"Coroutine that completes after a given time (in seconds).\"\"\" future = futures.Future(loop=loop) h = future._loop.call_later(delay, future._set_result_unless_cancelled, result) try: return (yield from future) finally: h.cancel() class Future: #blabla... def __iter__(self): if not self.done(): self._blocking = True yield self # This tells Task to wait for completion. assert self.done(), \"yield from wasn't used with future\" return self.result() # May raise too. # 当协程 yield from asyncio.sleep 时，事件循环其实是与 Future 对象建立了联系。程序运行结果如下： T-minus 2 (A) T-minus 5 (B) T-minus 1 (A) T-minus 4 (B) T-minus 3 (B) T-minus 2 (B) T-minus 1 (B) async 和 await hello world import asyncio import time async def main(): print(f\"started at {time.strftime('%X')}\") await say_after(1, 'hello') await say_after(2, 'world') print(f\"finished at {time.strftime('%X')}\") asyncio.run(main()) # 预期输出 started at 17:13:52 hello world finished at 17:13:55 async def main(): task1 = asyncio.create_task( say_after(1, 'hello')) task2 = asyncio.create_task( say_after(2, 'world')) print(f\"started at {time.strftime('%X')}\") # Wait until both tasks are completed (should take around 2 seconds.) await task1 await task2 print(f\"finished at {time.strftime('%X')}\") # 预期的输出显示代码段的运行时间比之前快了 1 秒 started at 17:14:32 hello world finished at 17:14:34 asyncio模块历史演进 asyncio是python3.4引入的库，翻译过来就是异步I/O 用await代替yield from，功能一模一样，程序调度 装饰器@asyncio.coroutine和关键字async @asyncio.coroutine def func1(): yield from asyncio.sleep(2) # 遇到IO耗时操作，自动化切换到tasks中的其他任务 # 等价于 async def func1(): yield from asyncio.sleep(2) # 遇到IO耗时操作，自动化切换到tasks中的其他任务 Event loop # 版本3.7以前 loop = asyncio.get_event_loop() # 创建一个事件循环 loop.run_until_complete(result) # 将协程当做任务提交到事件循环的任务列表中，协程执行完成之后终止 # 版本3.7引入 asyncio.run(result) 在 Python3.5 中引入了 async 和 await ，可以将它们理解为 asyncio.coroutine / yield from 的完美替身， async/await 让协程表面上独立于生成器而存在，将细节隐藏于 asyncio 模块之下。使用 await 可以针对耗时的操作进行挂起，类似于生成器里的 yield 一样，使函数让出控制权 协程遇到 await ，事件循环挂起该协程，直到其他协程也挂起或者执行完毕，再进行下一个协程的执行。耗时的操作一般是一些 IO 操作，如网络请求，文件读取等 如果一个对象可以在 await 语句中使用，那么它就是 可等待 对象 可等待对象有三种主要类型: 协程, 任务 和 Future 协程: Python 协程属于 可等待 对象，因此可以在其他协程中被等待 async def nested(): return 42 await nested() 任务: 被用来“并行的”调度协程，当一个协程通过 asyncio.create_task() 等函数被封装为一个 任务，该协程会被自动调度执行: # api asyncio.create_task(coro, *, name=None, context=None) # 调用 task = asyncio.create_task(nested()) await task Futures: Future 是一种特殊的 低层级 可等待对象，表示一个异步操作的 最终结果 当一个 Future 对象 被等待，这意味着协程将保持等待直到该 Future 对象在其他地方操作完毕 在 asyncio 中需要 Future 对象以便允许通过 async/await 使用基于回调的代码 通常情况下 没有必要 在应用层级的代码中创建 Future 对象 Future 对象有时会由库和某些 asyncio API 暴露给用户，用作可等待对象 async def main(): await function_that_returns_a_future_object() # this is also valid: await asyncio.gather( function_that_returns_a_future_object(), some_python_coroutine() ) 一个很好的返回对象的低层级函数的示例是 loop.run_in_executor() 这里可以使用 asyncio.sleep 来进行模拟举例： import asyncio import time now = lambda: time.time() async def do_some_work(x): print('Waiting: ', x) await asyncio.sleep(x) return 'Done after {}s'.format(x) start = now() coroutine1 = do_some_work(1) coroutine2 = do_some_work(2) coroutine3 = do_some_work(4) tasks = [ asyncio.ensure_future(coroutine1), asyncio.ensure_future(coroutine2), asyncio.ensure_future(coroutine3) ] loop = asyncio.get_event_loop() loop.run_until_complete(asyncio.wait(tasks)) for task in tasks: print('Task ret: ', task.result()) print('TIME: ', now() - start) Waiting: 1 Waiting: 2 Waiting: 4 Task ret: Done after 1s Task ret: Done after 2s Task ret: Done after 4s TIME: 4.003541946411133 在 sleep 的时候，使用 await 让出控制权。当遇到阻塞调用的函数的时候，使用 await 方法将协程的控制权让出，以便 loop 调用其他的协程 注意的区别是： await 接受的对象必须是一个 awaitable 的对象，所谓 awaitable 的对象，就是一个实现了 await() 方法的对象，而且这个方法必须返回一个不是协程的迭代器 在 Python3.6 中， yield 和 await 也可以在同一个函数中使用，初次之外，也可以在列表推导等地方使用 async for 或 await 语法 result = [i async for i in aiter() if i % 2] result = [await func() for fun in funcs if await condition()] async def test(x, y): for i in range(y): yield i await asyncio.sleep(x) 协程与异步 与多线程编程不同的是，多个协程总是运行在同一个线程中，一旦其中的一个协程发生阻塞行为，进而所有的协程都无法继续运行 例如在我们进行爬虫编写时，习惯使用 requests 库，而这个库就是阻塞的 尝试使用协程的方式进行编写： import asyncio import requests import time start = time.time() async def get(url): return requests.get(url) async def request(): url = 'http://127.0.0.1:5000' print('Waiting for', url) response = await get(url) print('Get response from', url, 'Result:', response.text) tasks = [asyncio.ensure_future(request()) for _ in range(5)] loop = asyncio.get_event_loop() loop.run_until_complete(asyncio.wait(tasks)) end = time.time() print('Cost time:', end - start) Waiting for http://127.0.0.1:5000 Get response from http://127.0.0.1:5000 Result: Hello! Waiting for http://127.0.0.1:5000 Get response from http://127.0.0.1:5000 Result: Hello! Waiting for http://127.0.0.1:5000 Get response from http://127.0.0.1:5000 Result: Hello! Waiting for http://127.0.0.1:5000 Get response from http://127.0.0.1:5000 Result: Hello! Waiting for http://127.0.0.1:5000 Get response from http://127.0.0.1:5000 Result: Hello! Cost time: 15.134317874908447 而不使用协程，使用普通方式，也是这个时间。为什么会这样呢，究其原因是 requests 并不支持异步操作。在运行时阻塞并未挂起 另外 await 后面所跟的对象必须是：一个原生 coroutine 对象，一个由 types.coroutine 装饰的生成器，这个生成器可以返回 coroutine 对象 而 requests 返回的对象不符合上述条件。为了程序运行不报错，上面代码在 await 时对 requsts 进行了一次 async 函数的包装，但是它并不是“原生的coroutine对象”，因此也就不能真正异步了 可以通过使用实现了异步的 aiohttp 或者 Trip 库改写上述爬虫 import asyncio import time import aiohttp from spider_normal import targets, show_results final_results = {} async def get_content(url): async with aiohttp.ClientSession() as session: async with session.get(url) as resp: content = await resp.read() return len(content) async def spider(url): length = await get_content(url) final_results[url] = length return True def main(): loop = asyncio.get_event_loop() cor = [spider(url) for url in targets] start_time = time.time() result = loop.run_until_complete(asyncio.gather(*cor)) print(\"Use time: {:.2f}s\".format(time.time() - start_time)) show_results(final_results) print(\"loop result: \", result) if __name__ == '__main__': main() # lightless @ LL-DESKTOP in C:\\Users\\lightless\\Desktop [22:49:11] $ python .\\spider_asyncio.py Use time: 2.23s Length: 227 URL:https://www.baidu.com/ Length: 11759 URL:https://www.zhihu.com/ Length: 40740 URL:https://lightless.me/archives/python-coroutine-from-start-to-boom.html Length: 150227 URL:https://github.com/aio-libs -engtn: LO1 Length: 59391 URL:https://vww.python.org/dev/peps/pep-0380/ loop result: [True,True,True,True,True] # lightless @ LL-DESKTOP in C:\\Users\\lightless\\Desktop [22:49:15] $ python .\\spider_asyncio.py Use time: 1.62s Length: 11759 URL:https://www.zhihu.com/ Length: 227 URL:https://www.baidu.com/ Length: 40830 URL:https://lightless.me/archives/python-coroutine-from-start-to-boom.html Length: 59391 URL:https://ww.python.org/dev/peps/pep-0380/ Length: 150227 URL:https://github.com/aio-libs loop result: [True,True,True,True,True] # lightless @ LL-DESKTOP in C:\\Users\\lightless\\Desktop [22:49:20] $ python .\\spider_asyncio.py Use time: 1.59s Length: 11759 URL:https://ww.zhihu.com/ Length: 227 URL:https://www.baidu.com/ Length: 40629 URL:https://lightless.me/archives/python-coroutine-from-start-to-boom.html Length: 59391 URL:https://ww.python.org/dev/peps/pep-0380/ Length: 150219 URL:https://github.com/aio-libs loop result: [True,True,True,True.True] # lightless @ LL-DESKToP in C:\\Users\\lightless\\Desktop [22:49:23] $ python .\\spider_asyncio.py Use time: 4 70s Length: 227 URL:https://www.baidu.com Length: 11759 URL:https://www.zhihu.com/ Lengtn: 59391 URL:https://www.python.org/dev/peps/pep-0380/ Length:150218 URL:https://github.com/aio-libs Length: 40740 URL:https://lightless.me/archives/python-coroutine-from-start-to-boom.html loop result: [True.True,True,True.True] # lightless @ LL-DESKTOP in C:\\Users\\lightless\\Desktop [22:49:29] $ python .\\spider_asyncio.py Use time: 1.85s Length: 227 URL:https://www.baidu.com/ Length: 11759 URL:https://www.zhihu.com/ Length: 40777 URL:https://lightless.me/archives/python-coroutine-from-start-to-boom.html Length: 59391 URL:https://www.python.org/dev/peps/pep-0380/ Length:150227 URL:httos://github.com/aio-libs loop result:[True,True,True,True,True] # lightless @ LL-DESKTOP in C:\\Users\\lightless\\Desktop [22:49:32] 应用 生产者和消费者 #!/usr/bin/env Python # -- coding: utf-8 -- \"\"\" @version: v1.0 @author: huangyc @file: async_test.py @Description: @time: 2023/4/6 10:31 \"\"\" import asyncio, random import threading random.seed(5) async def rnd_sleep(t): # sleep for T seconds on average await asyncio.sleep(t * random.random() * 2) async def producer(queue): lst = list(range(10)) for token in lst: # produce a token and send it to a consumer print(f'produced {token}') await queue.put(token) await rnd_sleep(.1) async def consumer(queue): while True: token = await queue.get() # process the token received from a producer await rnd_sleep(.1) queue.task_done() print(f'consumed {token}') async def main(): queue = asyncio.Queue() # fire up the both producers and consumers producers = [asyncio.create_task(producer(queue)) for _ in range(3)] consumers = [asyncio.create_task(consumer(queue)) for _ in range(10)] # with both producers and consumers running, wait for # the producers to finish await asyncio.gather(*producers) print('---- done producing') # wait for the remaining tasks to be processed await queue.join() # cancel the consumers, which are now idle for c in consumers: c.cancel() if __name__ == '__main__': print(\"hello\") # 多线程+协程方式 t1 = threading.Thread(target=asyncio.run, args=(main(),)) t1.start() t1.join() # 协程调用 # asyncio.run(main()) print(\"end\") 多进程、多线程与协程 计算密集型：多进程，可以最大限度发挥CPU运算能力 IO 密集型：推荐优先使用协程，内存开销少，执行效率高；其次是多线程，虽然不如协程高效，但同样能极大提升程序运行效率 CPU 密集和 IO 密集：多进程+协程，既充分利用多核，又充分发挥协程的高效率，可获得极高的性能 Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-12-01 00:57:11 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/14.python基础知识.html":{"url":"chapters/14.python基础知识.html","title":"python基础知识","keywords":"","body":"python基础知识python解释器字符集和字符编码字符集字符编码生成器、迭代器和可迭代对象Pipenv依赖管理工具私有仓库查看库中常用模块linux后台执行py脚本代码调试技巧库和常用方法变量存储工具方法序列排序import路径问题判断是否有中文分割字符串并转换类型检查对象是否可调用随机打乱多个数组pandas读写csvclassmethod 和 staticmethod优雅判断数字所属等级取整操作其他库python cookbook记录数据结构和算法最大或最小的 N 个元素实现一个优先级队列字典的运算查找两字典的相同点删除序列相同元素并保持顺序命名切片字符串和文本使用多个界定符分割字符串用 Shell 通配符匹配字符串字符串匹配和搜索字符串搜索和替换将 Unicode 文本标准化删除字符串中不需要的字符审查清理文本字符串字符串对齐以指定列宽格式化字符串在字符串中处理 html 和 xml字符串令牌解析迭代器与生成器顺序迭代合并后的排序迭代对象文件与 IO数据编码和处理函数给函数参数增加元信息将单方法的类转换为函数带额外状态信息的回调函数内联回调函数访问闭包中定义的变量类与对象元编程pycharm基础设置pycharm激活git配置python模板设置快捷键设置项目依赖导出依赖安装依赖离线安装环境迁移Nginx什么是Nginx？Nginx作用Http代理负载均衡动静分离基本使用配置监听关闭nginxNginx常用命令指令防火墙演示 python基础知识 python解释器 CPython 这个解释器是用C语言开发的，所以叫 CPython，在命名行下运行python，就是启动CPython解释器 CPython是使用最广的Python解释器 IPython IPython是基于CPython之上的一个交互式解释器，也就是说，IPython只是在交互方式上有所增强 PyPy PyPy是另一个Python解释器，它的目标是执行速度 PyPy采用JIT技术，对Python代进行动态编译，所以可以显著提高Python代码的执行速度 Jython Jython是运行在Java平台上的Python解释器，可以直接把Python代码编译成Java字节码执行 IronPython IronPython和Jython类似，只不过IronPython是运行在微软.Net平台上的Python解释器 可以直接把Python代码编译成.Net的字节码。 字符集和字符编码 十分钟搞清字符集和字符编码 程序员必备：彻底弄懂常见的7种中文字符编码 计算机屏幕上看到实体化的文字，在计算机存储介质中存放的实际是二进制的比特流 两者之间的转换规则就需要一个统一的标准 字符集 字符集：为每一个「字符」分配一个唯一的 ID（学名为码位 / 码点 / Code Point） 为什么那么多字符集 问题实际非常容易回答。问问自己为什么我们的插头拿到英国就不能用了呢？为什么显示器同时有DVI，VGA，HDMI，DP这么多接口呢？很多规范和标准在最初制定时并不会意识到这将会是以后全球普适的准则，或者处于组织本身利益就想从本质上区别于现有标准。于是，就产生了那么多具有相同效果但又不相互兼容的标准了。 说了那么多我们来看一个实际例子，下面就是屌这个字在各种编码下的十六进制和二进制编码结果，怎么样有没有一种很屌的感觉？ 字符集 16进制编码 对应的二进制数据 UTF-8 0xE5B18C 111001011011000110001100 UTF-16 0x5C4C 101110001001100 GBK 0x8CC5 1000110011000101 常见字符集 ASCII 字符集： ASCII编码每个字母或符号占1byte(8bits)，并且8bits的最高位是0，因此ASCII能编码的字母和符号只有128个。有一些编码把8bits最高位为1的后128个值也编码上，使得1byte可以表示256个值，但是这属于扩展的ASCII，并非标准ASCII。通常所说的标准ASCII只有前128个值！ ASCII编码几乎被世界上所有编码所兼容（UTF16和UTF32是个例外），因此如果一个文本文档里面的内容全都由ASCII里面的字母或符号构成，那么不管你如何展示该文档的内容，都不可能出现乱码的情况。 GB2312 字符集： 最早一版的中文编码，每个字占据2bytes。由于要和ASCII兼容，那这2bytes最高位不可以为0了（否则和ASCII会有冲突）。在GB2312中收录了6763个汉字以及682个特殊符号，已经囊括了生活中最常用的所有汉字。 GBK 字符集： 由于GB2312只有6763个汉字，我汉语博大精深，只有6763个字怎么够？于是GBK中在保证不和GB2312、ASCII冲突（即兼容GB2312和ASCII）的前提下，也用每个字占据2bytes的方式又编码了许多汉字。经过GBK编码后，可以表示的汉字达到了20902个，另有984个汉语标点符号、部首等。值得注意的是这20902个汉字还包含了繁体字，但是该繁体字与台湾Big5编码不兼容，因为同一个繁体字很可能在GBK和Big5中数字编码是不一样的。 Unicode 字符集： Unicode（中文：万国码、国际码、统一码、单一码）是计算机科学领域里的一项业界标准 ASCII编码是1个字节，而Unicode编码通常是2个字节 它对世界上大部分的文字系统进行了整理、编码，使得电脑可以用更为简单的方式来呈现和处理文字 字符编码 编码规则：将「码位」转换为字节序列的规则（编码/解码 可以理解为 加密/解密 的过程） 常见编码方式： ASCII 编码： 将ASCII字符集转换为计算机可以接受的数字系统的数的规则。使用7位（bits）表示一个字符，共128字符；但是7位编码的字符集只能支持128个字符，为了表示更多的欧洲常用字符对ASCII进行了扩展，ASCII扩展字符集使用8位（bits）表示一个字符，共256字符。 UTF-8 编码： UTF-8（8-bit Unicode Transformation Format）是一种针对Unicode的可变长度字符编码，也是一种前缀码。其编码中的第一个字节仍与ASCII兼容，这使得原来处理ASCII字符的软件无须或只须做少部分修改，即可继续使用。 生成器、迭代器和可迭代对象 迭代器和可迭代对象 可迭代对象包含迭代器 如果一个对象拥有iter方法，其是可迭代对象；如果一个对象拥有next方法，其是迭代器 定义可迭代对象，必须实现iter方法；定义迭代器，必须实现iter和next方法 生成器是一种特殊的迭代器 在使用生成器时，我们创建一个函数；在使用迭代器时，我们使用内置函数iter()和next() 在生成器中，我们使用关键字‘yield’来每次生成/返回一个对象 生成器中有多少‘yield’语句，你可以自定义 每次‘yield’暂停循环时，生成器会保存本地变量的状态 迭代器并不会使用局部变量，它只需要一个可迭代对象进行迭代 使用类可以实现你自己的迭代器，但无法实现生成器 生成器运行速度快，语法简洁，更简单 迭代器更能节约内存 可迭代对象，迭代器和生成器之间的关系如下 Pipenv依赖管理工具 Pipenv：新一代Python项目环境与依赖管理工具 pipenv使用指南 使用 pipdeptree 工具来管理依赖树 $ pip install pipdeptree ... $ pipdeptree certifi==2020.6.20 Flask==1.1.2 - click [required: >=5.1, installed: 7.1.2] - itsdangerous [required: >=0.24, installed: 1.1.0] - Jinja2 [required: >=2.10.1, installed: 2.11.3] - MarkupSafe [required: >=0.23, installed: 1.1.1] - Werkzeug [required: >=0.15, installed: 1.0.1] pipdeptree==2.0.0 - pip [required: >=6.0.0, installed: 19.3.1] setuptools==44.0.0.post20200106 wheel==0.36.2 pip-autoremove可以删除依赖包 $ pip install flask $ pip install pip-autoremove $ pip-autoremove flask -y $ pipdeptree certifi==2020.6.20 pip-autoremove==0.9.1 pipdeptree==2.0.0 - pip [required: >=6.0.0, installed: 19.3.1] setuptools==44.0.0.post20200106 wheel==0.36.2 私有仓库 Python 笔记 | 建立python私有仓库 安装pypiserver pip install pypiserver 安装htpasswd的相关依赖 yum install apache2 pip install passlib yum -y install httpd-tools htpasswd生成上传密码 htpasswd -sc ~/pypipasswd.txt user_name 创建python包存放的目录，可以放个whl包进去 mkdir ~/packages 开启服务 pypi-server -p 8282 -P ~/pypipasswd.txt ~/packages pip安装 pip install --extra-index-url http://120.120.120.120:8080/simple/ --trusted-host 120.120.120.120 workbench_toutiao 或者修改pip.ini(linux下是pip.conf)，用命令pip -v config list就可以找到路径 [global] no-cache-dir = true trusted-host = pypi.tuna.tsinghua.edu.cn 192.168.xx.xx:8282 pypi.ngc.nvidia.com index-url = https://pypi.tuna.tsinghua.edu.cn/simple extra-index-url = http://192.168.xx.xx:8282/simple https://pypi.ngc.nvidia.com 上传 用户目录下创建.pypirc文件， 内容如下 [distutils] index-servers = local [local] repository: http://192.168.xx.xx:8282 username: user_name password: password twine upload dist/* # 或者 python setup.py sdist bdist_wheel upload -r http://192.168.xx.xx:8282 查看库中常用模块 import math dir(math) ['__doc__', '__loader__', '__name__', '__package__', '__spec__', 'acos', 'acosh', 'asin', 'asinh', 'atan', 'atan2', ...] linux后台执行py脚本 程序后台运行 nohup python test.py > tt.log>&1 & nohup.out中显示不出来python程序中print的东西，这是因为python的输出有缓冲，导致nohup.out并不能够马上看到输出 python 有个-u参数，使得python不启用缓冲 nohup python -u test.py>tt.log>&1 & 还有一种方式(未尝试过) import sys sys.stdout = Unbuffered(sys.stdout) 代码调试技巧 Python 代码调试技巧 Python 3.7，则无需导入任何内容，只需在代码中要放入调试器的位置调用breakpoint() Some complicated code with bugs breakpoint() Python 3.6及更早版本中，你可以通过显式导入pdb来执行相同的操作，像breakpoint()一样，pdb.set_trace()会将你带入pdb调试器 import pdb; pdb.set_trace() 库和常用方法 变量存储工具方法 什么是酸洗和去除？ Pickle模块接受任何Python对象并将其转换为字符串表示形式，并使用dump函数将其转储到文件中，此过程称为pickling 从存储的字符串表示中检索原始Python对象的过程称为unpickling import dill import pickle as pk def dump_obj(obj, file_name): with open(file_name, 'wb') as f: dill.dump(obj, f) def load_obj(file_name): with open(file_name, \"rb\") as f: try: return dill.load(f) except Exception: return pk.load(f) 序列排序 Python中拥有内置函数实现排序，可以直接调用它们实现排序功能 list.sort(): 直接修改列表 sorted(): 从一个可迭代对象构建一个新的排序列表 不管是 list.sort 方法还是 sorted 函数，都有两个可选的关键字参数 list.sort(cmp=None, key=None, reverse=False) cmp -- 可选参数, 如果指定了该参数会使用该参数的方法进行排序 key -- 主要是用来进行比较的元素，只有一个参数，函数的参数就是取自于可迭代对象中，指定可迭代对象中的一个元素来进行排序 reverse -- 排序规则，reverse = True 降序， reverse = False 升序(默认) sorted(iterable, cmp=None, key=None, reverse=False) erable -- 可迭代对象 cmp -- 比较的函数，这个具有两个参数，参数的值都是从可迭代对象中取出 此函数必须遵守的规则为，大于则返回1，小于则返回-1，等于则返回0。 key -- 主要是用来进行比较的元素，只有一个参数，函数的参数就是取自于可迭代对象中，指定可迭代对象中的一个元素来进行排序 reverse -- 排序规则，reverse = True 降序 ，reverse = False 升序(默认) sort ()与sorted()区别 sort() 是应用在 list 上的方法，sorted() 可以对所有可迭代的对象进行排序操作 list 的 sort() 方法返回的是对已经存在的列表进行操作，无返回值 而内建函数 sorted() 方法返回的是一个新的 list，而不是在原来的基础上进行的操作 单关键字排序 # 默认情况下，sorted()已按升序对输入进行排序，而reverse关键字参数则按降序排序。 >>> sorted(['cat', 'dog', 'cheetah', 'rhino', 'bear'], reverse=True) ['rhino', 'dog', 'cheetah', 'cat', 'bear] >>> animals = [ ... {'type': 'penguin', 'name': 'Stephanie', 'age': 8}, ... {'type': 'elephant', 'name': 'Devon', 'age': 3}, ... {'type': 'puma', 'name': 'Moe', 'age': 5}, ... ] >>> sorted(animals, key=lambda animal: animal['age']) [ {'type': 'elephant', 'name': 'Devon', 'age': 3}, {'type': 'puma', 'name': 'Moe', 'age': 5}, {'type': 'penguin', 'name': 'Stephanie, 'age': 8}, ] # 通过传入一个返回每个元素年龄的lambda函数，可以轻松地按每个字典的单个值对字典列表进行排序。在这种情况下，字典现在按年龄按升序排序。 多关键字排序 # itemgetter() 函数也支持多个 keys rows_by_lfname = sorted(rows, key=itemgetter('lname','fname')) # itemgetter() 有时候也可以用 lambda 表达式代替 rows_by_lfname = sorted(rows, key=lambda r: (r['lname'],r['fname'])) 使用 itemgetter() 方式会运行的稍微快点，如果你对性能要求比较高的话就使用 itemgetter() 方式 import路径问题 import sys sys.path.append(\"..\") 判断是否有中文 def is_chinese(uchar): if u'\\u4e00' 分割字符串并转换类型 import numpy as np line = '12,26,31,17,90,28,88,40,77' npyArray = np.fromstring(line, dtype=int, sep=',') print(npyArray) array([12, 26, 31, 17, 90, 28, 88, 40, 77]) 检查对象是否可调用 def fuunction(param): print(param) print(callable(fuunction)) True 随机打乱多个数组 import numpy as np def shuffledata(*arrs): # 調用案例 x,y = shuffledata(X,Y) arrs = list(arrs) for i, arr in enumerate(arrs): assert len(arrs[0]) == len(arrs[i]) arrs[i] = np.array(arr) p = np.random.permutation(len(arrs[0])) return tuple(arr[p] for arr in arrs) pandas读写csv #!/usr/bin/env Python # -- coding: utf-8 -- \"\"\" @version: v0.1 @author: narutohyc @file: hyc_test.py @Description: @time: 2020/5/28 20:08 \"\"\" import pandas as pd def csv_test(): csv_name = 'my_csv.csv' # 初始化一个带表头的空表 f1 = pd.DataFrame(None, columns=['name', 'price', 'marks']) f1.to_csv(csv_name, mode='w', encoding='utf-8', index=False) # 追加数据到已存在的表 data = {\"name\": ['google', 'baidu', 'yahoo'], \"marks\": [100, 200, 300], \"price\": [1, 2, 3]} f1 = pd.DataFrame(data, columns=['name', 'price', 'marks']) f1.to_csv(csv_name, mode='a', encoding='utf-8', header=False, index=False) # 读取csv,index_col=False表示不存在索引列 f2 = pd.read_csv(csv_name, encoding='utf-8', index_col=False) print() if __name__ == '__main__': csv_test() classmethod 和 staticmethod Python 中的 classmethod 和 staticmethod 有什么具体用途？ 优雅判断数字所属等级 有从 A 到 F 的 5 个等级，现要判断某个数值（从 0 到 1 之间）所属的等级 举例，如数值 >= 0.9，则属于 A；若数值 >= 0.8，则属于 B；以此类推 from typing import Iterable from bisect import bisect def cal_level(score: float, breakpoints: Iterable, name_codes: List): \"\"\" 二分查找 计算分数等级 示例： obj = cal_level(score=78, breakpoints=(60, 70, 80, 90), name_codes='FDCBA') :param score: 得分 :param breakpoints: 分值区间 :param name_codes: 各区间对应结果 :return: 该分值的对应结果 \"\"\" return name_codes[bisect(breakpoints, score)] 取整操作 def learning(): import math # 向上取整 math.ceil(2.3) # 3 math.ceil(2.6) # 3 # 向下取整 math.floor(2.3) # 2 math.floor(2.6) # 2 # 四舍五入 round(6.6) # 7 round(6.4) # 6 round(6.55) # 7 # 只要不是.5的形式，也就是小数位不为5，round基本用法就是四舍五入。除此之外，round用于圆整，保留最近偶数： round(6.5) # 6 是偶数 round(7.5) # 8 是偶数 import torch import torch.nn as nn m = nn.Linear(20, 30) input = torch.randn(128, 20) output = m(input) logger.info(output.shape) # torch.Size([128, 30]) m = nn.Conv2d(16, 33, 3, stride=2) input = torch.randn(20, 16, 50, 100) output = m(input) logger.info(output.shape) # torch.Size([20, 33, 24, 95]) (190 + 2 * 0 - (3 - 1) - 1) / 2 + 1 embedding = nn.Embedding(10, 3) input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 9]]) logger.info(input.shape) # torch.Size([2, 4]) output = embedding(input) logger.info(output.shape) # torch.Size([2, 4, 3]) 腾讯三面：40亿个QQ号码如何去重？ 其他库 哪些 Python 库让你相见恨晚？ python cookbook记录 数据结构和算法 最大或最小的 N 个元素 怎样从一个集合中获得最大或者最小的 N 个元素列表？ heapq 模块有两个函数：nlargest() 和 nsmallest() 可以完美解决这个问题。 import heapq nums = [1, 8, 2, 23, 7, -4, 18, 23, 42, 37, 2] print(heapq.nlargest(3, nums)) # Prints [42, 37, 23] print(heapq.nsmallest(3, nums)) # Prints [-4, 1, 2] 两个函数都能接受一个关键字参数，用于更复杂的数据结构中： portfolio = [ {'name': 'IBM', 'shares': 100, 'price': 91.1}, {'name': 'AAPL', 'shares': 50, 'price': 543.22}, {'name': 'FB', 'shares': 200, 'price': 21.09}, {'name': 'HPQ', 'shares': 35, 'price': 31.75}, {'name': 'YHOO', 'shares': 45, 'price': 16.35}, {'name': 'ACME', 'shares': 75, 'price': 115.65} ] cheap = heapq.nsmallest(3, portfolio, key=lambda s: s['price']) expensive = heapq.nlargest(3, portfolio, key=lambda s: s['price']) 译者注：上面代码在对每个元素进行对比的时候，会以 price 的值进行比较 如果你想在一个集合中查找最小或最大的 N 个元素，并且 N 小于集合元素数量，那么这些函数提供了很好的性能。因为在底层实现里面，首先会先将集合数据进行堆排序后放入一个列表中： nums = [1, 8, 2, 23, 7, -4, 18, 23, 42, 37, 2] import heapq heap = list(nums) heapq.heapify(heap) heap [-4, 2, 1, 23, 7, 2, 18, 23, 42, 37, 8] 堆数据结构最重要的特征是 heap[0] 永远是最小的元素。并且剩余的元素可以很容易的通过调用heapq.heappop() 方法得到，该方法会先将第一个元素弹出来，然后用下一个最小的元素来取代被弹出元素（这种操作时间复杂度仅仅是$Olog(N)$，$N$是堆大小）。 小结： 当要查找的元素个数相对比较小的时候，函数 nlargest() 和 nsmallest() 是很合适的。 如果你仅仅想查找唯一的最小或最大（N=1）的元素的话，那么使用 min() 和max() 函数会更快些。 类似的，如果 N 的大小和集合大小接近的时候，通常先排序这个集合然后再使用切片操作会更快点（sorted(items)[:N] 或者是 sorted(items)[-N:]）。 需要在正确场合使用函数 nlargest() 和 nsmallest() 才能发挥它们的优势（如果N 快接近集合大小了，那么使用排序操作会更好些）。 实现一个优先级队列 怎样实现一个按优先级排序的队列？并且在这个队列上面每次 pop 操作总是返回优先级最高的那个元素 利用 heapq 模块实现了一个简单的优先级队列： import heapq class PriorityQueue: def __init__(self): self._queue = [] self._index = 0 def push(self, item, priority): heapq.heappush(self._queue, (-priority, self._index, item)) self._index += 1 def pop(self): return heapq.heappop(self._queue)[-1] >>> class Item: ... def __init__(self, name): ... self.name = name ... def __repr__(self): ... return 'Item({!r})'.format(self.name) ... >>> q = PriorityQueue() >>> q.push(Item('foo'), 1) >>> q.push(Item('bar'), 5) >>> q.push(Item('spam'), 4) >>> q.push(Item('grok'), 1) >>> q.pop() Item('bar') >>> q.pop() Item('spam') >>> q.pop() Item('foo') >>> q.pop() Item('grok') >>> 仔细观察可以发现，第一个 pop() 操作返回优先级最高的元素。另外注意到如果两个有着相同优先级的元素（foo 和 grok ），pop 操作按照它们被插入到队列的顺序返回的。 在上面代码中，队列包含了一个 (-priority, index, item) 的元组。优先级为负数的目的是使得元素按照优先级从高到低排序。这个跟普通的按优先级从低到高排序的堆排序恰巧相反。 index 变量的作用是保证同等优先级元素的正确排序。通过保存一个不断增加的index 下标变量，可以确保元素按照它们插入的顺序排序。而且，index 变量也在相同优先级元素比较的时候起到重要作用 通过引入另外的 index 变量组成三元组 (priority, index, item) ，就能很好的避免上面的错误，因为不可能有两个元素有相同的 index 值。Python 在做元组比较时候，如果前面的比较已经可以确定结果了，后面的比较操作就不会发生了： a = (1, 0, Item('foo')) b = (5, 1, Item('bar')) c = (1, 2, Item('grok')) a 如果你想在多个线程中使用同一个队列，那么你需要增加适当的锁和信号量机制。 字典的运算 怎样在数据字典中执行一些计算操作（比如求最小值、最大值、排序等等）？ 考虑下面的股票名和价格映射字典： prices = { 'ACME': 45.23, 'AAPL': 612.78, 'IBM': 205.55, 'HPQ': 37.20, 'FB': 10.75} 为了对字典值执行计算操作，通常需要使用 zip() 函数先将键和值反转过来。比如，下面是查找最小和最大股票价格和股票值的代码 min_price = min(zip(prices.values(), prices.keys())) # min_price is (10.75, 'FB') max_price = max(zip(prices.values(), prices.keys())) # max_price is (612.78, 'AAPL') 类似的，可以使用 zip() 和 sorted() 函数来排列字典数据： prices_sorted = sorted(zip(prices.values(), prices.keys())) # prices_sorted is [(10.75, 'FB'), (37.2, 'HPQ'), # (45.23, 'ACME'), (205.55, 'IBM'), # (612.78, 'AAPL')] 如果你在一个字典上执行普通的数学运算，你会发现它们仅仅作用于键，而不是值。 需要注意的是在计算操作中使用到了 (值，键) 对。当多个实体拥有相同的值的时候，键会决定返回结果。比如，在执行 min() 和 max() 操作的时候，如果恰巧最小或最大值有重复的，那么拥有最小或最大键的实体会返回： prices = { 'AAA' : 45.23, 'ZZZ': 45.23 } min(zip(prices.values(), prices.keys())) (45.23, 'AAA') max(zip(prices.values(), prices.keys())) (45.23, 'ZZZ') 查找两字典的相同点 为了寻找两个字典的相同点，可以简单的在两字典的 keys() 或者 items() 方法返回结果上执行集合操作。 # Find keys in common a.keys() & b.keys() # { 'x', 'y' } # Find keys in a that are not in b a.keys() - b.keys() # { 'z' } # Find (key,value) pairs in common a.items() & b.items() # { ('y', 2) } 这些操作也可以用于修改或者过滤字典元素。比如，假如你想以现有字典构造一个排除几个指定键的新字典。下面利用字典推导来实现这样的需求： # Make a new dictionary with certain keys removed c = {key:a[key] for key in a.keys() - {'z', 'w'}} # c is {'x': 1, 'y': 2} 小结： 一个字典就是一个键集合与值集合的映射关系。字典的 keys() 方法返回一个展现键集合的键视图对象。键视图的一个很少被了解的特性就是它们也支持集合操作，比如集合并、交、差运算。所以，如果你想对集合的键执行一些普通的集合操作，可以直接使用键视图对象而不用先将它们转换成一个 set。 字典的 items() 方法返回一个包含 (键，值) 对的元素视图对象。这个对象同样也支持集合操作，并且可以被用来查找两个字典有哪些相同的键值对。 尽管字典的 values() 方法也是类似，但是它并不支持这里介绍的集合操作。某种程度上是因为值视图不能保证所有的值互不相同，这样会导致某些集合操作会出现问题。不过，如果你硬要在值上面执行这些集合操作的话，你可以先将值集合转换成 set，然后再执行集合运算就行了。 删除序列相同元素并保持顺序 如果你想消除元素不可哈希（比如 dict 类型）的序列中重复元素的话 def dedupe(items, key=None): seen = set() for item in items: val = item if key is None else key(item) if val not in seen: yield item seen.add(val) 这里的 key 参数指定了一个函数，将序列元素转换成 hashable 类型。 >>> a = [ {'x':1, 'y':2}, {'x':1, 'y':3}, {'x':1, 'y':2}, {'x':2, 'y':4}] >>> list(dedupe(a, key=lambda d: (d['x'],d['y']))) [{'x': 1, 'y': 2}, {'x': 1, 'y': 3}, {'x': 2, 'y': 4}] >>> list(dedupe(a, key=lambda d: d['x'])) [{'x': 1, 'y': 2}, {'x': 2, 'y': 4}] >>> 如果你想基于单个字段、属性或者某个更大的数据结构来消除重复元素，第二种方案同样可以胜任。 命名切片 内置的 slice() 函数创建了一个切片对象，可以被用在任何切片允许使用的地方。 如果你有一个切片对象 a，你可以分别调用它的 a.start , a.stop , a.step 属性来获取更多的信息。 还能通过调用切片的 indices(size) 方法将它映射到一个确定大小的序列上，这个方法返回一个三元组 (start, stop, step) ，所有值都会被合适的缩小以满足边界限制，从而使用的时候避免出现 IndexError 异常。 SHARES = slice(20, 23) PRICE = slice(31, 37) cost = int(record[SHARES]) * float(record[PRICE]) 字符串和文本 使用多个界定符分割字符串 函数 re.split() 是非常实用的，因为它允许你为分隔符指定多个正则模式。比如，在上面的例子中，分隔符可以是逗号，分号或者是空格，并且后面紧跟着任意个的空格。只要这个模式被找到，那么匹配的分隔符两边的实体都会被当成是结果中的元素返回。返回结果为一个字段列表，这个跟 str.split() 返回值类型是一样的。 当你使用 re.split() 函数时候，需要特别注意的是正则表达式中是否包含一个括号捕获分组。如果使用了捕获分组，那么被匹配的文本也将出现在结果列表中。 >>> fields = re.split(r'(;|,|\\s)\\s*', line) >>> fields ['asdf', ' ', 'fjdk', ';', 'afed', ',', 'fjek', ',', 'asdf', ',', 'foo'] 用 Shell 通配符匹配字符串 使用 Unix Shell 中常用的通配符 (比如 .py , Dat[0-9].csv 等) 去匹配文本字符串 fnmatch 模块提供了两个函数——fnmatch() 和 fnmatchcase() ，可以用来实现这样的匹配。 >>> from fnmatch import fnmatch, fnmatchcase >>> fnmatch('foo.txt', '*.txt') True >>> fnmatch('foo.txt', '?oo.txt') True >>> fnmatch('Dat45.csv', 'Dat[0-9]*') True >>> names = ['Dat1.csv', 'Dat2.csv', 'config.ini', 'foo.py'] >>> [name for name in names if fnmatch(name, 'Dat*.csv')] ['Dat1.csv', 'Dat2.csv'] fnmatch() 函数使用底层操作系统的大小写敏感规则 (不同的系统是不一样的) 来匹配模式。 >>> # On OS X (Mac) >>> fnmatch('foo.txt', '*.TXT') False >>> # On Windows >>> fnmatch('foo.txt', '*.TXT') True 如果你对这个区别很在意，可以使用 fnmatchcase() 来代替。它完全使用你的模 式大小写匹配 >>> fnmatchcase('foo.txt', '*.TXT') False fnmatch() 函数匹配能力介于简单的字符串方法和强大的正则表达式之间。 如果在数据处理操作中只需要简单的通配符就能完成的时候，这通常是一个比较合理的方案。 如果你的代码需要做文件名的匹配，最好使用 glob 模块。 字符串匹配和搜索 匹配或者搜索特定模式的文本 如果你想匹配的是字面字符串，那么你通常只需要调用基本字符串方法就行，比如str.find() , str.endswith() , str.startswith() 或者类似的方法 对于复杂的匹配需要使用正则表达式和 re 模块。为了解释正则表达式的基本原理，假设你想匹配数字格式的日期字符串比如 11/27/2012 ，你可以这样做 >>> text1 = '11/27/2012' >>> text2 = 'Nov 27, 2012' >>> >>> import re >>> # Simple matching: \\d+ means match one or more digits >>> if re.match(r'\\d+/\\d+/\\d+', text1): ... print('yes') ... else: ... print('no') ... yes >>> if re.match(r'\\d+/\\d+/\\d+', text2): ... print('yes') ... else: ... print('no') ... no 如果你想使用同一个模式去做多次匹配，你应该先将模式字符串预编译为模式对象。 match() 总是从字符串开始去匹配，如果你想查找字符串任意部分的模式出现位置，使用 findall() 方法去代替 在定义正则式的时候，通常会利用括号去捕获分组。 datepat = re.compile(r'(\\d+)/(\\d+)/(\\d+)') 捕获分组可以使得后面的处理更加简单，因为可以分别将每个组的内容提取出来。 >>> m = datepat.match('11/27/2012') >>> m >>> # Extract the contents of each group >>> m.group(0) '11/27/2012' >>> m.group(1) '11' >>> m.group(2) '27' >>> m.group(3) '2012' >>> m.groups() ('11', '27', '2012') >>> month, day, year = m.groups() >>> >>> # Find all matches (notice splitting into tuples) >>> text 'Today is 11/27/2012. PyCon starts 3/13/2013.' >>> datepat.findall(text) [('11', '27', '2012'), ('3', '13', '2013')] >>> for month, day, year in datepat.findall(text): ... print('{}-{}-{}'.format(year, month, day)) ... 2012-11-27 2013-3-13 >>> findall() 方法会搜索文本并以列表形式返回所有的匹配。如果你想以迭代方式返回匹配，可以使用 finditer() 方法来代替 核心步骤就是先使用 re.compile() 编译正则表达式字符串，然后使用 match() , findall() 或者 finditer() 等方法。 字符串搜索和替换 对于简单的字面模式，直接使用 str.replace() 方法即可 对于复杂的模式，请使用 re 模块中的 sub() 函数。为了说明这个，假设你想将形式为 11/27/2012 的日期字符串改成 2012-11-27 。示例如下： >>> text = 'Today is 11/27/2012. PyCon starts 3/13/2013.' >>> import re >>> re.sub(r'(\\d+)/(\\d+)/(\\d+)', r'\\3-\\1-\\2', text) 'Today is 2012-11-27. PyCon starts 2013-3-13.' sub() 函数中的第一个参数是被匹配的模式，第二个参数是替换模式。反斜杠数字比如 \\3 指向前面模式的捕获组号。 对于更加复杂的替换，可以传递一个替换回调函数来代替 >>> from calendar import month_abbr >>> def change_date(m): ... mon_name = month_abbr[int(m.group(1))] ... return '{} {} {}'.format(m.group(2), mon_name, m.group(3)) ... >>> datepat.sub(change_date, text) 'Today is 27 Nov 2012. PyCon starts 13 Mar 2013.' >>> 一个替换回调函数的参数是一个 match 对象，也就是 match() 或者 find() 返回的对象。使用 group() 方法来提取特定的匹配部分。回调函数最后返回替换字符串。 如果除了替换后的结果外，你还想知道有多少替换发生了，可以使用 re.subn()来代替。 将 Unicode 文本标准化 删除字符串中不需要的字符 去掉文本字符串开头，结尾或者中间不想要的字符，比如空白。 strip() 方法能用于删除开始或结尾的字符。lstrip() 和 rstrip() 分别从左和从右执行删除操作。默认情况下，这些方法会去除空白字符，但是你也可以指定其他字符。 >>> # Whitespace stripping >>> s = ' hello world \\n' >>> s.strip() 'hello world' >>> s.lstrip() 'hello world \\n' >>> s.rstrip() ' hello world' >>> >>> # Character stripping >>> t = '-----hello=====' >>> t.lstrip('-') 'hello=====' >>> t.strip('-=') 'hello' >>> 这些 strip() 方法在读取和清理数据以备后续处理的时候是经常会被用到的。比如，你可以用它们来去掉空格，引号和完成其他任务。 但是需要注意的是去除操作不会对字符串的中间的文本产生任何影响。 如果你想处理中间的空格，那么你需要求助其他技术。比如使用 replace() 方法或者是用正则表达式替换。 审查清理文本字符串 一些无聊的幼稚黑客在你的网站页面表单中输入文本”pýtĥöñ”，然后你想将这些字符清理掉。 文本清理问题会涉及到包括文本解析与数据处理等一系列问题。在非常简单的情形下，你可能会选择使用字符串函数 (比如 str.upper() 和 str.lower() ) 将文本转为标准格式。使用 str.replace() 或者 re.sub() 的简单替换操作能删除或者改变指定的字符序列。你同样还可以使用 2.9 小节的 unicodedata.normalize() 函数将 nicode 文本标准化。 字符串对齐 对于基本的字符串对齐操作，可以使用字符串的 ljust() , rjust() 和 center()方法。 >>> text = 'Hello World' >>> text.ljust(20) 'Hello World ' >>> text.rjust(20) ' Hello World' >>> text.center(20) ' Hello World ' 所有这些方法都能接受一个可选的填充字符。 >>> text.rjust(20,'=') '=========Hello World' >>> text.center(20,'*') '****Hello World*****' 函数 format() 同样可以用来很容易的对齐字符串。你要做的就是使用 或者 ^字符后面紧跟一个指定的宽度。 >>> format(text, '>20') ' Hello World' >>> format(text, '>> format(text, '^20') ' Hello World ' >>> 如果你想指定一个非空格的填充字符，将它写到对齐字符的前面即可 >>> format(text, '=>20s') '=========Hello World' >>> format(text, '*^20s') '****Hello World*****' 当格式化多个值的时候，这些格式代码也可以被用在 format() 方法中。 >>> '{:>10s} {:>10s}'.format('Hello', 'World') ' Hello World' format() 函数的一个好处是它不仅适用于字符串。它可以用来格式化任何值，使得它非常的通用。比如，你可以用它来格式化数字： >>> x = 1.2345 >>> format(x, '>10') ' 1.2345' >>> format(x, '^10.2f') ' 1.23 ' 在新版本代码中，你应该优先选择 format() 函数或者方法。format() 要比% 操作符的功能更为强大。并且 format() 也比使用 ljust() , rjust() 或 center() 方法更通用，因为它可以用来格式化任意对象，而不仅仅是字符串。 以指定列宽格式化字符串 使用 textwrap 模块来格式化字符串的输出。比如，假如你有下列的长字符串： s = \"Look into my eyes, look into my eyes, the eyes, the eyes, \\ the eyes, not around the eyes, don't look around the eyes, \\ look into my eyes, you're under.\" 下面演示使用 textwrap 格式化字符串的多种方式： >>> import textwrap >>> print(textwrap.fill(s, 70)) Look into my eyes, look into my eyes, the eyes, the eyes, the eyes, not around the eyes, don't look around the eyes, look into my eyes, you're under. >>> print(textwrap.fill(s, 40)) Look into my eyes, look into my eyes, the eyes, the eyes, the eyes, not around the eyes, don't look around the eyes, look into my eyes, you're under. >>> print(textwrap.fill(s, 40, initial_indent=' ')) Look into my eyes, look into my eyes, the eyes, the eyes, the eyes, not around the eyes, don't look around the eyes, look into my eyes, you're under. >>> print(textwrap.fill(s, 40, subsequent_indent=' ')) Look into my eyes, look into my eyes, the eyes, the eyes, the eyes, not around the eyes, don't look around the eyes, look into my eyes, you're under. textwrap 模块对于字符串打印是非常有用的，特别是当你希望输出自动匹配终端大小的时候。你可以使用 os.get_terminal_size() 方法来获取终端的大小尺寸。 >>> import os >>> os.get_terminal_size().columns 80 fill() 方法接受一些其他可选参数来控制 tab，语句结尾等。 在字符串中处理 html 和 xml 字符串令牌解析 迭代器与生成器 顺序迭代合并后的排序迭代对象 有一系列排序序列，想将它们合并后得到一个排序序列并在上面迭代遍历 heapq.merge() 函数可以帮你解决这个问题 >>> import heapq >>> a = [1, 4, 7, 10] >>> b = [2, 5, 6, 11] >>> for c in heapq.merge(a, b): ... print(c) ... 1 2 4 5 6 7 10 11 heapq.merge 可迭代特性意味着它不会立马读取所有序列。这就意味着你可以在非常长的序列中使用它，而不会有太大的开销。 要强调的是 heapq.merge() 需要所有输入序列必须是排过序的。特别的，它并不会预先读取所有数据到堆栈中或者预先排序，也不会对输入做任何的排序检测。 它仅仅是检查所有序列的开始部分并返回最小的那个，这个过程一直会持续直到所有输入序列中的元素都被遍历完。 文件与 IO 数据编码和处理 函数 给函数参数增加元信息 写好了一个函数，然后想为这个函数的参数增加一些额外的信息，这样的话其他使用者就能清楚的知道这个函数应该怎么使用。 使用函数参数注解是一个很好的办法，它能提示程序员应该怎样正确使用这个函数。例如，下面有一个被注解了的函数： def add(x:int, y:int) -> int: return x + y python 解释器不会对这些注解添加任何的语义。它们不会被类型检查，运行时跟没有加注解之前的效果也没有任何差距。 尽管你可以使用任意类型的对象给函数添加注解 (例如数字，字符串，对象实例等等)，不过通常来讲使用类或者字符串会比较好点。 函数注解只存储在函数的 __annotations__ 属性中 >>> add.__annotations__ {'y': , 'return': , 'x': } 尽管注解的使用方法可能有很多种，但是它们的主要用途还是文档。 因为 python并没有类型声明，通常来讲仅仅通过阅读源码很难知道应该传递什么样的参数给这个函数。 这时候使用注解就能给程序员更多的提示，让他们可以正确的使用函数。 ps：这里可以结合元类，为参数做类型检测（narutohyc个人想法，还未实现） 将单方法的类转换为函数 你有一个除 __init__() 方法外只定义了一个方法的类。为了简化代码，你想将它转换成一个函数 def urltemplate(template): def opener(**kwargs): return urlopen(template.format_map(kwargs)) return opener # Example use yahoo = urltemplate('http://finance.yahoo.com/d/quotes.csv?s={names}&f=,→ {fields}') 大部分情况下，你拥有一个单方法类的原因是需要存储某些额外的状态来给方法使用。 带额外状态信息的回调函数 你的代码中需要依赖到回调函数的使用 (比如事件处理器、等待后台任务完成后的回调等)，并且你还需要让回调函数拥有额外的状态值，以便在它的内部使用到。 内联回调函数 当你编写使用回调函数的代码的时候，担心很多小函数的扩张可能会弄乱程序控制流。你希望找到某个方法来让代码看上去更像是一个普通的执行序列。 通过使用生成器和协程可以使得回调函数内联在某个函数中。 为了演示说明，假设你有如下所示的一个执行某种计算任务然后调用一个回调函数的函数 def apply_async(func, args, *, callback): # Compute the result result = func(*args) # Invoke the callback with the result callback(result) 接下来让我们看一下下面的代码，它包含了一个 Async 类和一个 inlined_async装饰器： from queue import Queue from functools import wraps class Async: def __init__(self, func, args): self.func = func self.args = args def inlined_async(func): @wraps(func) def wrapper(*args): f = func(*args) result_queue = Queue() result_queue.put(None) while True: result = result_queue.get() try: a = f.send(result) apply_async(a.func, a.args, callback=result_queue.put) except StopIteration: break return wrapper 这两个代码片段允许你使用 yield 语句内联回调步骤。比如： def add(x, y): return x + y @inlined_async def test(): r = yield Async(add, (2, 3)) print(r) r = yield Async(add, ('hello', 'world')) print(r) for n in range(10): r = yield Async(add, (n, n)) print(r) print('Goodbye') 如果你调用 test() ，你会得到类似如下的输出： 5 helloworld 0 2 4 6 8 10 12 14 16 18 Goodbye 你会发现，除了那个特别的装饰器和 yield 语句外，其他地方并没有出现任何的回调函数 (其实是在后台定义的)。 本小节会实实在在的测试你关于回调函数、生成器和控制流的知识。 首先，在需要使用到回调的代码中，关键点在于当前计算工作会挂起并在将来的某个时候重启 (比如异步执行)。当计算重启时，回调函数被调用来继续处理结果。applyasync() 函数演示了执行回调的实际逻辑，尽管实际情况中它可能会更加复杂(包括线程、进程、事件处理器等等)。 计算的暂停与重启思路跟生成器函数的执行模型不谋而合。具体来讲，yield 操作会使一个生成器函数产生一个值并暂停。接下来调用生成器的 \\_next__() 或 send()方法又会让它从暂停处继续执行。 根据这个思路，这一小节的核心就在 inline_async() 装饰器函数中了。关键点就是，装饰器会逐步遍历生成器函数的所有 yield 语句，每一次一个。为了这样做，刚开始的时候创建了一个 result 队列并向里面放入一个 None 值。然后开始一个循环操作，从队列中取出结果值并发送给生成器，它会持续到下一个 yield 语句，在这里一个 Async 的实例被接受到。然后循环开始检查函数和参数，并开始进行异步计算apply_async() 。然而，这个计算有个最诡异部分是它并没有使用一个普通的回调函数，而是用队列的 put() 方法来回调。 这时候，是时候详细解释下到底发生了什么了。主循环立即返回顶部并在队列上执行 get() 操作。如果数据存在，它一定是 put() 回调存放的结果。如果没有数据，那么先暂停操作并等待结果的到来。这个具体怎样实现是由 apply_async() 函数来决定的。如果你不相信会有这么神奇的事情，你可以使用 multiprocessing 库来试一下，在单独的进程中执行异步计算操作，如下所示： if __name__ == '__main__': import multiprocessing pool = multiprocessing.Pool() apply_async = pool.apply_async # Run the test function test() 实际上你会发现这个真的就是这样的，但是要解释清楚具体的控制流得需要点时间了。 将复杂的控制流隐藏到生成器函数背后的例子在标准库和第三方包中都能看到。比如，在 contextlib 中的 @contextmanager 装饰器使用了一个令人费解的技巧，通过一个 yield 语句将进入和离开上下文管理器粘合在一起。另外非常流行的 Twisted 包中也包含了非常类似的内联回调。 访问闭包中定义的变量 类与对象 元编程 pycharm基础设置 如果已经有setting.zip可以直接导入即可 ./res/5. python基础知识/settings.zip pycharm激活 idea PyCharm 全家桶激活码 - lookdiv 秘钥: lookdiv.com git配置 配置git路径 Setting->Version Control->git Path to Git executable: S:\\Git\\cmd\\git.exe python模板设置 打开Setting->Editor->File and Code Templates->Files->Python Script 配置如下信息(修改作者名，其他无需修改)： #!/usr/bin/env Python # -- coding: utf-8 -- \"\"\" @version: v0.1 @author: narutohyc @file: ${NAME}.py @Description: @time: ${DATE} ${TIME} \"\"\" 快捷键设置 操作 快捷键 补充说明 Reformat Code Ctrl+Alt+F 格式化代码 Close Ctrl+R 关闭当前py文件窗口 Split Vertically Ctrl+Shift+向右箭头 垂直切分窗口 Split Horizontally Ctrl+Shift+向左箭头 水平切分窗口 Git Pull Ctrl+Shift+L git pull Git commit File Ctrl+K git commit Git Commit and push Ctrl+Shift+K git push Change font size ctrl+N->Actions->Change font size with Ctrl+Mouse Wheel Show in Explorer Crtl+Shift+S 项目依赖 导出依赖 安装pipreqs pip install pipreqs 导出依赖, 强制覆盖原先的依赖 pipreqs ./ --encoding=utf8 --force 安装依赖 安装依赖库 pip install -r requirements.txt 离线安装 单个下载 pip download package_name -d \"下载的路径(windows下双引号来表示文件夹)\" 批量下载 pip download -d /tmp/packages -r requirements.txt 批量安装已经导出的包 其中 --no-index 代表忽视pip 忽视默认的依赖包索引 --find-links= 代表从你指定的目录寻下找离线包 pip install --no-index --find-links=/tmp/packages -r requirements.txt 环境迁移 基于anaconda虚拟环境迁移 新建虚拟环境 切换到虚拟环境 安装conda install conda-pack 打包： conda pack -n 自己的虚拟环境名 -o env_name.tar.gz --ignore-missing-files 目标服务器上，在env_name.tar.gz文件同目录下创建restore.py文件 # 获取环境的 tar.gz 文件，env_name 是要打包的环境名 # conda install conda-pack # conda pack -n env_name import os import tarfile name = 'bilu' file_name = '/home/faduit/huangyc/install_packages/bilu.tar.gz' new_env_path = f'/home/faduit/huangyc/anaconda3/envs/{name}' # ananconda 存放环境的路径 if not os.path.exists(new_env_path): os.mkdir(new_env_path) def untar(file_name, dir): t_file = tarfile.open(file_name) t_file.extractall(new_env_path) untar(file_name, new_env_path) 以上文件中需要改 3 个地方（目标服务器上需要已经安装好 Anaconda） name = '' # 给环境赋一个新的名字 file_name = '/home/username/env_name.tar.gz' # file_name 改成自己的 .tar.gz 文件的绝对路径 new_env_path = f'/home/username/anaconda3/envs/{name}' # 此路径中 username 改为自己的用户名 改好后执行脚本文件即可 激活 # Activate the environment. This adds `my_env/bin` to your path source my_env/bin/activate Nginx Nginx快速入门 什么是Nginx？ Nginx (engine x) 是一个高性能的HTTP和反向代理web服务器，同时也提供了IMAP/POP3/SMTP服务。Nginx是由伊戈尔·赛索耶夫为俄罗斯访问量第二的Rambler.ru站点（俄文：Рамблер）开发的，第一个公开版本0.1.0发布于2004年10月4日。2011年6月1日，nginx 1.0.4发布。 其特点是占有内存少，并发能力强，事实上nginx的并发能力在同类型的网页服务器中表现较好，中国大陆使用nginx网站用户有：百度、京东、新浪、网易、腾讯、淘宝等。在全球活跃的网站中有12.18%的使用比率，大约为2220万个网站。 Nginx 是一个安装非常的简单、配置文件非常简洁（还能够支持perl语法）、Bug非常少的服务。Nginx 启动特别容易，并且几乎可以做到7*24不间断运行，即使运行数个月也不需要重新启动。你还能够不间断服务的情况下进行软件版本的升级。 Nginx代码完全用C语言从头写成。官方数据测试表明能够支持高达 50,000 个并发连接数的响应。 Nginx作用 Http代理 Http代理，反向代理：作为web服务器最常用的功能之一，尤其是反向代理。 正向代理 反向代理 负载均衡 Nginx提供的负载均衡策略有2种：内置策略和扩展策略。内置策略为轮询，加权轮询，Ip hash。扩展策略，就天马行空，只有你想不到的没有他做不到的。 轮询 加权轮询 iphash 对客户端请求的ip进行hash操作，然后根据hash结果将同一个客户端ip的请求分发给同一台服务器进行处理，可以解决session不共享的问题。 fair 按后端服务器的响应时间来分配请求，响应时间短的优先分配。 url_hash 按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。 动静分离 动静分离，在我们的软件开发中，有些请求是需要后台处理的，有些请求是不需要经过后台处理的（如：css、html、jpg、js等等文件），这些不需要经过后台处理的文件称为静态文件。让动态网站里的动态网页根据一定规则把不变的资源和经常变的资源区分开来，动静资源做好了拆分以后，我们就可以根据静态资源的特点将其做缓存操作。提高资源响应的速度。 基本使用 配置监听 nginx的配置文件是conf目录下的nginx.conf，默认配置的nginx监听的端口为80，如果80端口被占用可以修改为未被占用的端口即可。 当我们修改了nginx的配置文件nginx.conf 时，不需要关闭nginx后重新启动nginx，只需要执行命令 nginx -s reload 即可让改动生效 关闭nginx 如果使用cmd命令窗口启动nginx， 关闭cmd窗口是不能结束nginx进程的，可使用两种方法关闭nginx (1)输入nginx命令 nginx -s stop(快速停止nginx) 或 nginx -s quit(完整有序的停止nginx) (2)使用taskkill taskkill /f /t /im nginx.exe taskkill是用来终止进程的， /f是强制终止 . /t终止指定的进程和任何由此启动的子进程。 /im示指定的进程名称 . Nginx常用命令 指令 cd /usr/local/nginx/sbin/ ./nginx 启动 ./nginx -s stop 停止 ./nginx -s quit 安全退出 ./nginx -s reload 重新加载配置文件 ps aux|grep nginx 查看nginx进程 防火墙 # 开启 service firewalld start # 重启 service firewalld restart # 关闭 service firewalld stop # 查看防火墙规则 firewall-cmd --list-all # 查询端口是否开放 firewall-cmd --query-port=8080/tcp # 开放80端口 firewall-cmd --permanent --add-port=80/tcp # 移除端口 firewall-cmd --permanent --remove-port=8080/tcp #重启防火墙(修改配置后要重启防火墙) firewall-cmd --reload # 参数解释 1、firwall-cmd：是Linux提供的操作firewall的一个工具； 2、--permanent：表示设置为持久； 3、--add-port：标识添加的端口； 演示 upstream lb{ server 127.0.0.1:8080 weight=1; server 127.0.0.1:8081 weight=1; } location / { proxy_pass http://lb; } Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-12-01 00:57:11 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/2.python多线程.html":{"url":"chapters/2.python多线程.html","title":"python多线程","keywords":"","body":"python多线程 python多线程 Python多线程学习 多线程用比较少 Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-12-01 00:57:11 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/1.python多进程.html":{"url":"chapters/1.python多进程.html","title":"python多进程","keywords":"","body":"python多进程基本概念任务类型多进程实现方式Process进程池其他进程池多进程通信进程队列QueueJoinableQueue队列管道PipeManagers注意事项无法调用多层生成器(待验证) python多进程 基本概念 Python中的多进程是通过multiprocessing包来实现的，和多线程的threading.Thread差不多，它可以利用multiprocessing.Process对象来创建一个进程对象 这个进程对象的方法和线程对象的方法差不多也有start()，run()，join()等方法，其中有一个方法不同Thread线程对象中的守护线程方法是setDeamon，而Process进程对象的守护进程是通过设置daemon属性来完成的 与多线程的共享式内存不同，由于各个进程都是相互独立的，因此进程间通信再多进程中扮演这非常重要的角色，Python中我们可以使用multiprocessing模块中的pipe、queue、Array、Value等等工具来实现进程间通讯和数据共享，但是在编写起来仍然具有很大的不灵活性 任务类型 同步与异步 同步就是指一个进程在执行某个请求的时候，若该请求需要一段时间才能返回信息 那么这个进程将会一直等待下去，直到收到返回信息才继续执行下去 异步是指进程不需要一直等下去，而是继续执行下面的操作，不管其他进程的状态 当有消息返回时系统会通知进程进行处理，这样可以提高执行的效率 IO密集和计算密集 对于IO密集型任务: python的多线程能够节省时间 对于计算(CPU)密集型任务: Python的多线程并没有用处，建议使用多进程 其他组合搭配 python使用多核，即开多个进程 方法一: 协程+多进程，使用方法简单，效率还可以，一般使用该方法 协程yield是你自己写的，是自己定义什么时候切换进程 方法二：IO多路复用，使用复杂，但效率很高，不常用 多进程相关模块 # 创建管理进程模块： Process(用于创建进程):通过创建一个Process对象然后调用它的start()方法来生成进程。Process遵循threading.Thread的API。 Pool(用于创建进程管理池)：可以创建一个进程池，该进程将执行与Pool该类一起提交给它的任务，当子进程较多需要管理时使用。 Queue（用于进程通信，资源共享）：进程间通信，保证进程安全。 Value，Array（用于进程通信，资源共享）： Pipe（用于管道通信）：管道操作。 Manager（用于资源共享）：创建进程间共享的数据，包括在不同机器上运行的进程之间的网络共享。 # 同步子进程模块： Condition Event：用来实现进程间同步通信。 Lock：当多个进程需要访问共享资源的时候，Lock可以用来避免访问的冲突。 RLock Semaphore：用来控制对共享资源的访问数量，例如池的最大连接数。 python多线程低效原因 GIL的全称是 Global Interpreter Lock(全局解释器锁)，来源是 Python 设计之初的考虑，为了数据安全所做的决定 某个线程想要执行，必须先拿到 GIL，我们可以把 GIL 看作是“通行证”，并且在一个 Python 进程中，GIL 只有一个 拿不到通行证的线程，就不允许进入 CPU 执行 目前 Python 的解释器有多种，例如： CPython：CPython 是用C语言实现的 Python 解释器，作为官方实现，它是最广泛使用的 Python 解释器 PyPy：PyPy 是用RPython实现的解释器。RPython 是 Python 的子集， 具有静态类型。这个解释器的特点是即时编译，支持多重后端(C, CLI, JVM)。PyPy 旨在提高性能，同时保持最大兼容性(参考 CPython 的实现) Jython：Jython 是一个将 Python 代码编译成 Java 字节码的实现，运行在JVM(Java Virtual Machine)上。另外，它可以像是用 Python 模块一样，导入 并使用任何Java类 IronPython：IronPython 是一个针对 .NET 框架的 Python 实现。它 可以用 Python 和 .NET framewor k的库，也能将 Python 代码暴露给 .NET 框架中的其他语言 GIL 只在 CPython 中才有，而在 PyPy 和 Jython 中是没有 GIL 的 注意: 每次释放GIL锁，线程进行锁竞争、切换线程，会消耗资源 这就导致打印线程执行时长，会发现耗时更长的原因 并且由于 GIL 锁存在，Python 里一个进程永远只能同时执行一个线程(拿到 GIL 的线程才能执行)，这就是为什么在多核CPU上，Python 的多线程效率并不高的根本原因 多进程实现方式 Process 普通Process from multiprocessing import Process def func(name): print('测试%s多进程' %name) if __name__ == '__main__': process_list = [] for i in range(5): #开启5个子进程执行fun1函数 p = Process(target=func, args=('Python',)) #实例化进程对象 p.start() process_list.append(p) for i in process_list: p.join() print('结束测试') 测试Python多进程 测试Python多进程 测试Python多进程 测试Python多进程 测试Python多进程 结束测试 Process finished with exit code 0 上面的代码开启了5个子进程去执行函数，我们可以观察结果，是同时打印的，这里实现了真正的并行操作，就是多个CPU同时执行任务。 我们知道进程是python中最小的资源分配单元，也就是进程中间的数据，内存是不共享的，每启动一个进程，都要独立分配资源和拷贝访问的数据，所以进程的启动和销毁的代价是比较大了，所以在实际中使用多进程，要根据服务器的配置来设定。 继承Process from multiprocessing import Process class MyProcess(Process): #继承Process类 def __init__(self,name): super(MyProcess,self).__init__() self.name = name def run(self): print('测试%s多进程' % self.name) if __name__ == '__main__': process_list = [] for i in range(5): #开启5个子进程执行fun1函数 p = MyProcess('Python') #实例化进程对象 p.start() process_list.append(p) for i in process_list: p.join() print('结束测试') 测试Python多进程 测试Python多进程 测试Python多进程 测试Python多进程 测试Python多进程 结束测试 Process finished with exit code 0 通过类继承的方法来实现的，python多进程的第二种实现方式也是一样的，效果和第一种方式一样 Process类的其他方法 构造方法： Process([group [, target [, name [, args [, kwargs]]]]]) 　　group: 线程组 　　target: 要执行的方法 　　name: 进程名 　　args/kwargs: 要传入方法的参数 实例方法： 　　is_alive()：返回进程是否在运行,bool类型。 　　join([timeout])：阻塞当前上下文环境的进程程，直到调用此方法的进程终止或到达指定的timeout（可选参数）。 　　start()：进程准备就绪，等待CPU调度 　　run()：strat()调用run方法，如果实例进程时未制定传入target，这star执行t默认run()方法。 　　terminate()：不管任务是否完成，立即停止工作进程 属性： 　　daemon：和线程的setDeamon功能一样 　　name：进程名字 　　pid：进程号 进程池 # apply_async：异步 from multiprocessing import Pool,cpu_count import os, time, random def fun1(name): print('Run task %s (%s)...' % (name, os.getpid())) start = time.time() time.sleep(random.random() * 3) end = time.time() print('Task %s runs %0.2f seconds.' % (name, (end - start))) return f'{name}: {os.getpid()}' if __name__=='__main__': results = [] pool = Pool(cpu_count()-1) for i in range(4): results.append(pool.apply_async(func=fun1, args=(i,))) pool.close() pool.join() print() for result in results: print(result.get()) print('All Done!!!') print('结束测试') Run task 0 (30716)... Run task 1 (15020)... Run task 2 (23200)... Run task 3 (5884)... Task 0 runs 1.34 seconds. Task 2 runs 1.53 seconds. Task 1 runs 1.88 seconds. Task 3 runs 2.48 seconds. 0: 30716 1: 15020 2: 23200 3: 5884 All Done!!! 结束测试 Process finished with exit code 0 对Pool对象调用join()方法会等待所有子进程执行完毕，调用join()之前必须先调用close()，调用close()之后就不能继续添加新的Process了 # map_async：异步 from multiprocessing import Pool, cpu_count, Manager from functools import partial def job(data, mgrDicTask, lock): res = f'a+b = {data[0] + data[1]}' lock.acquire() # Manager对象无法监测到它引用的可变对象值的修改，需要通过触发__setitem__方法来让它获得通知 tempDic = list(mgrDicTask['result']) tempDic.append(res) mgrDicTask['result'] = tempDic lock.release() return res if __name__ == \"__main__\": data = [[2, 3], [3, 4], [2, 5]] pool = Pool(processes=cpu_count() - 1) mgr = Manager() lock = mgr.Lock() mgrDicTask = mgr.dict() mgrDicTask['result'] = [] fun = partial(job, mgrDicTask=mgrDicTask, lock=lock) pool.map_async(fun, data) pool.close() pool.join() print(mgrDicTask['result']) print('All Done!!!') ['a+b = 7', 'a+b = 7', 'a+b = 5'] All Done!!! Process finished with exit code 0 其他进程池 # 进程池的另外一种创建方式，跟线程池的创建方式一样。其方法等也相同。 def process_pool_test(url_list): book_list = [] # 创建进程池 pool = ProcessPoolExecutor(max_workers=20) start = time.time() for url in url_list: time.sleep(0.5) result = pool.submit(get_book_info, url) book_list.append(result) pool.shutdown() print('time: ', time.time() - start) book_name_list = [] author_list = [] author_info_list = [] print('book_list: ', len(book_list)) for future in book_list: book_name_list.extend(future.result()['name']) author_list.extend(future.result()['author']) author_info_list.extend(future.result()['info']) ExcelUtils.write_data_to_excel('bookInfo', book_name_list, author_list, author_info_list) if __name__ == '__main__': sys.setrecursionlimit(10000) url_list = ['https://www.edge.org/library'] for i in range(1, 52): url_list.append('https://www.edge.org/library?page=%s' % i) thread_pool_test(url_list) 多进程通信 内容提取神器 beautiful Soup 的用法 进程是系统独立调度核分配系统资源(CPU、内存)的基本单位，进程之间是相互独立的，每启动一个新的进程相当于把数据进行了一次克隆，子进程里的数据修改无法影响到主进程中的数据，不同子进程之间的数据也不能共享，这是多进程在使用中与多线程最明显的区别 但是难道Python多进程中间难道就是孤立的吗？ 当然不是，python也提供了多种方法实现了多进程中间的通信和数据共享(可以修改一份数据) 进程队列Queue #!/usr/bin/env Python # -- coding: utf-8 -- \"\"\" @version: v1.0 @author: narutohyc @file: multiprocessing_queue.py @Description: 多进程队列使用示例 @time: 2020/5/14 15:53 \"\"\" from multiprocessing import Process, Queue, Manager from multiprocessing import cpu_count import os import time class Task: def __init__(self, task_name: str, data: list, **kwargs): self.task_name = task_name self.data = data def __repr__(self): return f'task_name:{self.task_name} data:{self.data}' class MultiProcessingQueue: def __init__(self): # 进程数 self.num_of_worker = cpu_count() # 进程队列大小，根据不同的任务需求 self.size_of_queue = 10 def start_work(self): print(\"start_work 开始\") # 进程队列 process_list = [] # 新建一个大小为10的队列 work_queue = Queue(self.size_of_queue) # 进程间共享列表, 其他的还有共享字典等，都是进程安全的 dealed_sample_lst = Manager().list() # 一个生产者 sent = Process(target=self.productor, args=(work_queue, dealed_sample_lst,)) sent.start() process_list.append(sent) # 多个消费者 for _ in range(self.num_of_worker - 1): process = Process(target=self.consumer, args=(work_queue, dealed_sample_lst,)) process.start() process_list.append(process) [process.join() for process in process_list] print(\"start_work 结束\") return dealed_sample_lst def productor(self, work_queue: Queue, dealed_sample_lst): print(\"生产者开始工作\") for ii in range(100): work_queue.put(Task(task_name=f'{str(os.getpid())}-{str(ii)}', data=[ii for _ in range(2)])) if ii % 30 == 0: time.sleep(1) print(\"生产者休息ing\") ''' JoinableQueue 比Queue多了task_done() 与join()两个函数，多用于生产者消费者问题。 task_done()是用在get()后，发送通知说我get完了 join()是说Queue里所有的task都已处理。 ''' # 这里需要加入结束标识，还有就是JoinableQueue的方式 for _ in range(self.num_of_worker - 1): work_queue.put(None) print(\"生产者工作结束\") def consumer(self, work_queue: Queue, dealed_sample_lst): while True: task: Task = work_queue.get() if task is None: break # 处理数据 task.data = [ii * 2 for ii in task.data] dealed_sample_lst.append(task) print(task) print(f'进程{os.getpid()} 处理结束') def multiprocessing_queue_test(): multiprocessing_queue = MultiProcessingQueue() dealed_sample_lst = multiprocessing_queue.start_work() # for sample in dealed_sample_lst: # print(sample) print(\"测试结束\") if __name__ == '__main__': multiprocessing_queue_test() start_work 开始 生产者开始工作 task_name:28868-0 data:[0, 0] 生产者休息ing task_name:28868-1 data:[2, 2] ... task_name:28868-6 data:[12, 12] 生产者休息ing task_name:28868-31 data:[62, 62] ... task_name:28868-58 data:[116, 116] 生产者休息ing task_name:28868-61 data:[122, 122] ... task_name:28868-64 data:[128, 128] ... 生产者休息ing task_name:28868-91 data:[182, 182] ... task_name:28868-96 data:[192, 192] 生产者工作结束 进程29208 处理结束 task_name:28868-97 data:[194, 194] 进程20632 处理结束 进程28496 处理结束 task_name:28868-98 data:[196, 196] 进程30200 处理结束 进程26512 处理结束 进程29776 处理结束 task_name:28868-99 data:[198, 198] 进程30072 处理结束 start_work 结束 测试结束 上面的代码结果可以看到我们主进程中可以通过Queue获取子进程中put的数据，实现进程间的通信 JoinableQueue队列 JoinableQueue([maxsize])：这就像是一个Queue对象，但队列允许项目的使用者通知生成者项目已经被成功处理 通知进程是使用共享的信号和条件变量来实现的 参数介绍： maxsize: 是队列中允许最大项数，省略则无大小限制 方法介绍： q.task_done()：使用者使用此方法发出信号，表示q.get()的返回项目已经被处理 如果调用此方法的次数大于从队列中删除项目的数量将引发ValueError异常 q.join():生产者调用此方法进行阻塞，直到队列中所有的项目均被处理 阻塞将持续到队列中的每个项目均调用q.task_done()方法为止 示例代码 #!/usr/bin/env Python # -- coding: utf-8 -- \"\"\" @version: v1.0 @author: narutohyc @file: multiprocessing_queue.py @Description: 多进程队列使用示例 @time: 2020/5/14 15:53 \"\"\" from multiprocessing import Process, Queue, JoinableQueue, Manager from multiprocessing import cpu_count import os, time class Task: def __init__(self, task_name: str, data: list, **kwargs): self.task_name = task_name self.data = data def __repr__(self): return f'task_name:{self.task_name} data:{self.data}' class MultiProcessingJoinableQueue: def __init__(self): # 进程数 self.num_of_worker = cpu_count() # 进程队列大小，根据不同的任务需求 self.size_of_queue = 10 def start_work(self): print(\"start_work 开始\") # 进程队列 process_list = [] # 新建一个大小为10的队列 work_queue = JoinableQueue(self.size_of_queue) # 进程间共享列表, 其他的还有共享字典等，都是进程安全的 dealed_sample_lst = Manager().list() # 一个生产者 sent = Process(target=self.productor, args=(work_queue, dealed_sample_lst,)) process_list.append(sent) # 多个消费者 for _ in range(self.num_of_worker - 1): process = Process(target=self.consumer, args=(work_queue, dealed_sample_lst,)) process.daemon = True process_list.append(process) [process.start() for process in process_list] # 这里需要注意的一点是，这里join只需要调用生产者(别调消费者的join，否则无法正常退出) # 消费者不需要，个人感觉应该是生产者那边已经调用了work_queue.join()的方法 # 消费者结束后，整个程序退出 [process.join() for process in process_list[:1]] print(\"start_work 结束\") return dealed_sample_lst def productor(self, work_queue: Queue, dealed_sample_lst): print(\"生产者开始工作\") for ii in range(100): work_queue.put(Task(task_name=f'{str(os.getpid())}-{str(ii)}', data=[ii for _ in range(2)])) if ii % 30 == 0: time.sleep(1) print(\"生产者休息ing\") print(\"生产者工作结束\") work_queue.join() def consumer(self, work_queue: Queue, dealed_sample_lst): while True: task: Task = work_queue.get() if task is None: break # 处理数据 task.data = [ii * 2 for ii in task.data] dealed_sample_lst.append(task) print(task) work_queue.task_done() print(f'进程{os.getpid()} 处理结束') def multiprocessing_joinablequeue_test(): multiprocessing_joinablequeue = MultiProcessingJoinableQueue() dealed_sample_lst = multiprocessing_joinablequeue.start_work() # for sample in dealed_sample_lst: # print(sample) print(\"测试结束\") if __name__ == '__main__': multiprocessing_joinablequeue_test() 结果输出 start_work 开始 生产者开始工作 task_name:14608-0 data:[0, 0] 生产者休息ing task_name:14608-1 data:[2, 2] ... task_name:14608-7 data:[14, 14] 生产者休息ing task_name:14608-31 data:[62, 62] ... task_name:14608-60 data:[120, 120] 生产者休息ing task_name:14608-61 data:[122, 122 ... task_name:14608-90 data:[180, 180] 生产者休息ing 生产者工作结束 task_name:14608-91 data:[182, 182] ... task_name:14608-93 data:[186, 186] start_work 结束 测试结束 管道Pipe Pipe的本质是进程之间的用管道数据传递，而不是数据共享，这和socket有点像 pipe()返回两个连接对象分别表示管道的两端，每端都有send()和recv()函数 如果两个进程试图在同一时间的同一端进行读取和写入那么，这可能会损坏管道中的数据 管道是数据不安全的，多个进程同时收发数据可道引起数据异常，这时候就应该配合锁使用 from multiprocessing import Process, Pipe def fun1(conn): print('子进程发送消息：') conn.send('你好主进程') print('子进程接受消息：') print(conn.recv()) conn.close() if __name__ == '__main__': conn1, conn2 = Pipe() #关键点，pipe实例化生成一个双向管 p = Process(target=fun1, args=(conn2,)) #conn2传给子进程 p.start() print('主进程接受消息：') print(conn1.recv()) print('主进程发送消息：') conn1.send(\"你好子进程\") p.join() print('结束测试') 主进程接受消息： 子进程发送消息： 子进程接受消息： 你好主进程 主进程发送消息： 你好子进程 结束测试 Process finished with exit code 0 上面可以看到主进程和子进程可以相互发送消息 Managers Queue和Pipe只是实现了数据交互，并没实现数据共享，即一个进程去更改另一个进程的数据，那么就要用到Managers from multiprocessing import Process, Manager def fun1(dic,lis,index): dic[index] = 'a' dic['2'] = 'b' lis.append(index) #[0,1,2,3,4,0,1,2,3,4,5,6,7,8,9] #print(l) if __name__ == '__main__': with Manager() as manager: dic = manager.dict()#注意字典的声明方式，不能直接通过{}来定义 l = manager.list(range(5))#[0,1,2,3,4] process_list = [] for i in range(10): p = Process(target=fun1, args=(dic,l,i)) p.start() process_list.append(p) for res in process_list: res.join() print(dic) print(l) {0: 'a', '2': 'b', 3: 'a', 1: 'a', 2: 'a', 4: 'a', 5: 'a', 7: 'a', 6: 'a', 8: 'a', 9: 'a'} [0, 1, 2, 3, 4, 0, 3, 1, 2, 4, 5, 7, 6, 8, 9] 可以看到主进程定义了一个字典和一个列表，在子进程中，可以添加和修改字典的内容 在列表中插入新的数据，实现进程间的数据共享，即可以共同修改同一份数据 注意事项 无法调用多层生成器(待验证) #!/usr/bin/env Python # -- coding: utf-8 -- \"\"\" @version: v0.1 @author: narutohyc @file: text.py @Description: @time: 2020/5/29 19:51 \"\"\" from multiprocessing import Process, Queue, JoinableQueue, Manager from multiprocessing import cpu_count import os, time from abc import (ABC, abstractmethod, ABCMeta) from comm.logger.logger_config import logger class SampleIterator(ABC, metaclass=ABCMeta): def __init__(self): pass @abstractmethod def __iter__(self): ''' 样本处理并返回 ''' pass class DataSource1(SampleIterator): def __init__(self): super(DataSource1, self).__init__() def __iter__(self): for ii in range(10): yield ii class DataSource2(SampleIterator): def __init__(self, data_source): super(DataSource2, self).__init__() self.data_source = data_source def __iter__(self): for ii in self.data_source: yield ii class DataSource3(SampleIterator): def __init__(self, data_source): super(DataSource3, self).__init__() self.data_source = data_source def __iter__(self): for ii in self.data_source: yield ii class DataSource4(SampleIterator): def __init__(self, data_source): super(DataSource4, self).__init__() self.data_source = data_source def __iter__(self): for ii in self.data_source: yield ii class HUCY(): def __init__(self, data_source=None): self.num_of_worker = cpu_count() self.size_of_queue = 2 self.data_source = data_source def start_work(self): # 进程队列 process_list = [] # 新建一个大小为10的队列 work_queue = Queue(self.size_of_queue) # 一个生产者 produce_num = 1 for _ in range(produce_num): sent = Process(target=self.productor, args=(work_queue,)) sent.start() process_list.append(sent) # 多个消费者 for _ in range(self.num_of_worker - produce_num): process = Process(target=self.consumer, args=(work_queue,)) process.start() process_list.append(process) # 这里需要加入结束标识，还有就是JoinableQueue的方式 [process.join() for process in process_list[:produce_num]] for _ in range(self.num_of_worker - produce_num): work_queue.put(None) [process.join() for process in process_list[produce_num:]] print(\"start_work 结束\") def productor(self, work_queue): [work_queue.put(ii) for ii in self.data_source] logger.info(\"生产者结束\") def consumer(self, work_queue): while True: data = work_queue.get() if data is None: break logger.info(f\"数据: {data}\") logger.info(\"消费者结束\") def hyc_test(): da1=DataSource2(DataSource3(DataSource4(DataSource1()))) da2 = DataSource2(DataSource3(DataSource4(da1))) da3 = DataSource2(DataSource3(DataSource4(da2))) da4 = DataSource2(DataSource3(DataSource4(da3))) da5 = DataSource2(DataSource3(DataSource4(da4))) da6 = DataSource2(DataSource3(DataSource4(DataSource2(da5)))) hucy = HUCY(da6) hucy.start_work() if __name__ == '__main__': hyc_test() 几个问题: 以上代码 若有多个生产者 就会各自拥有自己的数据生成器，导致数据重复 有些定义的方法 好像会使程序卡住 Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-12-01 00:57:11 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/23.python常用库学习.html":{"url":"chapters/23.python常用库学习.html","title":"python常用库学习","keywords":"","body":"图像相关PILcv2时间相关库time库的使用datetime 模块calendar库的使用random库的使用命令行脚本传参sys.argvargparse速览ArgumentParseradd_argument实际例子tf.app.run位运算路径pathlib和os常用操作对比配置文件解读initomlyaml系统变量流 图像相关 10个Python图像处理工具，非常全了！ 图像的RGB 色彩模式 RGB 三个颜色通道的变化和叠加得到各种颜色，其中 R 红色，取值范围，0-255 G 绿色，取值范围，0-255 B 蓝色，取值范围，0-255 一幅图像上的所有像素点的信息就完全可以采用矩阵来表示，通过矩阵的运算实现更加复杂的操作 网络图片读取 def get_url_img_io(url: str) -> BytesIO: \"\"\" 获取网络图片的io流 \"\"\" return BytesIO(requests.get(url).content) def get_url_img(url: str): \"\"\" 获取网络图片,并转为np.array \"\"\" return np.asarray(bytearray(get_url_img_io(url).read()), dtype=\"uint8\") plt.imshow()函数负责对图像进行处理，并显示其格式 plt.show()则是将plt.imshow()处理后的函数显示出来 PIL Python图像库（PIL(Python Image Library)）是一个第三方Python包，为Python解释器添加了图像处理功能，允许处理照片并执行许多常见的图像文件操作，官方教程 打开图像：返回一个PIL.JpegImagePlugin.JpegImageFile对象 # 导入 Image 模块 import requests from PIL import Image # 图像路径 img_path = r'frame.jpg' img_url = r'https://pic.hycbook.com/i/hexo/post_cover/蕾姆10.webp' # 打开本地图片 RGB im = Image.open(img_path) # JpegImageFile image mode=RGB size=1408x1152 # 打开网络图片 img = Image.open(get_url_img_io(img_url)) # [H,W,C] JpegImageFile对象常用方法 import matplotlib.pyplot as plt # Returns a histogram for the image histogram = image.histogram() # 可视化颜色分布直方图 plt.hist(histogram, bins=len(histogram)) plt.xlabel('Histogram') plt.show() # 裁剪图像 用于裁剪的开始和结束x/y坐标 # 用于裁剪的坐标将随照片而变化。事实上，可能应该更改此代码，使其接受裁剪坐标作为参数。可以自己反复尝试，找出要使用的裁剪边界框。可以使用像Gimp这样的工具来帮助你，用Gimp绘制一个边界框，并记下它给你的坐标，方便尝试使用Pillow cropped = image.crop((40, 590, 979, 1500)) # 重新调整图像大小 img = im.resize((240,240)) # 图像旋转 img.rotate(90).show() 使用过滤器：Pillow包含有几个过滤器，可以将其应用于图像。以下是当前支持的筛选器 BLUR CONTOUR DETAIL EDGE_ENHANCE EDGE_ENHANCE_MORE EMBOSS FIND_EDGES SHARPEN SMOOTH SMOOTH_MORE from PIL import ImageFilter blurred_image = image.filter(ImageFilter.BLUR) 显示图片 # 打开图片 im = Image.open(img_path) # 显示图片 im.show() 保存图像 infile = \"in.jpg\" outfile = \"output.jpg\" with Image.open(infile) as im: im.save(outfile) 类型转换 # JpegImageFile -> np.array [H,W,C] np.asanyarray(im) # np.array -> JpegImageFile img = Image.fromarray(arr) cv2 在计算机视觉项目的开发中，OpenCV作为较大众的开源库，拥有了丰富的常用图像处理函数库，采用C/C++语言编写，可以运行在Linux/Windows/Mac等操作系统上，能够快速的实现一些图像处理和识别的任务。此外，OpenCV还提供了Java、python、cuda等的使用接口、机器学习的基础算法调用，从而使得图像处理和图像分析变得更加易于上手，让开发人员更多的精力花在算法的设计上 打开图像：cv2.imread(filepath, flags)，返回一个np.array对象 flags：读入图片的标志 cv2.IMREAD_COLOR：默认参数，读入一副彩色图片，忽略alpha通道 cv2.IMREAD_GRAYSCALE：读入灰度图片 cv2.IMREAD_UNCHANGED：顾名思义，读入完整图片，包括alpha通道 # 图像路径 img_path = r'frame.jpg' img_url = r'https://pic.hycbook.com/i/hexo/post_cover/蕾姆10.webp' # 打开网络图片 img = get_url_img(img_url) # 打开本地图片 image = cv2.imread(img_path) # [H,W,C] np.array 打开视频 常见操作 resize: cv2.resize(InputArray src, OutputArray dst, Size, fx, fy, interpolation) fx, fy 沿x轴，y轴的缩放系数 interpolation 插入方式 INTER_NEAREST 最近邻插值 INTER_LINEAR 双线性插值（默认设置） INTER_AREA 使用像素区域关系进行重采样。 INTER_CUBIC 4x4像素邻域的双三次插值 INTER_LANCZOS4 8x8像素邻域的Lanczos插值 img_test = cv2.resize(img, (0, 0), fx=0.25, fy=0.25, interpolation=cv2.INTER_NEAREST) img_test = cv2.resize(img_test, (0, 0), fx=4, fy=4, interpolation=cv2.INTER_NEAREST) RGB转灰度： img2 = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) 灰度转RBG： img3 = cv2.cv2Color(img, cv2.COLOR_GRAY2RGB) 显示图片 cv2.imshow(\"local pic\", image) cv2.waitKey(0) 保存图像 cv2.imwrite(outfile, image) 时间相关库 协调世界时(Coordinated Universal Time, UTC)是一种标准的时间表述方式，它与时区无关 有些计算机，用某时刻与UNIX时间原点之间相差的秒数，来表示那个时刻所对应的时间 对于这些计算机来说，UTC是一种非常好的计时方式 Python提供了两种时间转换方式 旧的方式，是使用内置的time模块，这是一种极易出错的方式 新的方式，则是采用内置的datetime模块，该模块的效果非常好 小结： time模块需要依赖操作系统而运作。该模块的实际行为，取决于底层的C函数如何与宿主操作系统相交互 这种工作方式，使得time模块的功能不够稳定。它无法协调地处理各种本地时区，因此，我们应该尽量少用这个模块 如果一定要使用time模块，那就只应该用它在UTC与宿主计算机的当地时区之间进行转换 对于其他类型的转换来说，还是使用datetime模块比较好 常见的时间模块 time: Python内置时间库，通过时间戳或元组表示时间，功能简约但实用 datetime: 内置日期库，处理日期时间对象和属性 dateutil: 基于datetime库的实用拓展，增强了对时间间隔和时间序列的处理 time库的使用 time— Time access and conversions time库是Python中处理时间的标准库 计算机时间的表达 提供获取系统时间并格式化输出功能 提供系统级精确计时功能，用于程序性能分析 时间获取 # 获取当前时间戳，即计算机内部时间值，浮点数 >>>time.time() 1568506809.169575 # 获取当前时间并以易读方式表示，返回字符串, 将 gmtime()或localtime() 返回的 表示时间的 元组或struct_time 转换成一个字符串，格式：'Sun Jun 20 23:21:051993'。 >>>time.ctime() # 等价于 time.asctime(time.localtime(time.time())) 'Sun Sep 15 08:20:16 2019' # 获取当前时间，表示为计算机可处理的时间格式，返回的时间是两个时区对相同时刻的时间表示 # 返回的是struct_time格式，是tuple的子类 >>>time.gmtime() # 0时区:time.gmtime() 本地时区:time.localtime(seconds=None) time.struct_time(tm_year=2019, tm_mon=9, tm_mday=15, tm_hour=0, tm_min=21, tm_sec=40, tm_wday=6, tm_yday=258, tm_isdst=0) # time.mktime(t)是localtime()的反函数 >>>time.mktime(time.localtime()) 1568508316.0 struct_time元组的属性如下 序号 属性 值 0 tm_year 2008 1 tm_mon 1 到 12 2 tm_mday 1 到 31 3 tm_hour 0 到 23 4 tm_min 0 到 59 5 tm_sec 0 到 61 (60或61 是闰秒) 6 tm_wday 0到6 (0是周一) 7 tm_yday 1 到 366(儒略历) 8 tm_isdst -1, 0, 1, -1是决定是否为夏令时的旗帜 时间格式化 对象转字符串: time.strftime(format, obj) 字符串转结构化: time.strptime(time_str, format) # strftime(tpl, ts) # tpl是格式化模板字符串，用来定义输出效果 # ts是计算机内部时间类型变量 >>>t = time.gmtime() >>>time.strftime(\"%Y-%m-%d %H:%M:%S\", t) '2019-09-15 00:24:23' # strptime(str, tpl) # str是字符串形式的时间值 # tpl是格式化模板字符串，用来定义输入效果 >>>timeStr = '2018-01-26 12:55:20' >>>time.strptime(timeStr, \"%Y-%m-%d %H:%M:%S\") time.struct_time(tm_year=2019, tm_mon=9, tm_mday=15, tm_hour=0, tm_min=25, tm_sec=50, tm_wday=6, tm_yday=258, tm_isdst=-1) # 结构化对象转时间戳 >>>time.mktime(time.localtime()) 1668562455.0 # 时间戳转结构化对象 >>>time.localtime(time.time()) time.struct_time(tm_year=2022, tm_mon=11, tm_mday=16, tm_hour=9, tm_min=37, tm_sec=9, tm_wday=2, tm_yday=320, tm_isdst=0) 常用格式(年月日、时分秒) | 格式化字符串 | 日期/时间说明 | 值范围和实例 | | ------------ | -------------- | ------------------------------ | | %Y | 年份 | 0000~9999，例如：1900 | | %y | 去掉世纪的年份 | 00~99 | | %m | 月份 | 01~12，例如：10 | | %B | 月份名称 | January~December，例如：April | | %b | 月份名称缩写 | Jan~Dec，例如：Apr | | %H | 小时（24h制） | 00~23，例如：12 | | %h | 小时（12h制） | 01~12，例如：7 | | %M | 分钟 | 00~59，例如：26 | | %S | 秒 | 00~59，例如：26 | 其他格式 | 格式化字符串 | 日期/时间说明 | 值范围和实例 | | ------------ | -------------------- | ------------------------------ | | %w | 星期中的第几天 | 0~6，0对应星期天 | | %d | 日期，一月中的第几天 | 01~31，例如：25 | | %j | 一年中的第几天 | 001~366 | | %A | 星期 | Monday~Sunday，例如：Wednesday | | %a | 星期缩写 | Mon~Sun，例如：Wed | | %p | 上/下午 | AM, PM，例如：PM | | %Z | 时区的名字 | | 将时间以合理的方式展示出来 格式化：类似字符串格式化，需要有展示模板 展示模板由特定的格式化控制符组成 strftime()方法 程序计时: 测量起止动作所经历时间的过程 测量时间：perf_counter() 产生时间：sleep() # 返回一个CPU级别的精确时间计数值，单位为秒 # 由于这个计数值起点不确定，连续调用差值才有意义 >>>start = time.perf_counter() 318.66599499718114 >>>end = time.perf_counter() 341.3905185375658 >>>end - start 22.724523540384666 # s拟休眠的时间，单位是秒，可以是浮点数 time.sleep(5) datetime 模块 python 处理日期和时间的标准库，常用 datetime 类，及 date 类 和 time 类，可做计算之类的操作 time模块比较底层，能完成的功能相对有限，这个时候就需要更高级的datetime模块来参与了 可以简单理解为，datetime模块是对time模块进行了更高一层的封装 基本定义和属性方法 import datetime import sys from typing import Tuple from dateutil import relativedelta from dateutil.parser import parse def fn_basic() -> Tuple: \"\"\" 基本定义和属性方法 \"\"\" # 获取当前日期时间 d_now = datetime.datetime.now() # 等价于datetime.datetime.today() d_utc_now = datetime.datetime.utcnow() # 获取utc对象, 北京时间+8h=d_utc_now # 至少要给定年月日 d_free = datetime.datetime(year=2022, month=11, day=11) print(\"d_now \", d_now) print(f\"d_now的详细信息: year:{d_now.year} month:{d_now.month} day:{d_now.day}\") print(f\"d_now的详细信息: hour:{d_now.hour} minute:{d_now.minute} second:{d_now.second} microsecond:{d_now.microsecond}\") print(\"d_free\", d_free) # 常用属性 d = d_free.date() # 获取日期对象 t = d_free.time() # 获取时间对象 dt = datetime.datetime.combine(date=d, time=t) # 根据date和time, 创建一个datetime对象 ts = d_free.timestamp() # 获取对象的时间戳 print(\"ts \", ts) print(sys._getframe().f_code.co_name, \"=\" * 60, '\\n') return d_now, d_free # 输出 d_now 2022-11-15 16:59:45.447093 d_now的详细信息: year:2022 month:11 day:15 d_now的详细信息: hour:16 minute:59 second:45 microsecond:447093 d_free 2022-11-11 00:00:00 ts 1668096000.0 fn_basic ============================================================ 日期时间差异 def fn_dif(d_now, d_free): \"\"\" 日期时间差异 \"\"\" # type(dif) == timedelta dif = d_now - d_free print(\"d_free+dif==d_now is\", d_free + dif == d_now) # 下个月 前一天 往后9小时 再往前10分钟 d_dif = d_free + relativedelta.relativedelta(months=1, days=-1, hours=9, minutes=-10) print(\"d_dif \", d_dif) # 指定时间，只能是整数 d_dif_abs = d_free + relativedelta.relativedelta(month=1, day=5, hour=9, minute=10) print(\"d_dif_abs \", d_dif_abs) print(sys._getframe().f_code.co_name, \"=\" * 60, '\\n') # 输出 d_free+dif==d_now is True d_dif 2022-12-10 08:50:00 d_dif_abs 2022-01-05 09:10:00 fn_dif ============================================================ 日期时间的格式化 def fn_format_transform(): \"\"\" 日期时间的格式化 \"\"\" date_format_str = \"%Y-%m-%d %H:%M:%S\" date_format_str_2 = \"{:%Y-%m-%d %H:%M:%S}\" # 获取当前日期时间 d_now = datetime.datetime.now() # 对象转字符串 date_string = d_now.strftime(date_format_str) print(\"date_string:\", date_string) date_string_2 = date_format_str_2.format(d_now) print(\"date_string_2:\", date_string_2) # 字符串转对象 d_from_str = datetime.datetime.strptime(date_string, date_format_str) print(\"d_from_str:\", d_from_str) # dateutil.parse 可以解析几乎所有人类能够理解的日期表示形式 d_parse = parse(date_string) print(\"parse:\", d_parse) print(\"parse('Jan 31, 2021 10:45 PM'): \", parse('Jan 31, 2022 10:45 PM')) print(\"parse(str(d_now)): \", parse(str(d_now))) print(sys._getframe().f_code.co_name, \"=\" * 60, '\\n') # 输出 date_string: 2022-11-15 16:59:45 date_string_2: 2022-11-15 16:59:45 d_from_str: 2022-11-15 16:59:45 parse: 2022-11-15 16:59:45 parse('Jan 31, 2021 10:45 PM'): 2022-01-31 22:45:00 parse(str(d_now)): 2022-11-15 16:59:45.447093 fn_format ============================================================ 产生指定范围内的日期时间 def gen_date_times(): \"\"\" 产生指定范围内的日期时间 \"\"\" def date_time(p_start_time, p_end_time, p_format: str = '%Y-%m-%d'): while p_start_time calendar库的使用 提供与日历相关功能，如：为给定的月份或年份，打印文本日历 import calendar c = calendar.month(2019, 9) print(c) September 2019 Mo Tu We Th Fr Sa Su 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 random库的使用 random库是使用随机数的Python标准库 伪随机数: 采用梅森旋转算法生成的(伪)随机序列中元素 random库主要用于生成随机数 使用random库: import random 主要方法 基本随机数函数： seed(), random() 扩展随机数函数： randint(), getrandbits(), uniform(), randrange(), choice(), shuffle() 基本随机数函数 # 初始化给定的随机数种子，默认为当前系统时间 >>>random.seed(10) #产生种子10对应的序列 # 生成一个[0.0, 1.0)之间的随机小数 >>>random.random() 0.5714025946899135 # 返回捕获当前生成器内部状态的对象.该对象可以用于函数setstate()取保存当前的状态. >>> state = random.getstate() >>> random.setstate(state) # 以相同顺序打乱多个数组 # 等价于 np.random.get_state() np.random.shuffle(a) np.random.set_state(state) a = np.arange(10) b=['A','B','C','D','E','F','G','H','I','J'] state=random.getstate() random.shuffle(a) print(a) random.setstate(state) random.shuffle(b) print(b) [9 4 5 0 1 2 6 8 7 3] ['J', 'E', 'F', 'A', 'B', 'C', 'G', 'I', 'H', 'D'] 扩展随机数函数 # 生成一个[a, b]之间的整数 >>>random.randint(10, 100) 64 # 生成一个[m, n)之间以k为步长的随机整数 >>>random.randrange(10, 100, 10) 80 # 生成一个k比特长的随机整数 >>>random.getrandbits(16) 37885 # 生成一个[a, b]之间的随机小数 >>>random.uniform(10, 100) 13.096321648808136 # 从序列seq中随机选择一个元素 >>>random.choice([1,2,3,4,5,6,7,8,9]) 8 # 将序列seq中元素随机排列，返回打乱后的序列 >>> s=[1,2,3,4,5,6,7,8,9]; >>> random.shuffle(s); >>> print(s) [3, 5, 8, 9, 6, 1, 2, 7, 4] 函数 描述 randint(a, b) 生成一个[a, b]之间的整数 randrange(m, n[, k]) 生成一个[m, n)之间以k为步长的随机整数 getrandbits(k) 生成一个k比特长的随机整数 uniform(a, b) 生成一个[a, b]之间的随机小数 choice(seq) 从序列seq中随机选择一个元素 shuffle(seq) 将序列seq中元素随机排列，返回打乱后的序列 命令行脚本传参 命令行运行Python脚本时传入参数的三种方式 如果在运行python脚本时需要传入一些参数，例如gpus与batch_size，可以使用如下三种方式。 python script.py 0,1,2 10 python script.py --gpus=0,1,2 --batch-size=10 python script.py --gpus=0,1,2 --batch_size=10 这三种格式对应不同的参数解析方式，分别为sys.argv, argparse, tf.app.run 前两者是python自带的功能，后者是tensorflow提供的便捷方式。 sys.argv sys模块是很常用的模块， 它封装了与python解释器相关的数据，例如sys.modules里面有已经加载了的所有模块信息，sys.path里面是PYTHONPATH的内容，而sys.argv则封装了传入的参数数据 使用sys.argv接收上面第一个命令中包含的参数方式如下： import sys gpus = sys.argv[1] # gpus = [int(gpus.split(','))] batch_size = sys.argv[2] print gpus print batch_size argparse 官方文档 使用argparse从命令行接收bool类型的参数 Python argparse库用法总结 速览 import argparse # 创建一个解析对象 parser = argparse.ArgumentParser(prog='我的解析对象') # 添加你要关注的命令行参数和选项 parser.add_argument('--doc_path', '-dp') parser.add_argument('-tip', '--tp') # 解析参数 opt = parser.parse_args() # 带--的参数可以直接用名字访问 print(opt.tp, opt.doc_path) 可以用python arg.py -h获取帮助 ArgumentParser class argparse.ArgumentParser(prog=None, usage=None, description=None, epilog=None, parents=[], formatter_class=argparse.HelpFormatter, prefix_chars='-', fromfile_prefix_chars=None, argument_default=None, conflict_handler='error', add_help=True, allow_abbrev=True, exit_on_error=True) 创建一个新的ArgumentParser对象，所有的参数都应当作为关键字参数传入，每个参数在下面都有它更详细的描述 prog - 程序的名称 (默认值: os.path.basename(sys.argv[0])) usage - 描述程序用途的字符串（默认值：从添加到解析器的参数生成） description - 在参数帮助文档之后显示的文本 (默认值:无) epilog - Text to display after the argument help (by default, no text) parents - 一个 ArgumentParser 对象的列表，它们的参数也应包含在内 formatter_class - 用于自定义帮助文档输出格式的类 argparse.RawDescriptionHelpFormatter：表示 description 和 epilog 已经被正确的格式化 argparse.RawTextHelpFormatter：保留所有种类文字的空格，包括参数的描述 argparse.ArgumentDefaultsHelpFormatter：自动添加默认的值的信息到每一个帮助信息的参数中: argparse.MetavarTypeHelpFormatter：每一个参数中使用 type 的参数名当作它的显示名 prefix_chars - 可选参数的前缀字符集合（默认值： '-'），许多命令行会使用 - 当作前缀，比如 -f/--foo。如果解析器需要支持不同的或者额外的字符，比如像 +f 或者 /foo 的选项，可以在参数解析构建器中使用 prefix_chars= 参数 fromfile_prefix_chars - 当需要从文件中读取其他参数时，用于标识文件名的前缀字符集合（默认值： None） argument_default - 参数的全局默认值（默认值： None） conflict_handler - 解决冲突选项的策略（通常是不必要的） add_help - 为解析器添加一个 -h/--help 选项（默认值： True），有时候可能会需要关闭额外的帮助信息 allow_abbrev - 如果缩写是无歧义的，则允许缩写长选项 （默认值：True） exit_on_error - 正常情况下，当你向 ArgumentParser 的 parse_args() 方法传入一个无效的参数列表时，它将会退出并发出错误信息。如果用户想要手动捕获错误，可通过将 exit_on_error 设为 False 来启用该特性 import argparse # 实例化 parser = argparse.ArgumentParser(prog=\"hyc_prog\", usage='%(prog)s [options]', description='[%(prog)s] 参数解析描述，用于演示', epilog=\"description 参数后显示额外的对程序的描述\", # 表示 description 和 epilog 已经被正确的格式化了 formatter_class=argparse.RawDescriptionHelpFormatter, prefix_chars='-+', # 解析器需要支持不同的或者额外的字符 argument_default=argparse.SUPPRESS, # 全局禁止在 parse_args() 中创建属性 ) # fromfile_prefix_chars参数 with open('args.txt', 'w', encoding=sys.getfilesystemencoding()) as fp: fp.write('-f\\nbar') parser = argparse.ArgumentParser(fromfile_prefix_chars='@') parser.add_argument('-f') parser.parse_args(['-f', 'foo', '@args.txt']) # conflict_handler参数 parser = argparse.ArgumentParser(prog='PROG', conflict_handler='resolve') parser.add_argument('-f', '--foo', help='old foo help') parser.add_argument('--foo', help='new foo help') parser.print_help() >>> usage: PROG [-h] [-f FOO] [--foo FOO] options: -h, --help show this help message and exit -f FOO old foo help --foo FOO new foo help # exit_on_error参数 parser = argparse.ArgumentParser(exit_on_error=False) parser.add_argument('--integers', type=int) try: parser.parse_args('--integers a'.split()) except argparse.ArgumentError: print('Catching an argumentError') add_argument ArgumentParser.add_argument(name or flags...[, action][, nargs][, const][, default][, type][, choices][, required][, help][, metavar][, dest]) 定义单个的命令行参数应当如何解析，每个形参都在下面有它自己更多的描述，长话短说有： name or flags - 一个命名或者一个选项字符串的列表，例如 foo 或 -f, --foo action - 当参数在命令行中出现时使用的动作基本类型 nargs - 命令行参数应当消耗的数目 const - 被一些 action 和 nargs 选择所需求的常数 default - 当参数未在命令行中出现并且也不存在于命名空间对象时所产生的值 type - 命令行参数应当被转换成的类型 choices - A sequence of the allowable values for the argument. required - 此命令行选项是否可省略(仅选项可用)，只能用于可选参数(optional arguments) help - 一个此选项作用的简单描述 metavar - 在使用方法消息中使用的参数值示例 dest - 被添加到parse_args()所返回对象上的属性名 name or flags name就是指命令行参数中没有'-'的参数名字例如'myname'，而flags就是指前面有'-'的参数名，例如'-a'、'--age' 其中name对应位置参数，而flags对应可选参数，name在命令行必须输入，并按照顺序喂给程序 命令行传入参数时，对于位置参数我们直接给出其值，对于可选参数需要给出其flags。argparse会先将可选参数进行解析，对于剩余未解析的参数，传给位置参数 import argparse parser = argparse.ArgumentParser(description='a test') parser.add_argument('--age','-a') parser.add_argument('myname') parser.add_argument('sex') # 注意前两个参数必须是 myname和sex args = parser.parse_args('tom boy --age 22'.split()) print(args) # Namespace(age='22', sex='boy', myname='tom') 命令行传入参数时，先写位置参数，再写可选参数 action: 指定了参数是如何被处理的，支持额操作如下 store：这个只是简单的存储这个参数值，这也是默认操作 store_const: 存储const指定的值，配合const参数 store_false和store_true: 分别对应存储False和True值，它们是store_const的特例 append：保存为列表格式，将每个参数的值添加到这个列表 import argparse # action='store_const' 传参时只要传入参数名,会默认附上const, 如果不传该参数名, 默认为None parser=argparse.ArgumentParser() parser.add_argument('--foo',action='store_const',const=42) parser.parse_args('--foo'.split()) Out[0]: Namespace(foo=42) parser.parse_args(''.split()) Out[1]: Namespace(foo=None) # store_false和store_true # 以store_false为例, 当传入参数名时, 值为False, 不传参数名时值为True parser.add_argument('--bar', action='store_false') parser.parse_args(''.split()) Out[2]: Namespace(bar=True, foo=None) parser.parse_args('--bar'.split()) Out[3]: Namespace(bar=False, foo=None) # action='append' parser = argparse.ArgumentParser() parser.add_argument('--foo', action='append') parser.parse_args('--foo 1 --foo 2'.split()) Out[5]: Namespace(foo=['1', '2']) nargs: nargs的意思就是输入参数的个数 N—某正整数: 指定好后，不能输入多了，也不能输入少了 '?': 这时代表parser会读取0个或1个参数，具体遵循下面的原则： 如果给出了1个参数，照常读取这个参数保存起来 如果只给出了flags，比如‘--age’后面未给出具体值，则保存const参数的值（如果const未给出则为None） 如果什么都没给，则保存default参数的值（如果default未给出则为None） '*': 不确定具体个数，那么可以用nargs=* '+': 要求参数的个数必须大于 如果不给flags，parser会用default的值；如果只给flags，不给值，此时会报错 而当nargs='*'时，就算只给flags不给值，也不会报错，会得到一个空列表参数 import argparse # nargs=? parser = argparse.ArgumentParser(description='a test') parser.add_argument('name') parser.add_argument('--age','-a', nargs='?',const=16,default=18) args = parser.parse_args('tom -a 15'.split()) print(args) # Namespace(age='15', name='tom') args = parser.parse_args('tom -a'.split()) print(args) # Namespace(age=16, name='tom') args = parser.parse_args('tom'.split()) print(args) # Namespace(age=18, name='tom') const: 多是配合其它参数出演的配角 action参数为 'store_const' 时或是 'append_const' 时 nargs参数为 '?' 时 default 当命令行完全没有提到某个参数时，default参数就会发挥作用，default的默认值为None type 这个类型参数可以约束输入参数的类型，当类型转换合法时，会自动帮我们进行类型转换 import argparse def str2bool(v): if v.lower() in ('yes', 'true', 't', 'y', '1'): return True elif v.lower() in ('no', 'false', 'f', 'n', '0'): return False else: raise argparse.ArgumentTypeError('Unsupported value encountered.') # type=int parser = argparse.ArgumentParser(description='a test') parser.add_argument('name') parser.add_argument('--age','-a',type=int, default='17') args = parser.parse_args('tom'.split()) print(args) # Namespace(age=17, name='tom') # 自定义类型 parser.add_argument('--is_del_aft', type=str2bool, default=False) choices 这个选项参数可以使用列表约束输入参数的取值范围。如果输入参数不在候选参数列表中，程序会报错 import argparse parser = argparse.ArgumentParser(description='a test') parser.add_argument('name',choices=['tom','Jim','Bob']) parser.add_argument('--age','-a',type=int, default='17') args = parser.parse_args('Jim'.split()) print(args) # Namespace(age=17, name='Jim') args = parser.parse_args('Toy'.split()) print(args) # test.py: error: argument name: invalid choice: 'Toy' (choose from 'tom', 'Jim', 'Bob') 在参数获取阶段，约束好用户输入的参数范围，可以防止意想不到的参数带来的未知后果 metavar: 这个参数的功能也是个性化显示帮助信息 import argparse parser = argparse.ArgumentParser(description='a test') parser.add_argument('name',metavar='haha') # 加metavar parser.add_argument('--age','-a',type=int, default='17',metavar='hahahaha') parser.parse_args('-h'.split()) >>> usage: pydevconsole.py [-h] [--age hahahaha] haha a test positional arguments: haha optional arguments: -h, --help show this help message and exit --age hahahaha, -a hahahaha # 不加metavar parser.add_argument('--age','-a',type=int, default='17') >>> usage: pydevconsole.py [-h] [--age AGE] haha a test positional arguments: haha optional arguments: -h, --help show this help message and exit --age AGE, -a AGE dest 每个参数待parser处理完毕后，都会以‘属性名-属性’的形式保存起来 对于位置参数，属性名就是位置参数的name 对于可选参数，属性名是可选参数去掉‘--’后的那部分，如果没有‘--’，则为去掉‘-’的那部分 为了使得属性名合法，parser还会将单词中间的短横杠变为下划线 但如果，你不想用上述方法自动生成的属性名，你想自己指定属性名，就可以设定dest参数来指定。（只能指定可选参数的属性名） import argparse # 创建 ArgumentParser 对象 parser = argparse.ArgumentParser(description='An example of using argparse') # 添加位置参数和可选参数 parser.add_argument('--input', dest='input_file', help='Input file path') parser.add_argument('--output', dest='output_file', help='Output file path') # 解析命令行参数 args = parser.parse_args('--input 输入 --output 输出'.split()) # 访问解析后的参数值 input_path = args.input_file output_path = args.output_file # 执行相应操作 print(f'Input file path: {input_path}') print(f'Output file path: {output_path}') >>> Input file path: 输入 Output file path: 输出 可以看到，如果不指定dest，年龄的属性名是age。若此时指定了dest='myage'，那么年龄的属性名就人为设定成了myage 实际例子 脚本运行命令python script.py -gpus=0,1,2 --batch-size=10中的--batch-size会被自动解析成batch_size parser.add_argument 方法的type参数理论上可以是任何合法的类型，但有些参数传入格式比较麻烦 例如list，所以一般使用bool, int, str, float这些基本类型就行了，更复杂的需求可以通过str传入，然后手动解析 bool类型的解析比较特殊，传入任何值都会被解析成True，传入空值时才为`False import argparse def str2bool(v): if v.lower() in ('yes', 'true', 't', 'y', '1'): return True elif v.lower() in ('no', 'false', 'f', 'n', '0'): return False else: raise argparse.ArgumentTypeError('Unsupported value encountered.') if __name__ == '__main__': parser = argparse.ArgumentParser(description='manual to this script') parser.add_argument('--zook_host', type=str, default='127.0.0.1:2181') parser.add_argument('--num_of_task', type=int, default=10) parser.add_argument('--is_del_aft', type=str2bool, default=False) args = parser.parse_args() many_tasks_schedule_performance(zook_host=args.zook_host, num_of_task=args.num_of_task, is_del_aft=args.is_del_aft) tf.app.run tensorflow也提供了一种方便的解析方式 import tensorflow as tf tf.app.flags.DEFINE_string('gpus', None, 'gpus to use') tf.app.flags.DEFINE_integer('batch_size', 5, 'batch size') FLAGS = tf.app.flags.FLAGS def main(_): print FLAGS.gpus print FLAGS.batch_size if __name__ == \"__main__\": tf.app.run() >>> python script.py --gpus=0,1,2 --batch_size=10 有几点需要注意 tensorflow只提供以下四种方法： tf.app.flags.DEFINE_string, tf.app.flags.DEFINE_integer, tf.app.flags.DEFINE_boolean, tf.app.flags.DEFINE_float 分别对应str, int,bool,float类型的参数 这里对bool的解析比较严格，传入1会被解析成True，其余任何值都会被解析成False 脚本中需要定义一个接收一个参数的main方法：def main(_):，这个传入的参数是脚本名，一般用不到， 所以用下划线接收。 以batch_size参数为例，传入这个参数时使用的名称为--batch_size，也就是说，中划线不会像在argparse 中一样被解析成下划线。 tf.app.run()会寻找并执行入口脚本的main方法。也只有在执行了tf.app.run()之后才能从FLAGS中取出参数。 从它的签名来看，它也是可以自己指定需要执行的方法的，不一定非得叫main： run(main=None, argv=None) tf.app.flags只是对argpars的简单封装。 代码见https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/python/platform/flags.py 位运算 背景知识 &#x1F345;二进制 在Python中可以通过以\"0b\"或者\"-0b\"开头的字符串来表示二进制 # 转二进制 print(bin(5)) >>> 0b101 &#x1F96C;原码、反码、补码 原码: 将一个整数转换成二进制形式，就是其原码。例如6的原码就是0110；-18 的原码就是1000 0000 0001 0010 通俗的理解，原码就是一个整数本来的二进制形式 反码: 对于正数，它的反码就是其原码(原码和反码相同)；负数的反码是将原码中除符号位以外的所有位(数值位)取反 补码: 对于正数，它的补码就是其原码(原码、反码、补码都相同)；负数的补码是其反码加1 位运算 与: & 按位与 或: | 按位或 反：~ 按位取反 异或: ^ 按位异或 按位左移: 左移1位相当于 乘以2 按位右移: >> 按位右移，所有二进制位向右移动n位，移出的位删掉，进的位补符号位，右移不会改变一个数的符号 右移1位相当于 除以2 应用场景 判断奇数还是偶数 使用&运算，与1进行&，如果为1，那么该数为奇数；如果为0，那么该数是偶数 交换两个数值 第一行，a = a ^ b，很容易理解 第二行， b = b ^ a = b ^ a ^ b，由于 b ^ b = 0，所以 b = a ^ 0，即 b = a 第三行， a = a ^ b ,由于a在第一步重新赋值，所以，a = a ^ b ^ a = b，完成了数值交换 a ^= b b ^= a a ^= b 寻找数据列表中的独一无二 有一个数据列表（2N+1个整数），只有一个数出现了1次，其余N个数都出现了2次。如何找到这个独一无二的数据 from functools import reduce lst = [1,5,6,4,2,6,4,2,1] reduce(lambda a,b : a^b, lst) >>> 输出5 计算一个数值的二进制数中有多少个1 def count_ones(x): count = 0 while x: count = count + 1 x = x & (x-1) # 等价于 x = x & (x-1) return count 在一堆数字中找出只出现一次的两个数字 # 查找构成res的两个数 def split_res(res, lst): tmp = 1 num_0, num_1 = 0, 0 while not tmp & res: tmp 路径 Python路径操作模块pathlib，看这篇就够了！ pathlib和os常用操作对比 通过常用路径操作的对比，可以更深刻理解pathlib和os的区别，便于在实际操作中做对照，也便于进行使用替代，详细对比如下： pathlib操作 os及os.path操作 功能描述 Path.resolve() os.path.abspath() 获得绝对路径 Path.chmod() os.chmod() 修改文件权限和时间戳 Path.mkdir() os.mkdir() 创建目录 Path.rename() os.rename() 文件或文件夹重命名，如果路径不同，会移动并重新命名 Path.replace() os.replace() 文件或文件夹重命名，如果路径不同，会移动并重新命名，如果存在，则破坏现有目标。 Path.rmdir() os.rmdir() 删除目录 Path.unlink() os.remove() 删除一个文件 Path.unlink() os.unlink() 删除一个文件 Path.cwd() os.getcwd() 获得当前工作目录 Path.exists() os.path.exists() 判断是否存在文件或目录name Path.home() os.path.expanduser() 返回电脑的用户目录 Path.is_dir() os.path.isdir() 检验给出的路径是一个文件 Path.is_file() os.path.isfile() 检验给出的路径是一个目录 Path.is_symlink() os.path.islink() 检验给出的路径是一个符号链接 Path.stat() os.stat() 获得文件属性 PurePath.is_absolute() os.path.isabs() 判断是否为绝对路径 PurePath.joinpath() os.path.join() 连接目录与文件名或目录 PurePath.name os.path.basename() 返回文件名 PurePath.parent os.path.dirname() 返回文件路径 Path.samefile() os.path.samefile() 判断两个路径是否相同 PurePath.suffix os.path.splitext() 分离文件名和扩展名 配置文件解读 python配置文件INI/TOML/YAML/ENV的区别 ini ini文件可能是我们可以使用的最直接的配置文件。ini文件非常适合较小的项目，主要是因为这些文件仅支持1级深的层次结构，ini文件本质上是平面文件，但变量可以属于组 [APP] ENVIRONMENT = development DEBUG = False [DATABASE] USERNAME: root PASSWORD: p@ssw0rd HOST: 127.0.0.1 PORT: 5432 DB: my_database [LOGS] ERRORS: logs/errors.log INFO: data/info.log [FILES] STATIC_FOLDER: static TEMPLATES_FOLDER: templates python解析ini文件代码如下所示 import configparser config = configparser.ConfigParser() path = r'Q:\\pyCharmWS\\object_detection\\test.ini' cfg = config.read(path) # 多种方式获取值 config['DATABASE']['HOST'] Out[6]: '127.0.0.1' config.get('DATABASE', 'HOST') Out[7]: '127.0.0.1' # 获取指定类型 config.getboolean('APP', 'DEBUG') Out[8]: False config.get('APP', 'DEBUG') Out[9]: 'False' configparser还有许多其他类型检查方法，例如getint()，getfloat()等等 toml TOML文件似乎与ini文件共享某些语法相似之处，但支持更广泛的数据类型以及值本身之间的关系 如表中所示，TOML支持嵌套表的概念，该[environments]表后面带有多个子表，通过使用点符号，我们能够创建表的关联，这意味着它们是同一元素的不同实例 # Keys title = \"My TOML Config\" # Tables [project] name = \"Faceback\" description = \"Powerful AI which renders the back of somebody's head, based on their face.\" version = \"1.0.0\" updated = 1979-05-27T07:32:00Z author = \"Todd Birchard\" [database] host = \"127.0.0.1\" password = \"p@ssw0rd\" port = 5432 name = \"my_database\" connection_max = 5000 enabled = true # Nested `tables` [environments] [environments.dev] ip = \"10.0.0.1\" dc = \"eqdc10\" [environments.staging] ip = \"10.0.0.2\" dc = \"eqdc10\" [environments.production] ip = \"10.0.0.3\" dc = \"eqdc10\" # Array of Tables [[testers]] id = 1 username = \"JohnCena\" password = \"YouCantSeeMe69\" [[testers]] id = 3 username = \"TheRock\" password = \"CantCook123\" 同样有趣的是概念表列，如下表中的[[testers]]，双括号中的表会自动添加到数组中，其中数组中的每个项目都是具有相同名称的表，等价于下下面JSON所表达的信息 { \"testers\": [{ \"id\": 1, \"username\": \"JohnCena\", \"password\": \"YouCantSeeMe69\" }, { \"id\": 2, \"username\": \"TheRock\", \"password\": \"CantCook123\" } ] } python解析toml的代码如下 import toml path = r'Q:\\pyCharmWS\\object_detection\\test.toml' cfg = toml.load([path]) # Retrieving a dictionary cfg['project'] cfg.get('project') # Retrieving a value cfg['project']['author'] cfg.get('project').get('author') print(cfg) { 'title': 'My TOML Config', 'project': { 'name': 'Faceback', 'description': \"Powerful AI which renders the back of somebody's head, based on their face.\", 'version': '1.0.0', 'updated': datetime.datetime(1979, 5, 27, 7, 32, tzinfo = ), 'author': 'Todd Birchard' }, 'database': { 'host': '127.0.0.1', 'password': 'p@ssw0rd', 'port': 5432, 'name': 'my_database', 'connection_max': 5000, 'enabled': True }, 'environments': { 'dev': { 'ip': '10.0.0.1', 'dc': 'eqdc10' }, 'staging': { 'ip': '10.0.0.2', 'dc': 'eqdc10' }, 'production': { 'ip': '10.0.0.3', 'dc': 'eqdc10' } }, 'testers': [{ 'id': 1, 'username': 'JohnCena', 'password': 'YouCantSeeMe69' }, { 'id': 3, 'username': 'TheRock', 'password': 'CantCook123' }] } yaml 系统变量 from os import environ environ.get('ComSpec') Out[0]: 'C:\\\\WINDOWS\\\\system32\\\\cmd.exe' 流 Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-12-01 00:57:11 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/4.python异步编程.html":{"url":"chapters/4.python异步编程.html","title":"python异步编程","keywords":"","body":"基础概念进程、线程和协程进程线程协程三者关系阻塞和非阻塞异步和同步并行和并发概念总结asyncio 标准库协程核心概念入门示例进阶示例核心概念协程函数和对象Task 对象asyncio.Future对象futures.Future对象async/await 关键字event_loop 事件循环消息队列基本使用协程入门例子异步工具函数协程对象运行结果获取异步迭代器异步上下文管理器进阶使用uvloop自定义策略异步Redis异步MySQLFastAPI框架爬虫多任务多任务实现方式协程锁API索引高级API索引底层API索引asyncio与gevent比较web框架基础知识WSGI协议epoll技术异步框架tornado逻辑流程对比其它框架分布式系统分布式任务队列hueyCeleryweb框架企业级Django高并发Tornado快速建站Flask自定义协议Twisted消息队列RabbitMQRedis相关疑问多线程存在意义协程常见问题 基础概念 python3 asyncio官方文档中文版 asyncio --- 异步 I/O 2小时学会python asyncio【花39大洋买的课程】 Python进程、线程和协程实战指归 异步编程基本概念 python---异步IO(asyncio)协程 Python黑魔法 --- 异步IO(asyncio)协程 python协程系列（六）——asyncio的EventLoop以及Future详解 python协程系列（七）——asyncio结合多线程解决阻塞问题以及timer模拟 asyncio —— 异步I/O、事件循环、协程和任务 python协程与异步协程 [进阶]-Python3 异步编程详解（史上最全篇） python多线程、多进程、协程的使用 Python异步IO之协程(一): 从yield from到async的使用 python异步编程模块asyncio学习(一) python异步编程模块asyncio学习(二) Python中协程异步IO（asyncio）详解 理解 Python 中的异步编程 Python异步编程模块asyncio学习 !此模块非常之重要! 带你简单了解python协程和异步 进程、线程和协程 线程是并发，进程是并行；进程之间相互独立，是系统分配资源的最小单位，同一个进程中的所有线程共享资源 进程拥有自己的内存空间，所以进程间数据不共享，开销大。 线程：调度执行的最小单位，也叫执行路径，不能独立存在，依赖进程的存在而存在 一个进程至少有一个线程，叫主线程，多个线程共享内存（数据共享和全局变量），因此提升程序的运行效率。 协程：用户态的轻量级线程，调度有用户控制， 拥有自己的寄存器上下文和栈，切换基本没有内核切换的开销，切换灵活。 进程 进程最大优势是可以充分例用计算资源 使用进程处理计算密集型任务：因为不同的进程可以运行的不同CPU的不同的核上 假如一台计算机的CPU共有16核，则可以启动16个或更多个进程来并行处理任务 使用单个线程或两个线程的时候，耗时大约30+秒，改用两个进程后，耗时17.786秒，差不多快了一倍 如果使用4个进程（前提是运行的代码的计算机至少有4个CPU核）的话，速度还能提高一倍 对于计算密集型的任务，使用多进程并行处理是有效的提速手段，通常，进程数量选择CPU核数的整倍数 进程通信 线程间通信可以使用队列、互斥锁、信号量、事件和条件等多种同步方式，同样的，这些手段也可以应用在进程间 此外，multiprocessing 模块还提供了管道和共享内存等进程间通信的手段。 进程池 使用多进程并行处理任务时，处理效率和进程数量并不总是成正比，当进程数量超过一定限度后，完成任务所需时间反而会延长 进程池提供了一个保持合理进程数量的方案，但合理进程数量需要根据硬件状况及运行状况来确定，通常设置为 CPU 的核数 multiprocessing.Pool(n) 可创建 n 个进程的进程池供用户调用 如果进程池任务不满，则新的进程请求会被立即执行；如果进程池任务已满，则新的请求将等待至有可用进程时才被执行 向进程池提交任务有以下两种方式 apply_async(func[, args[, kwds[, callback]]]) ：非阻塞式提交。即使进程池已满，也会接受新的任务，不会阻塞主进程 新任务将处于等待状态 apply(func[, args[, kwds]]) ：阻塞式提交。若进程池已满，则主进程阻塞，直至有空闲进程可以使用 线程 线程的最大意义在于并行 使用线程处理IO密集型任务：对于IO密集型（本例仅测试网络IO，没有磁盘IO）的任务，适量的线程可以在一定程度上提高处理速度 随着线程数量的增加，速度的提升不再明显 使用线程处理**计算密集型**任务: 对一张千万级像素的照片做低端增强，借助于NumPy的广播和矢量化计算，耗时0.38秒钟；单线程逐像素处理的话，耗时相当于NumPy的100倍；启用多线程的话，速度不仅没有加快，反倒是比单线程更慢 这说明，对于计算密集型的任务来说，多线程并不能提高处理速度，相反，因为要创建和管理线程，处理速度会更慢一些 线程池 尽管多线程可以并行处理多个任务，但开启线程不仅花费时间，也需要占用系统资源 因此，线程数量不是越多越快，而是要保持在合理的水平上 线程池可以让我们用固定数量的线程完成比线程数量多得多的任务 协程 谈谈Python协程技术的演进 概念 线程常用于多任务并行。对于可以切分的IO密集型任务，将切分的每一小块任务分配给一个线程，可以显著提高处理速度 而协程，无论有多少个，都被**限定在一个线程内执行**，因此，协程又被称为**微线程** 从宏观上看，线程任务和协程任务都是并行的 从微观上看，线程任务是分时切片轮流执行的，这种切换是系统自动完成的，无需程序员干预 而协程则是根据任务特点，在任务阻塞时将控制权交给其他协程，这个权力交接的时机和位置，由程序员指定 参与协程管理的每一个任务，必须存在阻塞的可能，且阻塞条件会被其它任务破坏，从而得以在阻塞解除后继续执行 尽管协程难以驾驭，但是由于是在一个线程内运行，免除了线程或进程的切换开销，因而协程的运行效率高，在特定场合下仍然被广泛使用。 协程演进史 Py2时代，Python并不支持协程，仅可通过yield实现部分的协程功能 另外可以通过gevent等第三方库实现协程，gevent最好玩的，莫过于monkey_patch(猴子补丁) Py3.4开始，Python内置asyncio标准库，正式原生支持协程 asyncio的异步操作，需要在协程中通过yield from完成，协程函数则需要使用@asyncio.coroutine装饰器 为了更贴近人类思维，Py3.5引入了新的语法async和await，可以让协程的代码稍微易懂一点点 本质上，async就是@asyncio.coroutine，替换为await就是yield from，换个马甲，看起来就顺眼多了 三者关系 使用场景 计算密集型(CPU操作指令比较多，如科学计算，位数多的浮点运算) * 考虑可以使用多核 CPU，使用多进程 I/O密集型(读写数据操作较多的，比如爬虫) * I/O请求比较耗时的话，使用协程 * I/O请求比较快的话，使用多线程 协程和多线程异同点比较 共同点：都是并发操作，多线程同一时间点只能有一个线程在执行，协程同一时间点只能有一个任务在执行 不同点： 多线程，是在I/O阻塞时通过切换线程来达到并发的效果 在什么情况下做线程切换是由操作系统来决定的，开发者不用操心，但会造成竞争条件 (race condition) 协程，只有一个线程，在I/O阻塞时通过在线程内切换任务来达到并发的效果 在什么情况下做任务切换是开发者决定的，不会有竞争条件 (race condition) 的情况 多线程的线程切换比协程的任务切换开销更大 对于开发者而言，多线程并发的代码比协程并发的更容易书写，一般情况下协程并发的处理效率比多线程并发更高 ps：对第一点和第二点补充 对于io阻塞的操作，协程相较于线程，能更精确的获取（或者释放）对资源的控制权 这是因为用户层相较于语言层，用户层能更好的感知特定操作的时机 对于非io阻塞的操作，线程相较于协程，能更公平的分配对资源的控制权 这是因为语言层相较于用户层，语言层能更好的感知到多个线程的运行状态 并在掌握更多信息的前提下（线程运行的字节码和时长），进行更加合理的GIL的获取和释放 阻塞和非阻塞 阻塞 程序未得到所需计算资源时被挂起的状态 程序在等待某个操作完成期间，自身无法继续干别的事情，称程序在该操作上是阻塞的 常见的阻塞形式有：网络I/O阻塞、磁盘I/O阻塞、用户输入阻塞等 阻塞是无处不在的，包括CPU切换上下文时，所有的进程都无法真正干事情，它们也会被阻塞（如果是多核CPU则正在执行上下文切换操作的核不可被利用） 非阻塞 程序在等待某操作过程中，自身不被阻塞，可以继续运行干别的事情，则称该程序在该操作上是非阻塞的 非阻塞并不是在任何程序级别、任何情况下都可以存在的 仅当程序封装的级别可以囊括独立的子程序单元时，它才可能存在非阻塞状态 非阻塞的存在是因为阻塞存在，正因为某个操作阻塞导致的耗时与效率低下，我们才要把它变成非阻塞的 异步和同步 同步 不同程序单元为了完成某个任务，在执行过程中需靠某种通信方式以协调一致，称这些程序单元是同步执行的 例如购物系统中更新商品库存，需要用“行锁”作为通信信号，让不同的更新请求强制排队顺序执行，那更新库存的操作是同步的 简言之，同步意味着有序 异步 为完成某个任务，不同程序单元之间过程中无需通信协调，也能完成任务的方式 不相关的程序单元之间可以是异步的 例如，爬虫下载网页。调度程序调用下载程序后，即可调度其他任务，而无需与该下载任务保持通信以协调行为 不同网页的下载、保存等操作都是无关的，也无需相互通知协调。这些异步操作的完成时刻并不确定 简言之，异步意味着无序 当一个异步过程调用发出后，调用者在没有得到结果之前，就可以继续执行后续操作 当这个调用完成后，一般通过状态、通知和回调来通知调用者 对于异步调用，调用的返回并不受调用者控制 对于通知调用者的三种方式 状态：即监听被调用者的状态(轮询)，调用者需要每隔一定时间检查一次，效率会很低 通知：当被调用者执行完成后，发出通知告知调用者，无需消耗太多性能 回调：与通知类似，当被调用者执行完成后，会调用调用者提供的回调函数 并行和并发 并发(多线程) 任务特点：IO密集型任务：任务包含频繁的、持续的网络IO和磁盘IO 单个CPU（也可以多个CPU）将多个线程中的每个线程（多个进程中的每个进程）按时间分为一个一个的时间片 每一个时刻只执行某个线程（进程）的时间片，时间片过期后转而执行下一个线程（进程）的时间片 并发描述的是程序的组织结构。指程序要被设计成多个可独立执行的子任务 以利用有限的计算机资源使多个任务可以被实时或近实时执行为目的 并发提供了一种程序组织结构方式，让问题的解决方案可以并行执行 注：并发宏观上看起来像是并行但是微观上并不能做到并行 并行(多进程) 任务特点：计算密集型任务：任务包含大量计算，CPU占用率高 当有多个CPU或者是多核CPU时才有可能实现并行，并行就是多个线程或者多个进程同时运行 并行描述的是程序的执行状态。指多个任务同时被执行 以利用富余计算资源（多核CPU）加速完成多个任务为目的 组合分析同步阻塞、同步非阻塞，异步阻塞、异步非阻塞 举个简单的例子来描述这四种情况，老张要做两件事，用水壶烧开水，看电视，两件事情即两个任务，两个函数 同步阻塞：老张把水壶放到火上，就坐在那里等水开，开了之后我再去看电视 同步非阻塞：老张把水壶放到火上，去客厅看电视，时不时去厨房看看水开没有 老张还是觉得自己有点傻，于是变高端了，买了把会响笛的那种水壶。水开之后，能大声发出嘀~~~~的噪音 异步阻塞：老张把响水壶放到火上，然后就坐在旁边等着听那个烧开的提示音 异步非阻塞：老张把响水壶放到火上，去客厅看电视，水壶响之前不再去看它了，响了再去拿壶 乍一看，这同步阻塞、意不阻塞似乎没有什么区别，但实际上是有区别的，所谓同步异步，指的是消息通知的机制 区别在哪里呢 在这个例子中同步异步只是对于水壶而言。在使用普通水壶的时候，我要自己主动去观察水是不是烧开了，自己主动去获取烧开的这个结果，即所谓的同步 但是在响水壶的时候，我不需要再管水烧到什么程度了，因为只要水烧开了，那个滴滴的噪声就会通知我的，即所谓的异步 他们的相同点是，在烧水的过程中，老王啥也没干，即“阻塞” 四种总结——同步/异步与阻塞/非阻塞 同步阻塞形式：效率是最低的。拿上面的例子来说，在烧水的过程中，什么别的事都不做 同步非阻塞形式：实际上是效率低下的。因为老王需要不断的在看电视与烧水之间来回跑动，看一下电视，又要去看一下水烧开 没有，这样来回跑很多次，在程序中，程序需要在这两种不同的行为之间来回的切换，效率可想而知是低下的 异步阻塞形式：异步操作是可以被阻塞住的，只不过它不是在处理消息时阻塞，而是在等待消息通知时被阻塞 这个效率其实跟同步阻塞差不多的 异步非阻塞形式：效率更高。因为老王把水烧好之后就不用管了，可以安安心心去看电视，不用来回奔波看水烧开了没 因为水烧开了会有提示告诉他水烧好了，这样效率岂不是更高 那有没有更好的办法？ 当然有，如果老王还有一个帮手老张，让老王自己看电视、同时老张去烧开水，这样岂不是更好？这就是所谓的并行 并发/并行、同步/异步、阻塞/非阻塞 并发/并行：即能够开启多个任务，多个任务交替执行为并发，多个任务同时执行为并行 同步/异步：关注的是消息通知的机制，主动等候消息则为同步、被动听消息则为异步 阻塞/非阻塞：关注的是等候消息的过程中有没有干其他事 总结：上面的几组概念，时刻穿插的，并没有完全的等价关系 所以经常有人说，异步就是非阻塞，同步就是阻塞，并发就是非阻塞、并行就是非阻塞，这些说法都是不完全准确地。 概念总结 要支持并发，必须拆分为多任务，不同任务相对而言才有阻塞/非阻塞、同步/异步 并行是为了利用多核加速多任务完成的进度 并发是为了让独立的子任务都有机会被尽快执行，但不一定能加速整体进度 非阻塞是为了提高程序整体执行效率 异步是高效地组织非阻塞任务的方式 所以，并发、异步、非阻塞三个词总是如影随形 asyncio 标准库 协程核心概念 python协程的多种实现方式 在Python中有多种方式可以实现协程，例如： greenlet，是一个第三方模块，用于实现协程代码（Gevent协程就是基于greenlet实现） yield，生成器，借助生成器的特点也可以实现协程代码 asyncio，在Python3.4中引入的模块用于编写协程代码 async & awiat，Python3.5中引入的关键字，结合asyncio模块可以更方便的编写协程代码 目前主流使用是Python官方推荐的 asyncio 模块和 async&await 关键字的方式 例如：在tonado、sanic、fastapi、django3 中均已支持 任务类型 计算密集型任务：任务包含大量计算，CPU占用率高 IO密集型任务：任务包含频繁的、持续的网络IO和磁盘IO 混合型任务：既有计算也有IO 协程状态 协程函数相比于一般的函数来说，我们可以将协程包装成任务Task，任务Task就在于可以跟踪它的状态，我就知道它具体执行到哪一步了 一般来说，协程函数具有4种状态 Pending：创建future的时候，task为pending Running：事件循环调用执行的时候当然就是running Done：调用完毕自然就是done Cacelled：停止事件循环，就需要先把task取消，即为cancelled greenlet库 greentlet是一个第三方模块，需要提前安装 pip3 install greenlet才能使用 from greenlet import greenlet def func1(): print(1) # 第1步：输出 1 gr2.switch() # 第3步：切换到 func2 函数 print(2) # 第6步：输出 2 gr2.switch() # 第7步：切换到 func2 函数，从上一次执行的位置继续向后执行 def func2(): print(3) # 第4步：输出 3 gr1.switch() # 第5步：切换到 func1 函数，从上一次执行的位置继续向后执行 print(4) # 第8步：输出 4 gr1 = greenlet(func1) gr2 = greenlet(func2) gr1.switch() # 第1步：去执行 func1 函数 注意：switch中也可以传递参数用于在切换执行时相互传递值 yield关键字 基于Python的生成器的yield和yield form关键字实现协程代码 def func1(): yield 1 yield from func2() yield 2 def func2(): yield 3 yield 4 f1 = func1() for item in f1: print(item) 注意：yield form关键字是在Python3.3中引入的 入门示例 asyncio模块 在Python3.4之前官方未提供协程的类库，一般大家都是使用greenlet等其他来实现 在Python3.4发布后官方正式支持协程，即：asyncio模块 import asyncio @asyncio.coroutine def func1(): print(1) yield from asyncio.sleep(2) # 遇到IO耗时操作，自动化切换到tasks中的其他任务 print(2) @asyncio.coroutine def func2(): print(3) yield from asyncio.sleep(2) # 遇到IO耗时操作，自动化切换到tasks中的其他任务 print(4) tasks = [asyncio.ensure_future( func1() ), asyncio.ensure_future( func2() )] loop = asyncio.get_event_loop() loop.run_until_complete(asyncio.wait(tasks)) 注意：基于asyncio模块实现的协程比之前的要厉害，内部集成了遇到IO耗时操作自动切换的功能 async & awit 关键字 async & awit 关键字在Python3.5版本中正式引入，让代码可以更加简便 Python3.8之后 @asyncio.coroutine 装饰器就会被移除，推荐使用async & awit 关键字实现协程代码 import asyncio async def func1(): print(1) await asyncio.sleep(2) print(2) async def func2(): print(3) await asyncio.sleep(2) print(4) tasks = [asyncio.ensure_future( func1() ), asyncio.ensure_future( func2() )] loop = asyncio.get_event_loop() loop.run_until_complete(asyncio.wait(tasks)) 进阶示例 用代码实现下载 url_list 中的图片 同步编程实现 \"\"\" 下载图片使用第三方模块requests，请提前安装：pip3 install requests \"\"\" import requests def download_image(url): print(\"开始下载:\",url) # 发送网络请求，下载图片 response = requests.get(url) print(\"下载完成\") # 图片保存到本地文件 file_name = url.rsplit('_')[-1] with open(file_name, mode='wb') as file_object: file_object.write(response.content) if __name__ == '__main__': url_list = [ 'https://www3.autoimg.cn/newsdfs/g26/M02/35/A9/120x90_0_autohomecar__ChsEe12AXQ6AOOH_AAFocMs8nzU621.jpg', 'https://www2.autoimg.cn/newsdfs/g30/M01/3C/E2/120x90_0_autohomecar__ChcCSV2BBICAUntfAADjJFd6800429.jpg', 'https://www3.autoimg.cn/newsdfs/g26/M0B/3C/65/120x90_0_autohomecar__ChcCP12BFCmAIO83AAGq7vK0sGY193.jpg' ] for item in url_list: download_image(item) 基于协程的异步编程实现 #!/usr/bin/env Python # -- coding: utf-8 -- \"\"\" @version: v1.0 @author: huangyc @file: asyncio_test_http.py @Description: 下载图片使用第三方模块aiohttp，请提前安装：pip3 install aiohttp @time: 2021/2/19 9:23 \"\"\" import aiohttp import asyncio async def fetch(session, url): print(\"发送请求：\", url) async with session.get(url, verify_ssl=False) as response: content = await response.content.read() file_name = url.rsplit('_')[-1] with open(file_name, mode='wb') as file_object: file_object.write(content) async def main(): async with aiohttp.ClientSession() as session: url_list = [ 'https://www3.autoimg.cn/newsdfs/g26/M02/35/A9/120x90_0_autohomecar__ChsEe12AXQ6AOOH_AAFocMs8nzU621.jpg', 'https://www2.autoimg.cn/newsdfs/g30/M01/3C/E2/120x90_0_autohomecar__ChcCSV2BBICAUntfAADjJFd6800429.jpg', 'https://www3.autoimg.cn/newsdfs/g26/M0B/3C/65/120x90_0_autohomecar__ChcCP12BFCmAIO83AAGq7vK0sGY193.jpg' ] tasks = [asyncio.create_task(fetch(session, url)) for url in url_list] await asyncio.wait(tasks) if __name__ == '__main__': asyncio.run(main()) 上述两种的执行对比之后会发现，基于协程的异步编程 要比 同步编程的效率高了很多 同步编程，按照顺序逐一排队执行，如果图片下载时间为2分钟，那么全部执行完则需要6分钟。 异步编程，几乎同时发出了3个下载任务的请求（遇到IO请求自动切换去发送其他任务请求），如果图片下载时间为2分钟，那么全部执行完毕也大概需要2分钟左右就可以了。 第三方模块不支持协程方式异步编程 import asyncio import requests async def download_image(url): # 发送网络请求，下载图片（遇到网络下载图片的IO请求，自动化切换到其他任务） print(\"开始下载:\", url) loop = asyncio.get_event_loop() # requests模块默认不支持异步操作，所以就使用线程池来配合实现了。 future = loop.run_in_executor(None, requests.get, url) response = await future print('下载完成') # 图片保存到本地文件 file_name = url.rsplit('_')[-1] with open(file_name, mode='wb') as file_object: file_object.write(response.content) if __name__ == '__main__': url_list = [ 'https://www3.autoimg.cn/newsdfs/g26/M02/35/A9/120x90_0_autohomecar__ChsEe12AXQ6AOOH_AAFocMs8nzU621.jpg', 'https://www2.autoimg.cn/newsdfs/g30/M01/3C/E2/120x90_0_autohomecar__ChcCSV2BBICAUntfAADjJFd6800429.jpg', 'https://www3.autoimg.cn/newsdfs/g26/M0B/3C/65/120x90_0_autohomecar__ChcCP12BFCmAIO83AAGq7vK0sGY193.jpg' ] tasks = [download_image(url) for url in url_list] loop = asyncio.get_event_loop() loop.run_until_complete(asyncio.wait(tasks)) 核心概念 python协程系列（五）——asyncio的核心概念与基本架构 协程函数和对象 协程函数：定义形式为 async def fun() 的函数 在 Python 3.4 中，asyncio 模块出现，此时创建协程函数须使用 asyncio.coroutine 装饰器标记 此前的包含 yield from 语句的函数既可以称作生成器函数也可以称作协程函数 为了突出协程的重要性，现在使用 asyncio.coroutine 装饰器的函数就是真正的协程函数了 协程对象：调用 协程函数 所返回的对象 即协程函数的运行结果为协程对象，协程对象需要包装成任务注入到事件循环，由事件循环调用 怎么判断一个函数是不是协程？ 通过asyncio.iscoroutine（obj）和 asyncio.iscoroutinefunction(func)加以判断，返回true，则是 # 定义一个协程函数 async def func(): pass # 调用协程函数，返回一个协程对象 result = func() 注意：调用协程函数时，函数内部代码不会执行，只是会返回一个协程对象 要执行协程函数的内部代码，需要 事件循环 和 协程对象 配合才能实现 import asyncio async def func(): print(\"协程内部代码\") # 调用协程函数，返回一个协程对象。 result = func() # 方式一 # loop = asyncio.get_event_loop() # 创建一个事件循环 # loop.run_until_complete(result) # 将协程当做任务提交到事件循环的任务列表中，协程执行完成之后终止。 # 方式二 # 本质上方式一是一样的，内部先 创建事件循环 然后执行 run_until_complete，一个简便的写法。 # asyncio.run 函数在 Python 3.7 中加入 asyncio 模块， asyncio.run(result) 这个过程可以简单理解为： 将协程当做任务添加到 事件循环 的任务列表 然后事件循环检测列表中的协程是否 已准备就绪（默认可理解为就绪状态） 如果准备就绪则执行其内部代码 Task 对象 Tasks are used to schedule coroutines concurrently. When a coroutine is wrapped into a Task with functions like asyncio.create_task() the coroutine is automatically scheduled to run soon。 将协程对象作为参数创建任务，任务是对协程对象的封装，其中包含任务的各种状态，是一个可以挂起的函数 协程不是线程安全，这样可以让协程加入事件循环中等待被调度执行 异步编程最重要的就是对异步操作状态的把控 (1) 创建任务（两种方法）： task = asyncio.create_task(coro()) # 这是3.7版本新添加的(建议) task = asyncio.ensure_future(coro()) # Python 3.7 之前，可以使用低层级的函数 也可以使用：loop.create_future()或loop.create_task(coro) (不建议手动实例化 Task 对象) 本质上是将协程对象封装成task对象，并将协程立即加入事件循环，同时追踪协程的状态 import asyncio async def func(): print(1) await asyncio.sleep(2) print(2) return \"返回值\" async def main(): print(\"main开始\") # 创建协程，将协程封装到Task对象中并添加到事件循环的任务列表中，等待事件循环去执行（默认是就绪状态）。 # 在调用 task_list = [asyncio.create_task(func(), name=\"n1\"), asyncio.create_task(func(), name=\"n2\")] print(\"main结束\") # 当执行某协程遇到IO操作时，会自动化切换执行其他任务。 # 此处的await是等待所有协程执行完毕，并将所有协程的返回值保存到done # 如果设置了timeout值，则意味着此处最多等待的秒，完成的协程返回值写入到done中，未完成则写到pending中。 done, pending = await asyncio.wait(task_list, timeout=None) print(done, pending) asyncio.run(main()) 注意：asyncio.wait 源码内部会对列表中的每个协程执行ensure_future从而封装为Task对象 所以在和wait配合使用时task_list的值为[func(),func()] 也是可以的。 或者： import asyncio async def func(): print(\"执行协程函数内部代码\") # 遇到IO操作挂起当前协程（任务），等IO操作完成之后再继续往下执行。当前协程挂起时，事件循环可以去执行其他协程（任务）。 response = await asyncio.sleep(2) print(\"IO请求结束，结果为：\", response) coroutine_list = [func(), func()] # 错误：coroutine_list = [ asyncio.create_task(func()), asyncio.create_task(func()) ] # 此处不能直接 asyncio.create_task，因为将Task立即加入到事件循环的任务列表， # 但此时事件循环还未创建，所以会报错。 # 使用asyncio.wait将列表封装为一个协程，并调用asyncio.run实现执行两个协程 # asyncio.wait内部会对列表中的每个协程执行ensure_future，封装为Task对象。 done,pending = asyncio.run( asyncio.wait(coroutine_list) ) (2) 获取某一个任务的方法： 返回在某一个指定的loop中，当前正在运行的任务，如果没有任务正在运行，则返回None，如果loop为None，则默认为在当前的事件循环中获取 task=asyncio.current_task(loop=None) 返回某一个loop中还没有结束的任务 asyncio.all_tasks(loop=None) (3) Task类常见的一些使用函数 cancel()： 最好是使用他会出发CancelledError异常，所以需要取消的协程函数里面的代码最好在try-except语句块中进行，这样方便触发异常，打印相关信息，但是Task.cancel()没有办法保证任务一定会取消，而Future.cancel()是可以保证任务一定取消的 done()：当一个被包装得协程既没有触发异常、也没有被取消的时候，意味着它是done的，返回true result()： 返回任务的执行结果，当任务被正常执行完毕，则返回结果； 当任务被取消了，调用这个方法，会触发CancelledError异常； 当任务返回的结果是无用的时候，则调用这个方法会触发InvalidStateError； 当任务出发了一个异常而中断，调用这个方法还会再次触发这个使程序中断的异常。 exception()： 返回任务的异常信息，触发了什么异常，就返回什么异常 如果任务是正常执行的无异常，则返回None 当任务被取消了，调用这个方法会触发CancelledError异常 当任务没有做完，调用这个方法会触发InvalidStateError异常 还有一些不常用的方法： add_done_callback(callback, **, context=None*) remove_done_callback(callback) get_stack(**, limit=None*) print_stack(**, limit=None, file=None*) all_tasks(loop=None)，这是一个类方法 current_task(loop=None)，这是一个类方法 asyncio.Future对象 A Futureis a special low-level awaitable object that represents an eventual result of an asynchronous operation. asyncio中的Future对象是一个相对更偏向底层的可对象，通常我们不会直接用到这个对象，而是直接使用Task对象来完成任务的并和状态的追踪 Task 是 Futrue的子类，和task上没有本质上的区别，没有必要去用Future，用Task就可以了 Future为我们提供了异步编程中的 最终结果 的处理（Task类也具备状态处理的功能） 当一个Future对象被等待的时候，协程会一直等待，直到Future已经运算完毕 asyncio中的Future类是模仿concurrent.futures.Future类而设计的 示例1： async def main(): # 获取当前事件循环 loop = asyncio.get_running_loop() # # 创建一个任务（Future对象），这个任务什么都不干。 fut = loop.create_future() # 等待任务最终结果（Future对象），没有结果则会一直等下去。 await fut asyncio.run(main()) 示例2： import asyncio async def set_after(fut): await asyncio.sleep(2) fut.set_result(\"666\") # 通常是不会这样设置的，这里只是演示 async def main(): # 获取当前事件循环 loop = asyncio.get_running_loop() # 创建一个任务（Future对象），没绑定任何行为，则这个任务永远不知道什么时候结束。 fut = loop.create_future() # 创建一个任务（Task对象），绑定了set_after函数，函数内部在2s之后，会给fut赋值。 # 即手动设置future任务的最终结果，那么fut就可以结束了。 await loop.create_task(set_after(fut)) # 等待 Future对象获取 最终结果，否则一直等下去 data = await fut print(data) asyncio.run(main()) Future对象本身函数进行绑定，所以想要让事件循环获取Future的结果，则需要手动设置 而Task对象继承了Future对象，其实就对Future进行扩展，他可以实现在对应绑定的函数执行完成之后，自动执行set_result，从而实现自动结束 虽然，平时使用的是Task对象，但对于结果的处理本质是基于Future对象来实现的 扩展：支持 await 对象语 法的对象课成为可等待对象，所以 协程对象、Task对象、Future对象 都可以被成为可等待对象。 asyncio中关于Future的几个方法 asyncio.isfuture(obj)：判断一个对象是不是Future，注意python中一切皆对象哦，包括函数，当obj是下面几种情况时返回true asyncio.Future的实例对象 asyncio.Task的实例对象 一个具有 _asyncio_future_blocking属性的对象 asyncio.ensure_future(obj, *, loop=None)。将一个obj包装成Future asyncio.wrap_future(future, *, loop=None) 将concurrent.futures.Future对象包装成一个 asyncio.Future 对象。 Future对象的常用方法 result()：返回Future执行的结果返回值 如果Future被执行完成，如果使用set_result()方法设置了一个结果，那个设置的value就会被返回 如果Future被执行完成，如果使用set_exception()方法设置了一个异常，那么使用这个方法也会触发异常 如果Future被取消了，那么使用这个方法会触发CancelledError异常 如果Future的结果不可用或者是不可达，那么使用这个方法也会触发InvalidStateError异常 set_result(result)：标记Future已经执行完毕，并且设置它的返回值 set_exception(exception)：标记Future已经执行完毕，并且触发一个异常 done()：如果Future1执行完毕，则返回 True cancelled()：判断任务是否取消 add_done_callback(callback, *, context=None)：在Future完成之后，给它添加一个回调方法 这个方法就相当于是loop.call_soon()方法，如果要回调带有关键字参数的函数，也需要使用partial方法哦 remove_done_callback(callback) cancel() exception() get_loop()：返回Future所绑定的事件循环 futures.Future对象 在Python的concurrent.futures模块中也有一个Future对象，这个对象是基于线程池和进程池实现异步操作时使用的对象。 import time from concurrent.futures import Future from concurrent.futures.thread import ThreadPoolExecutor from concurrent.futures.process import ProcessPoolExecutor def func(value): time.sleep(1) print(value) pool = ThreadPoolExecutor(max_workers=5) # 或 pool = ProcessPoolExecutor(max_workers=5) for i in range(10): fut = pool.submit(func, i) print(fut) 两个Future对象是不同的，他们是为不同的应用场景而设计 例如：concurrent.futures.Future不支持await语法 等 官方提示两对象之间不同： unlike asyncio Futures, concurrent.futures.Future instances cannot be awaited. asyncio.Future.result() and asyncio.Future.exception() do not accept the timeout argument. asyncio.Future.result() and asyncio.Future.exception() raise an InvalidStateError exception when the Future is not done. Callbacks registered with asyncio.Future.add_done_callback() are not called immediately. They are scheduled with loop.call_soon() instead. asyncio Future is not compatible with the concurrent.futures.wait() and concurrent.futures.as_completed() functions. 在Python提供了一个将futures.Future 对象包装成asyncio.Future对象的函数 asynic.wrap_future 为什么python会提供这种功能？ 一般在程序开发中我们要么统一使用 asycio 的协程实现异步操作、要么都使用进程池和线程池实现异步操作 但如果 协程的异步和 进程池/线程池的异步 混搭时，那么就会用到此功能了 import time import asyncio import concurrent.futures def func1(): # 某个耗时操作 time.sleep(2) return \"defaulf pool\" async def main(): loop = asyncio.get_running_loop() # 1. Run in the default loop's executor ( 默认ThreadPoolExecutor ) # 第一步：内部会先调用 ThreadPoolExecutor 的 submit 方法去线程池中申请一个线程去执行func1函数，并返回一个concurrent.futures.Future对象 # 第二步：调用asyncio.wrap_future将concurrent.futures.Future对象包装为asycio.Future对象。 # 因为concurrent.futures.Future对象不支持await语法，所以需要包装为 asycio.Future对象 才能使用。 fut = loop.run_in_executor(None, func1) result = await fut print('default thread pool', result) # # 2. Run in a custom thread pool: # with concurrent.futures.ThreadPoolExecutor() as pool: # result = await loop.run_in_executor(pool, func1) # print('custom thread pool', result) # 3. Run in a custom process pool: # with concurrent.futures.ProcessPoolExecutor() as pool: # result = await loop.run_in_executor(pool, func1) # print('custom process pool', result) asyncio.run(main()) 应用场景： 当项目以协程式的异步编程开发时，如果要使用一个第三方模块，而第三方模块不支持协程方式异步编程时，就需要用到这个功能 import asyncio import requests async def download_image(url): # 发送网络请求，下载图片（遇到网络下载图片的IO请求，自动化切换到其他任务） print(\"开始下载:\", url) loop = asyncio.get_event_loop() # requests模块默认不支持异步操作，所以就使用线程池来配合实现了。 future = loop.run_in_executor(None, requests.get, url) response = await future print('下载完成') # 图片保存到本地文件 file_name = url.rsplit('_')[-1] with open(file_name, mode='wb') as file_object: file_object.write(response.content) if __name__ == '__main__': url_list = [ 'https://www3.autoimg.cn/newsdfs/g26/M02/35/A9/120x90_0_autohomecar__ChsEe12AXQ6AOOH_AAFocMs8nzU621.jpg', 'https://www2.autoimg.cn/newsdfs/g30/M01/3C/E2/120x90_0_autohomecar__ChcCSV2BBICAUntfAADjJFd6800429.jpg', 'https://www3.autoimg.cn/newsdfs/g26/M0B/3C/65/120x90_0_autohomecar__ChcCP12BFCmAIO83AAGq7vK0sGY193.jpg' ] tasks = [download_image(url) for url in url_list] loop = asyncio.get_event_loop() loop.run_until_complete(asyncio.wait(tasks)) 这个方法返回一个 asyncio.Future 对象，使用 functools.partial() 传递关键字参数 给 func asyncio.``wrap_future(future, **, loop=None*) 将一个 concurrent.futures.Future 对象封装到 asyncio.Future 对象中 async/await 关键字 python3.5 用于定义协程的关键字，async定义一个协程，await用于挂起阻塞的异步调用接口 await是一个只能在协程函数中使用的关键字，用于遇到IO操作时挂起 当前协程(任务) 当前协程(任务)挂起过程中 事件循环可以去执行其他的协程(任务)，当前协程IO处理完成时，可以再次切换回来执行await之后的代码 import asyncio async def others(): print(\"start\") await asyncio.sleep(2) print('end') return '返回值' async def func(): print(\"执行协程函数内部代码\") # 遇到IO操作挂起当前协程（任务），等IO操作完成之后再继续往下执行。当前协程挂起时，事件循环可以去执行其他协程（任务）。 response1 = await others() print(\"IO请求结束，结果为：\", response1) response2 = await others() print(\"IO请求结束，结果为：\", response2) asyncio.run( func() ) 事件循环的任务列表中只有一个任务，所以在IO等待时无法演示切换到其他任务效果 在程序想要创建多个任务对象，需要使用Task对象来实现 可暂停等待的对象： 有三类对象是可等待的，即 coroutines, Tasks, and Futures coroutine：本质上就是一个函数，一前面的生成器yield和yield from为基础，不再赘述 Tasks: 任务，顾名思义，就是要完成某件事情，其实就是对协程函数进一步的封装 Future：它是一个“更底层”的概念，他代表一个一步操作的最终结果，因为一步操作一般用于耗时操作，结果不会立即得到，会在“将来”得到异步运行的结果，故而命名为Future 三者的关系，coroutine可以自动封装成task，而Task是Future的子类 event_loop 事件循环 python Event_loop(事件循环) Python 协程与事件循环 程序开启一个无限循环，把一些函数注册到事件循环上，当满足事件发生的时候，调用相应的协程函数 任务列表 = [ 任务1, 任务2, 任务3,... ] while True: 可执行的任务列表，已完成的任务列表 = 去任务列表中检查所有的任务，将'可执行'和'已完成'的任务返回 for 就绪任务 in 已准备就绪的任务列表: 执行已就绪的任务 for 已完成的任务 in 已完成的任务列表: 在任务列表中移除 已完成的任务 如果 任务列表 中的任务都已完成，则终止循环 将多线程比喻为工厂里的多个车间，那么协程就是一个车间内的多台机器。 在线程级程序中，一台机器开始工作，车间内的其它机器不能同时工作，需要等上一台机器停止，但其它车间内的机器可以同时启动，这样就可以显著提高工作效率。 在协程程序中，一个车间内的不同机器可以同时运转，启动机器、暂停运转、延时启动、停止机器等操作都可以人为设置 事件循环能够控制任务运行流程，也就是任务的调用方 协程函数，不是像普通函数那样直接调用运行的，必须添加到事件循环中，然后由事件循环去运行，单独运行协程函数是不会有结果的 import time import asyncio async def say_after_time(delay,what): await asyncio.sleep(delay) print(what) async def main(): print(f\"开始时间为： {time.time()}\") await say_after_time(1,\"hello\") await say_after_time(2,\"world\") print(f\"结束时间为： {time.time()}\") loop=asyncio.get_event_loop() #创建事件循环对象 #loop=asyncio.new_event_loop() #与上面等价，创建新的事件循环 loop.run_until_complete(main()) #通过事件循环对象运行协程函数 loop.close() 获取事件循环对象的几种方式 下面几种方式可以用来获取、设置、创建事件循环对象loop loop=asyncio.get_running_loop() 返回（获取）在当前线程中正在运行的事件循环，如果没有正在运行的事件循环，则会显示错误；它是python3.7中新添加的 loop=asyncio.get_event_loop() 获得一个事件循环，如果当前线程还没有事件循环，则创建一个新的事件循环loop； loop=asyncio.set_event_loop(loop) 设置一个事件循环为当前线程的事件循环； loop=asyncio.new_event_loop() 创建一个新的事件循环 通过事件循环运行协程函数的两种方式 创建事件循环对象loop，即asyncio.get_event_loop()，通过事件循环运行协程函数 直接通过asyncio.run(function_name)运行协程函数 需要注意的是，首先run函数是python3.7版本新添加的，前面的版本是没有的； 其次，这个run函数总是会创建一个新的事件循环并在run结束之后关闭事件循环 所以，如果在同一个线程中已经有了一个事件循环，则不能再使用这个函数了，因为同一个线程不能有两个事件循环，而且这个run函数不能同时运行两次，因为他已经创建一个了 即同一个线程中是不允许有多个事件循环loop的 运行和停止事件循环 loop.run_until_complete(future)。运行事件循环，直到future运行结束 loop.run_forever()。在python3.7中已经取消了，表示事件循环会一直运行，直到遇到stop。 loop.stop()。停止事件循环 loop.is_running()。如果事件循环依然在运行，则返回True loop.is_closed()。如果事件循环已经close，则返回True loop.close()。关闭事件循环 创建Future和Task loop.create_future(coroutine) ，返回future对象 loop.create_task(corootine) ，返回task对象 loop.set_task_factory(factory) loop.get_task_factory() 事件循环的时钟 loop.time()。可以这么理解，事件循环内部也维护着一个时钟，可以查看事件循环现在运行的时间点是多少，就像普通的time.time()类似，它返回的是一个浮点数值 import asyncio async def hello1(a,b): print('准备做加法运算') await asyncio.sleep(3) return a+b loop=asyncio.get_event_loop() t1=loop.time() #开始时间 print(t1) loop.run_until_complete(hello1(3,4)) t2=loop.time() #结束时间 print(t2) print(t2-t1) #时间间隔 ''' 运行结果为： 28525.671 准备做加法运算 28528.703 3.0320000000028813 ''' 计划执行回调函数(CallBacks) loop.call_later(delay, callback, *args, context=None) 首先简单的说一下它的含义，就是事件循环在delay多长时间之后才执行callback函数，它的返回值是asyncio.TimerHandle类的一个实例对象 loop.call_at(when, callback, *args, context=None) 即在某一个时刻进行调用计划的回调函数，第一个参数不再是delay而是when，表示一个绝对的时间点，结合前面的loop.time使用，它的使用方法和call_later()很类似。它的返回值是asyncio.TimerHandle类的一个实例对象 loop.call_soon(callback, *args, context=None) 在下一个迭代的时间循环中立刻调用回调函数，用法同上面。它的返回值是asyncio.Handle类的一个实例对象 loop.call_soon_threadsafe(callback, *args, context=None) 这是call_soon()函数的线程安全版本，计划回调函数必须在另一个线程中使用 需要注意的是：上面的几个回调函数都只使用了“位置参数”哦，asyncio中，大部分的计划回调函数都不支持“关键字参数”，如果是想要使用关键字参数，则推荐使用functools.aprtial()对方法进一步包装 总结注意事项 CallBack函数只能够定义为同步方法，不能够定义为async方法，及不能使用async和@asyncio.coroutine修饰 每一个CallBack方法只会调用一次，如果在同一个时刻有另个CallBack方法需要调用，则他们的执行顺序是不确定的 注意使用functools.partial（）去修饰带有关键字参数的CallBack方法 如何理解 对于一般的异步函数，我们需要将它放在时间循环里面，然后通过事件循环去循环调用它，而因为CallBack并不是异步函数 它是定义为普通的同步方法，所以不能够放在时间循环里面，但是如果我依然想要让事件循环去执行它怎么办呢 那就不放进事件循环，直接让事件循环“立即、稍后、在什么时候”去执行它不就行了嘛，call的含义就是“执行” 消息队列 rabbitmq和redis用作消息队列的区别 基本使用 协程入门例子 import time import asyncio def main(): start = time.time() @asyncio.coroutine # 1 def do_some_work(): # 2 print('Start coroutine') time.sleep(0.1) # 3 print('This is a coroutine') loop = asyncio.get_event_loop() # 4 coroutine = do_some_work() # 5 loop.run_until_complete(coroutine) # 6 end = time.time() print('运行耗时：{:.4f}'.format(end - start)) # 7 main() In [53]: one() Start coroutine This is a coroutine 运行耗时：0.1062 代码说明： 1、使用协程装饰器创建协程函数 2、协程函数 3、模拟 IO 操作 4、创建事件循环。每个线程中只能有一个事件循环，get_event_loop 方法会获取当前已经存在的事件循环，如果当前线程中没有，新建一个 5、调用协程函数获取协程对象 6、将协程对象注入到事件循环，协程的运行由事件循环控制。事件循环的 run_until_complete 方法会阻塞运行，直到任务全部完成。协程对象作为 run_until_complete 方法的参数，loop 会自动将协程对象包装成任务来运行。后面我们会讲到多个任务注入事件循环的情况 7、打印程序运行耗时 异步工具函数 异步转yield函数 async def get_next(ait): try: asy_obj = await ait.__anext__() return False, asy_obj except StopAsyncIteration: return True, None def iter_over_async(ait, loop): \"\"\" 异步转yield @param ait: 异步函数 @param loop: 事件循环 @return: \"\"\" ait = ait.__aiter__() while True: done, asy_obj = loop.run_until_complete(get_next(ait)) if done: break yield asy_obj 异步调用post请求 from json import JSONDecodeError from typing import Dict, Callable import aiohttp async def asy_http_post(url: str, param_dic: Dict, encoding=\"utf8\", call_back: Callable = None): \"\"\" 异步调用post请求 @param url: url地址 @param param_dic: 参数字典 @param encoding: 编码 @param call_back: 回调函数，对结果的处理 \"\"\" async with aiohttp.ClientSession() as session: async with session.post(url, json=param_dic) as response: if response.status == 200: async for data in response.content.iter_any(): # 每次接收已有的全部数据 # 处理流式数据 try: yield call_back(data.decode(encoding)) if call_back else data.decode(encoding) except JSONDecodeError as e: # 在解码过程中出现异常，处理错误情况 print(f\"JSON decode error: {e}\") except UnicodeDecodeError as e: # 在解码数据时出现编码错误 print(f\"Unicode decode error: {e}\") except Exception as e: # 其他异常 print(f\"Error: {e}\") else: print(\"请求失败，状态码:\", response.status) 协程对象运行 协程对象不能直接运行，必须放入事件循环中或者由 yield from 语句调用 将协程对象注入事件循环的时候，其实是 run_until_complete 方法将协程包装成了一个任务（task）对象，任务对象保存了协程运行后的状态，用于未来获取协程的结果 import time import asyncio def main(): start = time.time() @asyncio.coroutine def do_some_work(): print('Start coroutine') time.sleep(0.1) print('This is a coroutine') loop = asyncio.get_event_loop() coroutine = do_some_work() task = loop.create_task(coroutine) # 1 print('task 是不是 asyncio.Task 的实例?', isinstance(task, asyncio.Task)) # 2 print('Task state:', task._state) # 3 loop.run_until_complete(task) # 4 print('Task state:', task._state) end = time.time() print('运行耗时：{:.4f}'.format(end - start)) main() task 是不是 asyncio.Task 的实例? True Task state: PENDING Start coroutine This is a coroutine Task state: FINISHED 运行耗时：0.1052 代码说明： 事件循环的 create_task 方法可以创建任务，另外 asyncio.ensure_future 方法也可以创建任务，参数须为协程对象 task 是 asyncio.Task 类的实例，为什么要使用协程对象创建任务？ 因为在这个过程中 asyncio.Task 做了一些工作，包括预激协程、协程运行中遇到某些异常时的处理 task 对象的 _state 属性保存当前任务的运行状态，任务的运行状态有 PENDING 和 FINISHED 两种 将任务注入事件循环，阻塞运行 在 Python 3.5 中新增了 async / await 关键字用来定义协程函数 这两个关键字是一个组合，其作用等同于 asyncio.coroutine 装饰器和 yield from 语句。此后协程与生成器就彻底泾渭分明了 结果获取 通过result获取 import asyncio async def hello1(a, b): print(\"Hello world 01 begin\") await asyncio.sleep(3) # 模拟耗时任务3秒 print(\"Hello again 01 end\") return a + b coroutine = hello1(10, 5) loop = asyncio.get_event_loop() # 第一步：创建事件循环 task = asyncio.ensure_future(coroutine) # 第二步:将多个协程函数包装成任务列表 loop.run_until_complete(task) # 第三步：通过事件循环运行 print('-------------------------------------') print(task.result()) loop.close() '''运行结果为 Hello world 01 begin Hello again 01 end ------------------------------------- 15 ''' 回调绑定 有了 asyncio / await 关键字，我们继续学习 asyncio 模块的基本功能。 假如协程包含一个 IO 操作（这几乎是肯定的），等它处理完数据后，我们希望得到通知，以便下一步数据处理 这一需求可以通过向 future 对象中添加回调来实现 那么什么是 future 对象 task 对象就是 future 对象，我们可以这样认为，因为 asyncio.Task 是 asyncio.Future 的子类 也就是说，task 对象可以添加回调函数 回调函数的最后一个参数是 future 或 task 对象，通过该对象可以获取协程返回值 如果回调需要多个参数，可以通过偏函数导入 简言之，一个任务完成后需要捎带运行的代码可以放到回调函数中 修改上一个程序如下： In [64]: def three(): ...: start = time.time() ...: ...: # @asyncio.coroutine ...: async def corowork(): # 1 ...: print('[corowork] Start coroutine') ...: time.sleep(0.1) ...: print('[corowork] This is a coroutine') ...: ...: def callback(name, task): # 2 ...: print('[callback] Hello {}'.format(name)) ...: print('[callback] coroutine state: {}'.format(task._state)) ...: ...: loop = asyncio.get_event_loop() ...: coroutine = corowork() ...: task = loop.create_task(coroutine) ...: task.add_done_callback(functools.partial(callback, 'Shiyanlou')) # 3 ...: loop.run_until_complete(task) ...: ...: end = time.time() ...: print('运行耗时：{:.4f}'.format(end - start)) ...: In [65]: import functools In [66]: three() [corowork] Start coroutine [corowork] This is a coroutine [callback] Hello Shiyanlou [callback] coroutine state: FINISHED 运行耗时：0.1051 代码说明： 使用 async 关键字替代 asyncio.coroutine 装饰器创建协程函数 回调函数，协程终止后需要顺便运行的代码写入这里，回调函数的参数有要求，最后一个位置参数须为 task 对象 task 对象的 add_done_callback 方法可以添加回调函数，注意参数必须是回调函数，这个方法不能传入回调函数的参数，这一点需要通过 functools 模块的 partial 方法解决，将回调函数和其参数 name 作为 partial 方法的参数，此方法的返回值就是偏函数，偏函数可作为 task.add_done_callback 方法的参数 异步迭代器 什么是异步迭代器 实现了 __aiter__() 和 __anext__() 方法的对象。__anext__ 必须返回一个 awaitable 对象。async for 会处理异步迭代器的 __anext__() 方法所返回的可等待对象，直到其引发一个 StopAsyncIteration 异常。由 PEP 492 引入。 什么是异步可迭代对象 可在 async for 语句中被使用的对象。必须通过它的 __aiter__() 方法返回一个 asynchronous iterator。由 PEP 492 引入。 import asyncio class Reader(object): \"\"\" 自定义异步迭代器（同时也是异步可迭代对象） \"\"\" def __init__(self): self.count = 0 async def readline(self): # await asyncio.sleep(1) self.count += 1 if self.count == 100: return None return self.count def __aiter__(self): return self async def __anext__(self): val = await self.readline() if val == None: raise StopAsyncIteration return val async def func(): # 创建异步可迭代对象 async_iter = Reader() # async for 必须要放在async def函数内，否则语法错误。 async for item in async_iter: print(item) asyncio.run(func()) 异步迭代器其实没什么太大的作用，只是支持了async for语法而已 异步上下文管理器 此种对象通过定义 __aenter__() 和 __aexit__() 方法来对 async with 语句中的环境进行控制。由 PEP 492 引入 import asyncio class AsyncContextManager: def __init__(self): self.conn = None async def do_something(self): # 异步操作数据库 return 666 async def __aenter__(self): # 异步链接数据库 self.conn = await asyncio.sleep(1) return self async def __aexit__(self, exc_type, exc, tb): # 异步关闭数据库链接 await asyncio.sleep(1) async def func(): async with AsyncContextManager() as f: result = await f.do_something() print(result) asyncio.run(func()) 异步上下文管理器还是比较有用的，平时在开发过程中 打开、处理、关闭 操作时，就可以用这种方式来处理 进阶使用 uvloop uvloop是 asyncio 中的事件循环的替代方案，替换后可以使得asyncio性能提高 uvloop实现了asyncio.AbstractEventLoop接口，这意味着它提供了asyncio事件循环的直接替换 事实上，uvloop要比nodejs、gevent等其他python异步框架至少要快2倍，性能可以比肩Go语言 安装uvloop: pip3 install uvloop 在项目中想要使用uvloop替换asyncio的事件循环也非常简单，只要在代码中这么做就行 import asyncio import uvloop asyncio.set_event_loop_policy(uvloop.EventLoopPolicy()) # 编写asyncio的代码，与之前写的代码一致。 # 内部的事件循环自动化会变为uvloop asyncio.run(...) 注意：知名的asgi uvicorn内部就是使用的uvloop的事件循环 uvloop用Cython编写，并建立在libuv之上。 libuv是nodejs使用的高性能，多平台异步I/O库。由于nodejs的普及和流行，libuv既快速又稳定。 uvloop实现所有异步事件循环API。高级Python对象包装了低级libuv结构和函数。继承用于保持代码DRY并确保任何手动内存管理与libuv原语的寿命保持同步。 asyncio附带下列内置策略: class asyncio.``DefaultEventLoopPolicy 默认asyncio策略。在Unix和Windows平台上都使用 SelectorEventLoop 不需要手动安装默认策略，asyncio已配置成自动使用默认策略。 class asyncio.``WindowsProactorEventLoopPolicy 使用 ProactorEventLoop 事件循环实现的另一种事件循环策略，可用性: Windows 自定义策略 要实现一个新的事件循环策略，建议子类化 DefaultEventLoopPolicy 并重写需要定制行为的方法 class MyEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): \"\"\"Get the event loop. This may be None or an instance of EventLoop. \"\"\" loop = super().get_event_loop() # Do something with loop ... return loop asyncio.set_event_loop_policy(MyEventLoopPolicy()) 异步Redis 当通过python去操作redis时，链接、设置值、获取值 这些都涉及网络IO请求 使用asycio异步的方式可以在IO等待时去做一些其他任务，从而提升性能 安装Python异步操作redis模块：pip3 install aioredis 示例1：异步操作redis #!/usr/bin/env python # -*- coding:utf-8 -*- import asyncio import aioredis async def execute(address, password): print(\"开始执行\", address) # 网络IO操作：创建redis连接 redis = await aioredis.create_redis(address, password=password) # 网络IO操作：在redis中设置哈希值car，内部在设三个键值对，即： redis = { car:{key1:1,key2:2,key3:3}} await redis.hmset_dict('car', key1=1, key2=2, key3=3) # 网络IO操作：去redis中获取值 result = await redis.hgetall('car', encoding='utf-8') print(result) redis.close() # 网络IO操作：关闭redis连接 await redis.wait_closed() print(\"结束\", address) asyncio.run(execute('redis://47.93.4.198:6379', \"root!2345\")) 示例2：连接多个redis做操作（遇到IO会切换其他任务，提供了性能） import asyncio import aioredis async def execute(address, password): print(\"开始执行\", address) # 网络IO操作：先去连接 47.93.4.197:6379，遇到IO则自动切换任务，去连接47.93.4.198:6379 redis = await aioredis.create_redis_pool(address, password=password) # 网络IO操作：遇到IO会自动切换任务 await redis.hmset_dict('car', key1=1, key2=2, key3=3) # 网络IO操作：遇到IO会自动切换任务 result = await redis.hgetall('car', encoding='utf-8') print(result) redis.close() # 网络IO操作：遇到IO会自动切换任务 await redis.wait_closed() print(\"结束\", address) task_list = [ execute('redis://47.93.4.197:6379', \"root!2345\"), execute('redis://47.93.4.198:6379', \"root!2345\") ] asyncio.run(asyncio.wait(task_list)) 更多redis操作参考aioredis官网 异步MySQL 当通过python去操作MySQL时，连接、执行SQL、关闭都涉及网络IO请求 使用asycio异步的方式可以在IO等待时去做一些其他任务，从而提升性能 安装Python异步操作mysql模块：pip3 install aiomysql 示例1： import asyncio import aiomysql async def execute(): # 网络IO操作：连接MySQL conn = await aiomysql.connect(host='127.0.0.1', port=3306, user='root', password='123', db='mysql', ) # 网络IO操作：创建CURSOR cur = await conn.cursor() # 网络IO操作：执行SQL await cur.execute(\"SELECT Host,User FROM user\") # 网络IO操作：获取SQL结果 result = await cur.fetchall() print(result) # 网络IO操作：关闭链接 await cur.close() conn.close() asyncio.run(execute()) 示例2： #!/usr/bin/env python # -*- coding:utf-8 -*- import asyncio import aiomysql async def execute(host, password): print(\"开始\", host) # 网络IO操作：先去连接 47.93.40.197，遇到IO则自动切换任务，去连接47.93.40.198:6379 conn = await aiomysql.connect(host=host, port=3306, user='root', password=password, db='mysql') # 网络IO操作：遇到IO会自动切换任务 cur = await conn.cursor() # 网络IO操作：遇到IO会自动切换任务 await cur.execute(\"SELECT Host,User FROM user\") # 网络IO操作：遇到IO会自动切换任务 result = await cur.fetchall() print(result) # 网络IO操作：遇到IO会自动切换任务 await cur.close() conn.close() print(\"结束\", host) task_list = [ execute('47.93.40.197', \"root!2345\"), execute('47.93.40.197', \"root!2345\") ] asyncio.run(asyncio.wait(task_list)) FastAPI框架 FastAPI是一款用于构建API的高性能web框架，框架基于Python3.6+的 type hints搭建 接下里的异步示例以FastAPI和uvicorn来讲解（uvicorn是一个支持异步的asgi） 安装FastAPI web 框架：pip3 install fastapi 安装uvicorn：pip3 install uvicorn 本质上为web提供socket server的支持的asgi（一般支持异步称asgi、不支持异步称wsgi） 示例： #!/usr/bin/env python # -*- coding:utf-8 -*- import asyncio import uvicorn import aioredis from aioredis import Redis from fastapi import FastAPI app = FastAPI() REDIS_POOL = aioredis.ConnectionsPool('redis://47.193.14.198:6379', password=\"root123\", minsize=1, maxsize=10) @app.get(\"/\") def index(): \"\"\" 普通操作接口 \"\"\" return {\"message\": \"Hello World\"} @app.get(\"/red\") async def red(): \"\"\" 异步操作接口 \"\"\" print(\"请求来了\") await asyncio.sleep(3) # 连接池获取一个连接 conn = await REDIS_POOL.acquire() redis = Redis(conn) # 设置值 await redis.hmset_dict('car', key1=1, key2=2, key3=3) # 读取值 result = await redis.hgetall('car', encoding='utf-8') print(result) # 连接归还连接池 REDIS_POOL.release(conn) return result if __name__ == '__main__': uvicorn.run(\"luffy:app\", host=\"127.0.0.1\", port=5000, log_level=\"info\") 在有多个用户并发请求的情况下，异步方式来编写的接口可以在IO等待过程中去处理其他的请求，提供性能。 例如： 同时有两个用户并发来向接口 http://127.0.0.1:5000/red 发送请求，服务端只有一个线程，同一时刻只有一个请求被处理 异步处理可以提供并发是因为：当视图函数在处理第一个请求时，第二个请求此时是等待被处理的状态，当第一个请求遇到IO等待时，会自动切换去接收并处理第二个请求，当遇到IO时自动化切换至其他请求，一旦有请求IO执行完毕，则会再次回到指定请求向下继续执行其功能代码。 基于上下文管理，来实现自动化管理的案例 示例1：redis #!/usr/bin/env python # -*- coding:utf-8 -*- import asyncio import uvicorn import aioredis from aioredis import Redis from fastapi import FastAPI app = FastAPI() REDIS_POOL = aioredis.ConnectionsPool('redis://47.193.14.198:6379', password=\"root123\", minsize=1, maxsize=10) @app.get(\"/\") def index(): \"\"\" 普通操作接口 \"\"\" return {\"message\": \"Hello World\"} @app.get(\"/red\") async def red(): \"\"\" 异步操作接口 \"\"\" print(\"请求来了\") async with REDIS_POOL.get() as conn: redis = Redis(conn) # 设置值 await redis.hmset_dict('car', key1=1, key2=2, key3=3) # 读取值 result = await redis.hgetall('car', encoding='utf-8') print(result) return result if __name__ == '__main__': uvicorn.run(\"fast3:app\", host=\"127.0.0.1\", port=5000, log_level=\"info\") 示例2：mysql #!/usr/bin/env python # -*- coding:utf-8 -*- import asyncio import uvicorn from fastapi import FastAPI import aiomysql app = FastAPI() # 创建数据库连接池 pool = aiomysql.Pool(host='127.0.0.1', port=3306, user='root', password='123', db='mysql', minsize=1, maxsize=10, echo=False, pool_recycle=-1, loop=asyncio.get_event_loop()) @app.get(\"/red\") async def red(): \"\"\" 异步操作接口 \"\"\" # 去数据库连接池申请链接 async with pool.acquire() as conn: async with conn.cursor() as cur: # 网络IO操作：执行SQL await cur.execute(\"SELECT Host,User FROM user\") # 网络IO操作：获取SQL结果 result = await cur.fetchall() print(result) # 网络IO操作：关闭链接 return {\"result\": \"ok\"} if __name__ == '__main__': uvicorn.run(\"fast2:app\", host=\"127.0.0.1\", port=5000, log_level=\"info\") 爬虫 在编写爬虫应用时，需要通过网络IO去请求目标数据，这种情况适合使用异步编程来提升性能 接下来我们使用支持异步编程的aiohttp模块来实现 安装aiohttp模块: pip3 install aiohttp 示例： import aiohttp import asyncio async def fetch(session, url): print(\"发送请求：\", url) async with session.get(url, verify_ssl=False) as response: text = await response.text() print(\"得到结果：\", url, len(text)) async def main(): async with aiohttp.ClientSession() as session: url_list = [ 'https://python.org', 'https://www.baidu.com', 'https://www.pythonav.com' ] tasks = [asyncio.create_task(fetch(session, url)) for url in url_list] await asyncio.wait(tasks) if __name__ == '__main__': asyncio.run(main()) 多任务 实际项目中，往往有多个协程创建多个任务对象，同时在一个 loop 里运行 为了把多个协程交给 loop，需要借助 asyncio.gather 方法，任务的 result 方法可以获得对应的协程函数的 return 值 In [67]: def four(): ...: start = time.time() ...: ...: async def corowork(name, t): ...: print('[corowork] Start coroutine', name) ...: await asyncio.sleep(t) # 1 ...: print('[corowork] Stop coroutine', name) ...: return 'Coroutine {} OK'.format(name) # 2 ...: ...: loop = asyncio.get_event_loop() ...: coroutine1 = corowork('ONE', 3) # 3 ...: coroutine2 = corowork('TWO', 1) # 3 ...: task1 = loop.create_task(coroutine1) # 4 ...: task2 = loop.create_task(coroutine2) # 4 ...: gather = asyncio.gather(task1, task2) # 5 ...: loop.run_until_complete(gather) # 6 ...: print('[task1] ', task1.result()) # 7 ...: print('[task2] ', task2.result()) # 7 ...: ...: end = time.time() ...: print('运行耗时：{:.4f}'.format(end - start)) In [68]: four() [corowork] Start coroutine ONE [corowork] Start coroutine TWO [corowork] Stop coroutine TWO [corowork] Stop coroutine ONE [task1] Coroutine ONE OK [task2] Coroutine TWO OK 运行耗时：3.0070 代码说明： await 关键字等同于 Python 3.4 中的 yield from 语句，后面接协程对象。asyncio.sleep 方法的返回值为协程对象，这一步为阻塞运行。asyncio.sleep 与 time.sleep 是不同的，前者阻塞当前协程，即 corowork 函数的运行，而 time.sleep 会阻塞整个线程，所以这里必须用前者，阻塞当前协程，CPU 可以在线程内的其它协程中执行 协程函数的 return 值可以在协程运行结束后保存到对应的 task 对象的 result 方法中 创建两个协程对象，在协程内部分别阻塞 3 秒和 1 秒 创建两个任务对象 将任务对象作为参数，asyncio.gather 方法创建任务收集器。注意，asyncio.gather 方法中参数的顺序决定了协程的启动顺序 将任务收集器作为参数传入事件循环的 run_until_complete 方法，阻塞运行，直到全部任务完成 任务结束后，事件循环停止，打印任务的 result 方法返回值，即协程函数的 return 值 到这一步，大家应该可以看得出，上面的代码已经是异步编程的结构了，在事件循环内部，两个协程是交替运行完成的 简单叙述一下程序协程部分的运行过程： 首先运行 task1 打印 [corowork] Start coroutine ONE 遇到 asyncio.sleep 阻塞 释放 CPU 转到 task2 中执行 打印 [corowork] Start coroutine TWO 再次遇到 asyncio.sleep 阻塞 这次没有其它协程可以运行了，只能等阻塞结束 task2 的阻塞时间较短，阻塞 1 秒后先结束，打印 [corowork] Stop coroutine TWO 又过了 2 秒，阻塞 3 秒的 task1 也结束了阻塞，打印 [corowork] Stop coroutine ONE 至此两个任务全部完成，事件循环停止 打印两个任务的 result 打印程序运行时间 程序全部结束 需要额外说明的几点： 多数情况下无需调用 task 的 add_done_callback 方法，可以直接把回调函数中的代码写入 await 语句后面，协程是可以暂停和恢复的 多数情况下同样无需调用 task 的 result 方法获取协程函数的 return 值，因为事件循环的 run_until_complete 方法的返回值就是协程函数的 return 值。修改上文 # 6 、7 的代码如下： result = loop.run_until_complete(gather) print(result) 再次运行结果为： In [73]: four() [corowork] Start coroutine ONE [corowork] Start coroutine TWO [corowork] Stop coroutine TWO [corowork] Stop coroutine ONE ['Coroutine ONE OK', 'Coroutine TWO OK'] # 变量 result 的值 运行耗时：3.0045 事件循环有一个 stop 方法用来停止循环和一个 close 方法用来关闭循环。以上示例中都没有调用 loop.close 方法，似乎并没有什么问题。所以到底要不要调用 loop.close 呢？简单来说，loop 只要不关闭，就还可以再次运行 run_until_complete 方法，关闭后则不可运行。有人会建议调用 loop.close，彻底清理 loop 对象防止误用，其实多数情况下根本没有这个必要 asyncio 模块提供了 asyncio.gather 和 asyncio.wait 两个任务收集方法，它们的作用相同，都是将协程任务按顺序排定，再将返回值作为参数加入到事件循环中 前者在上文已经用到，后者与前者的区别是它可以获取任务的执行状态（PENING & FINISHED），当有一些特别的需求例如在某些情况下取消任务，可以使用 asyncio.wait 方法。 多任务实现方式 使用gather同时注册多个任务，实现并发 awaitable asyncio.gather(**aws*, *loop=None*, *return_exceptions=False*) 注意事项：gather的返回值是它所绑定的所有任务的执行结果，而且顺序是不变的，即返回的result的顺序和绑定的顺序是保持一致的 除此之外，它是awaitable的，所以，如果需要获取多个任务的返回值，既然是awaitable的，就需要将它放在一个函数里面，所以我们引入一个包装多个任务的入口main，这也是python3.7的思想 import asyncio import time async def hello1(a,b): print(\"Hello world 01 begin\") await asyncio.sleep(3) #模拟耗时任务3秒 print(\"Hello again 01 end\") return a+b async def hello2(a,b): print(\"Hello world 02 begin\") await asyncio.sleep(2) #模拟耗时任务2秒 print(\"Hello again 02 end\") return a-b async def hello3(a,b): print(\"Hello world 03 begin\") await asyncio.sleep(4) #模拟耗时任务4秒 print(\"Hello again 03 end\") return a*b async def main(): #封装多任务的入口函数 task1=asyncio.ensure_future(hello1(10,5)) task2=asyncio.ensure_future(hello2(10,5)) task3=asyncio.ensure_future(hello3(10,5)) results=await asyncio.gather(task1,task2,task3) for result in results: #通过迭代获取函数的结果，每一个元素就是相对应的任务的返回值，顺序都没变 print(result) loop = asyncio.get_event_loop() loop.run_until_complete(main()) loop.close() ''' 运行结果为： Hello world 01 begin Hello world 02 begin Hello world 03 begin Hello again 02 end Hello again 01 end Hello again 03 end 15 5 50 ''' 使用wait可以同时注册多个任务，实现并发 await asyncio.wait(aws, *, loop=None, timeout=None, return_when=ALL_COMPLETED) 它与gather不同的地方是他的参数是集合类型，而且他的返回类型是这样一个形式，即 (done, pending) 返回dones是已经完成的任务，pending是未完成的任务，都是集合类型，不同的是每一个元素不再是返回值，而是某一个task 相同的是它依然也是awaitable的，故而也需要定义在一个异步函数main()中，如下： #前面的代码和上面一样 async def main(): #封装多任务的入口函数 task1=asyncio.ensure_future(hello1(10,5)) task2=asyncio.ensure_future(hello2(10,5)) task3=asyncio.ensure_future(hello3(10,5)) done,pending=await asyncio.wait([task1,task2,task3]) for done_task in done: print(done_task.result()) #这里返回的是一个任务，不是直接的返回值，故而需要使用result函数进行获取 loop = asyncio.get_event_loop() loop.run_until_complete(main()) loop.close() #运行结果也一样 使用as_completed可以同时注册多个任务，实现并发 这个方法使用的比较少，与前面的两个gather和wait不同的是，它不是awaitable 主调方获取任务的运行结果 async def main(): #封装多任务的入口函数 task1=asyncio.ensure_future(hello1(10,5)) task2=asyncio.ensure_future(hello2(10,5)) task3=asyncio.ensure_future(hello3(10,5)) return await asyncio.gather(task1,task2,task3) #不在这里获取结果，只是返回 loop = asyncio.get_event_loop() results=loop.run_until_complete(main()) #在这里再获取返回函数值,然后迭代获取 for result in results: print(result) loop.close() #y运行结果同上 或者是如下： async def main(): #封装多任务的入口函数 task1=asyncio.ensure_future(hello1(10,5)) task2=asyncio.ensure_future(hello2(10,5)) task3=asyncio.ensure_future(hello3(10,5)) return await asyncio.wait([task1,task2,task3]) #不在这里获取结果，只是返回 loop = asyncio.get_event_loop() done,pending=loop.run_until_complete(main()) #在这里再获取返回函数值,然后迭代获取 for done_task in done: print(done_task.result()) loop.close() 协程锁 按照字面意思来看，asyncio.lock 应该叫做异步 IO 锁，之所以叫协程锁，是因为它通常使用在子协程中，其作用是将协程内部的一段代码锁住，直到这段代码运行完毕解锁 协程锁的固定用法是使用 async with 创建协程锁的上下文环境，将代码块写入其中 举例说明，将以下代码写入 async_lock.py 文件： import asyncio l = [] lock = asyncio.Lock() # 协程锁 async def work(name): print('lalalalalalalala') # 打印此信息是为了测试协程锁的控制范围 # 这里加个锁，第一次调用该协程，运行到这个语句块，上锁 # 当语句块结束后解锁，开锁前该语句块不可被运行第二次 # 如果上锁后有其它任务调用了这个协程函数，运行到这步会被阻塞，直至解锁 # with 是普通上下文管理器关键字，async with 是异步上下文管理器关键字 # 能够使用 with 关键字的对象须有 __enter__ 和 __exit__ 方法 # 能够使用 async with 关键字的对象须有 __aenter__ 和 __aexit__ 方法 # async with 会自动运行 lock 的 __aenter__ 方法，该方法会调用 acquire 方法上锁 # 在语句块结束时自动运行 __aexit__ 方法，该方法会调用 release 方法解锁 # 这和 with 一样，都是简化 try ... finally 语句 async with lock: print('{} start'.format(name)) # 头一次运行该协程时打印 if 'x' in l: # 如果判断成功 return name # 直接返回结束协程，不再向下执行 await asyncio.sleep(0); print('----------') # 阻塞 0 秒，切换协程 l.append('x') print('{} end'.format(name)) return name async def one(): name = await work('one') print('{} ok'.format(name)) async def two(): name = await work('two') print('{} ok'.format(name)) def main(): loop = asyncio.get_event_loop() tasks = asyncio.wait([one(), two()]) loop.run_until_complete(tasks) if __name__ == '__main__': main() 运行程序如下： $ python3 async_lock.py lalalalalalalala one start lalalalalalalala ---------- one end one ok two start two ok API索引 高级API索引 高级API索引 任务 运行异步程序，创建Task对象，等待多件事运行超时的公共集 run() 创建事件循环，运行一个协程，关闭事件循环。 create_task() 启动一个asyncio的Task对象。 await sleep() 休眠几秒。 await gather() 并发执行所有事件的调度和等待。 await wait_for() 有超时控制的运行。 await shield() 屏蔽取消操作 await wait() 完成情况的监控器 current_task() 返回当前Task对象 all_tasks() 返回事件循环中所有的task对象。 Task Task对象 run_coroutine_threadsafe() 从其他OS线程中调度一个协程。 for in as_completed() 用 for 循环监控完成情况。 例子 使用 asyncio.gather() 并行运行 使用 asyncio.wait_for() 强制超时 撤销协程 asyncio.sleep() 的用法 请主要参阅 协程与任务文档 同步 能被用于Task对象集的，类似线程的同步基元组件 Lock 互斥锁。 Event 事件对象。 Condition 条件对象 Semaphore 信号量 BoundedSemaphore 有界的信号量。 例子 asyncio.Event 的用法 请参阅asyncio文档 synchronization primitives 异常 asyncio.TimeoutError 类似 wait_for() 等函数在超时时候被引发。请注意 asyncio.TimeoutError 与内建异常 TimeoutError 无关。 asyncio.CancelledError 当一个Task对象被取消的时候被引发。请参阅 Task.cancel()。 例子 在取消请求发生的运行代码中如何处理CancelledError异常 请参阅完整的 asyncio 专用异常 列表 底层API索引 底层API索引 获取事件循环 asyncio.get_running_loop() 获取当前运行的事件循环 首选 函数。 asyncio.get_event_loop() 获得一个事件循环实例(当前或通过策略)。 asyncio.set_event_loop() 通过当前策略将事件循环设置当前事件循环。 asyncio.new_event_loop() 创建一个新的事件循环。 例子 使用asyncio.get_running_loop()。 生命周期 loop.run_until_complete() 运行一个期程/任务/可等待对象直到完成。 loop.run_forever() 一直运行事件循环。 loop.stop() 停止事件循环。 loop.close() 关闭事件循环。 loop.is_running() 返回 True ， 如果事件循环正在运行。 loop.is_closed() 返回 True ，如果事件循环已经被关闭 。 await loop.shutdown_asyncgens() 关闭异步生成器。 调试 loop.set_debug() 开启或禁用调试模式。 loop.get_debug() 获取当前测试模式。 调度回调函数 loop.call_soon() 尽快调用回调。 loop.call_soon_threadsafe() loop.call_soon() 方法线程安全的变体。 loop.call_later() 在给定时间 之后 调用回调函数。 loop.call_at() 在 指定 时间调用回调函数。 线程/进程池 await loop.run_in_executor() 在 concurrent.futures 执行器中运行一个独占CPU或其它阻塞函数。 loop.set_default_executor() 设置 loop.run_in_executor() 默认执行器。 任务与期程 loop.create_future() 创建一个 Future 对象。 loop.create_task() 将协程当作 Task 一样调度。 loop.set_task_factory() 设置 loop.create_task() 使用的工厂，它将用来创建 Tasks 。 loop.get_task_factory() 获取 loop.create_task() 使用的工厂，它用来创建 Tasks 。 错误处理 loop.call_exception_handler() 调用异常处理器。 loop.set_exception_handler() 设置一个新的异常处理器。 loop.get_exception_handler() 获取当前异常处理器。 loop.default_exception_handler() 默认异常处理器实现。 例子 使用 asyncio.get_event_loop() 和 loop.run_forever() 使用 loop.call_later() 使用 loop.create_connection() 实现 echo客户端 使用 loop.create_connection() 去 链接socket 使用add_reader()监听FD(文件描述符)的读取事件 使用loop.add_signal_handler() 使用loop.add_signal_handler() asyncio与gevent比较 Python中asyncio与gevent有什么区别? asycio 需要自己在代码中让出CPU，控制权在自己手上 gevent 用会替换标准库，你以为调用的是标准库的方法实际已经被替换成gevent自己的实现，遇到阻塞调用，gevent会自动让出CPU，像不像手动挡和自动挡的区别····· gevent是第三方库，通过greenlet实现协程，其基本思路是： 当一个greenlet遇到IO操作时，就自动切换到其他的greenlet，等到IO操作完成，再在适当的时候切换回来继续执行 asyncio是Python 3.4版本引入的标准库，直接内置了对异步IO的支持，不需要第三方的支持 编程模型比较 asyncio的编程模型就是一个消息循环。我们从asyncio模块中直接获取一个EventLoop的引用，然后把需要执行的协程扔到EventLoop中执行，就实现了异步IO。很多异步io操作这两个库都可以用，只是他们在不同场景下的效率和易用性可能有区别，当然这个得进行深入的测试和研究，单就现在普通的场景来说，区别并不大 gevent 是补丁，asyncio 是 python 3 原生；都能做到 异步 IO。如果现在写异步IO程序，应该用 asyncio gevent 需要 patch，个人感觉不洁癖，比如有些时候不需要patch，或者自己实现的东西和patch冲突就麻烦了 警告或错误提示 asyncio提前到编译阶段 event只能运行阶段 asyncio 系统自带，官方无忧 前期：async await 一入门思维有点绕，gevent程序员一开始比较习惯 后期：由于 async 和 await 关键字，asyncio 才是所见即所得！gevent入口去掉第一个spawn就是普通函数差不多了 处理blocking，asyncio 是 run_in_executor 好用，gevent 是 ThreadPool，不好用 启动 asyncio提供run_until_complete gevent 是 join（spawn） 同时启动多个 asyncio 用 asyncio.wait 包装 或 gather gevent 是 joinall 未来：官方既然出了 asyncio，感觉gevent 将会过时 web框架 【python 异步框架tornado】Tornado原理理解及应用场景 基础知识 python中常见的web框架主要包括了flask、django、tornado 那么这些漂亮的框架的应用场景是什么呢？ 要性能，Tornado 首选 要开发速度，Django 和Flask 都行 区别是： Flask 把许多功能交给第三方库去完成了，因此Flask 更为灵活 Django适合初学者或者小团队的快速开发，适合做管理类、博客类网站、功能复杂需求多的网站 Tornado适合高度定制，适合访问量大，异步情况多的网站。也可以用于定制api服务 WSGI协议 中国有三家有名的通信运营商，分别是移动、联通和电信，这三家通信商的手机号是可以跨平台拨打的，假设三家通信商负责通信的协议不同且无法互通，用移动的手机号就无法给联通电信的手机打电话，为了方便通信就需要一个统一的规范 WSGI协议的角色就是这个统一的规范，是描述webserver如何与web application通信的规范，要实现WSGI协议，就必须同时实现web server和webapplication，目前常见的有Tornado、Flask和Django WSGI是个同步模型，不支持非阻塞的请求方式，Tornado默认是不推荐使用WSGI的，如果在Tornado中使用WSGI，将无法使用Tornado的异步非阻塞的处理方式，相应的异步接口也就无法使用，性能方面也就大打折扣，这个也是Tornado性能如此优越的原因 django这类框架，采用WSGI协议与服务器对接的，而这类服务器通常是基于多线程/多进程的，也就是说每有一个网络请求，服务器都会有一个线程/进程进行处理 epoll技术 epoll是Linux内核为处理bai大批量文件描述du符而作了改zhi进的poll，是Linux下多路复用daoIO接口zhuanselect/poll的增强shu本，它能显著提高程序权在大量并发连接中只有少量活跃的情况下的系统CPU利用率。 另一点原因就是获取事件的时候，它无须遍历整个被侦听的描述符集，只要遍历那些被内核IO事件异步唤醒而加入Ready队列的描述符集合就行了。 epoll除了提供select/poll那种IO事件的水平触发（Level Triggered）外，还提供了边缘触发（Edge Triggered），这就使得用户空间程序有可能缓存IO状态，减少epoll_wait/epoll_pwait的调用，提高应用程序效率。 异步框架tornado 逻辑流程 【python 异步框架tornado】Tornado原理理解及应用场景 Epoll原理解析 分析下Torando.ioloop.IOLoop.current().start()代码上 Torando.ioloop是Tornado的核心模块ioloop模块 IOLoop是ioloop模块的一个类 current()是IOLoop类的一个方法，结果是返回一个当前线程的IOLoop的实例 start()也是IOLoop的方法，调用后开启循环 # -*- encoding=utf-8 -*- import tornado.ioloop import tornado.web class IndexHandler(tornado.web.RequestHandler): def get(self): self.write(\"Hello, world\") def make_app(): return tornado.web.Application([(r\"/\", IndexHandler),]) if __name__ == \"__main__\": app = make_app() app.listen(8888) tornado.ioloop.IOLoop.current().start() 整体的流程如下图所示： 首先Tornado需要建立监听，会创建一个socket用于监听，如果有客户端A请求建立连接之后，Tornado会基于原先的socket新创建一个包含客户端A连接的有关信息的socket(分配新的监听端口)，用于监听和客户端A的请求 此时对Tornado来说就有两个socket需要进行监控，原先的socket继续用来监听建立新连接，新的socket用于和客户端A进行通信，假如没有epoll技术的话，Tornado需要自己去循环询问哪个socket有新的请求 有了epoll技术，Tornado只需要把所有的socket丢给epoll，epoll作为管家帮忙监控，然后Torando.ioloop.IOLoop.current().start()开启循环，不断的去询问epoll是否有请求需要处理，这就是ioloop所做的工作，也是Tornado的核心部分 当有客户端进行请求，epoll就发现有socket可处理，当ioloop再次询问epoll时，epoll就把需要处理的socket交由Tornado处理 Tornado对请求进行处理，取出报文，从报文中获取请求路径，然后从tornado.web.Applcation里配置的路由映射中把请求路径映射成对应的处理类，如上图IndexHandler就是处理类 处理类处理完成后，生成响应，将响应内容封装成http报文，通过请求时建立的连接（尚未中断）将响应内容返回给客户端 当有多个请求同时发生，Tornado会按顺序挨个处理 看了上面的流程，假如Tornado在处理一个非常耗时的请求时，后面的请求是不是就会被卡死呢？ 答案是肯定的，所以提到了Tornado的另一个特性—异步处理，当一个请求特别耗时，Tornado就把它丢在那处理，然后继续处理下一个请求，确保后面的请求不会被卡死。 Tornado异步：原生Tornado框架提供异步网络库IOLoop和IOStream以及异步协程库tornado.gen(必须使用Tornado的web框架和HTTP服务器，否则异步接口可能无法使用)，方便用户通过更直接的方法实现异步编程，而不是回调的方式，官方推荐yield协程方式完成异步 通过上面所讲，基本上已经对Tornado的整个处理流程了解了，总结一下Tornado之所以能同时处理大量连接的原因： 利用高效的epoll技术处理请求，单线程/单进程同时处理大量连接 没使用传统的wsgi协议，而是利用Tornado自己的web框架和http服务形成了一整套WSGI方案进行处理 异步处理方式，Tornado提供了异步接口可供调用 对比其它框架 ornado 除了web框架之外，数据库或者其他几乎都是阻塞的 Tornado自身可以写出非阻塞的代码，但是连数据库，想用ORM的时候却不行，所以也不是特别方便。 因此很多人选择使用Flask或者是Django Django优缺： 重量级web框架，功能大而全，注重高效开发 内置管理后台 内置封装完善的ORM操作 session功能 后台管理 缺陷：高耦合 Tornado优缺： 轻量级web框架，功能少而精，注重性能优越 HTTP服务器 异步编程 WebSocket 缺陷：入门门槛较高 关键知识点 tornado的基础web框架模块RequestHandler封装了请求和响应。 Application核心应用类，类似于flask的app，是和服务器对接的接口，保存了路由信息 listen方法绑定端口创建http服务器实例，但并未监听，与socket的listen不同/ Tornado的优缺 优势 轻量级web框架 异步非阻塞IO处理方式 出色的抗负载能力 优异的处理性能，不依赖多进程/多线程，一定程度上解决C10K问题 WSGI全栈替代产品，推荐同时使用其web框架和HTTP服务器 分布式系统 分布式任务队列 huey 使用python的分布式任务队列huey实现任务的异步化 Celery 什么是celery框架 使用Celery踩过的坑 Celery 分布式任务队列入门 面试必问的CELERY，你了解多少？ 简介 celery是一个任务分发系统 目的：利用后端待命的无数worker实现一系列任务的快速处理 业务模式：生产者消费者模型 可以选择Broker: RabbitMQ Redis other brokers web框架 Python 四大主流 Web 编程框架 web三大主流框架 同步框架：Django、Flask；异步框架：Tornado、Sanic 关系型数据库：MySQL、PostgreSQL、Oracle；非关系型数据库：Redis、Mongo 企业级Django Django于2003年诞生于美国堪萨斯（Kansas）州，最初用来制作在线新闻Web站点，于2005年加入了BSD许可证家族，成为开源网络框架 Django根据比利时的爵士音乐家Django Reinhardt命名，作者这样命名Django意味着Django能优雅地演奏（开发）功能丰富的乐曲（Web应用） 它是当前Python世界里最负盛名且最成熟的网络框架 最初用来制作在线新闻的Web站点，目前已发展为应用最广泛的Python网络框架 Django的各模块之间结合得比较紧密，所以在功能强大的同时又是一个相对封闭的系统，但是其健全的在线文档及开发社区，使开发者在遇到问题时能找到解决方法。 框架特点 功能最完整：Django定义了服务发布、路由映射、模板编程、数据处理的一整套功能 模块间紧密耦合：开发者需要学习Django自己定义的这一整套技术 完善的文档：广泛的应用和完善的在线文档，开发者遇到问题时可以搜索在线文档寻求解决方案 集成数据访问组件：Django的Model层自带数据库ORM组件，无须学习其他数据库访问技术（dbi、SQLAlchemy等） 强大的URL映射技术：Django使用正则表达式管理URL映射，给开发者带来了极高的灵活性 后台管理系统自动生成：简单的几行配置和代码就可以实现完整的后台数据管理Web控制台 错误信息完整：提供非常完整的错误信息帮助开发者定位问题，比如缺少xxx组件的配置引用等 Django是遵循MVC架构的Web开发框架，其主要由以下几部分组成 管理工具（Management）：一套内置的创建站点、迁移数据、维护静态文件的命令工具 模型（Model）：提供数据访问接口和模块，包括数据字段、元数据、数据关系等的定义及操作 视图（View）：Django的视图层封装了HTTP Request和Response的一系列操作和数据流，其主要功能包括URL映射机制、绑定模板等 模板（Template）：是一套Django自己的页面渲染模板语言，用若干内置的tags和filters定义页面的生成方 表单（Form）：通过内置的数据类型和控件生成HTML表单 管理站（Admin）：通过声明需要管理的Model，快速生成后台数据管理网站 高并发Tornado Tornado是使用Python编写的一个强大的可扩展的Web服务器 它在处理高网络流量时表现得足够强健，却在创建和编写时有着足够的轻量级，并能够被用在大量的应用和工具中 Tornado作为FriendFeed网站的基础框架，于2009年9月10日发布，目前已经获得了很多社区的支持，并且在一系列不同的场合中得到应用 除FriendFeed和Facebook外，还有很多公司在生产上转向Tornado，包括Quora、Turntable.fm、Bit.ly、Hipmunk及MyYearbook等 相对于其他Python网络框架，Tornado有如下特点 完备的Web框架：与Django、Flask等一样，Tornado也提供了URL路由映射、Request上下文、基于模板的页面渲染技术等开发Web应用的必备工具 高效的网络库，性能与Twisted、Gevent等底层Python框架相媲美：**提供了异步I/O支持、超时事件处理。这使得Tornado除了可以作为Web应用服务器框架，还可以用来做爬虫应用、物联网关、游戏服务器等后台应用 高效HTTPClient：除了服务器端框架，Tornado还提供了基于异步框架的HTTP客户端 高效的内部HTTP服务器：虽然其他Python网络框架（Django、Flask）也提供了内部HTTP服务器，但它们的HTTP服务器由于性能原因只能用于测试环境。而Tornado的HTTP服务器与Tornado异步调用紧密结合，可以直接用于生产环境 完备的WebSocket支持：WebSocket是HTML5的一种新标准，实现了浏览器与服务器之间的双向实时通信 因为Tornado的上述特点，Tornado常被用作大型站点的接口服务框架，而不像Django那样着眼于建立完整的大型网站 主要关注Tornado的异步及协程编程、身份认证框架、独特的非WSGI部署方式 快速建站Flask Flask是Python Web框架族里比较年轻的一个，于2010年出现，这使得它吸收了其他框架的优点，并且把自己的主要领域定义在了微小项目上 同时，它是可扩展的，Flask让开发者自己选择用什么数据库插件存储他们的数据 很多功能简单但性能卓越的网站就是基于Flask框架而搭建的，比如http://httpbin.org/就是一个功能简单但性能强大的HTTP测试项目 Flask是一个面向简单需求和小型应用的微框架 相对于其他Python语言的Web框架而言，Flask的特点可以归结如下。 内置开发服务器和调试器： 网络程序调试是在将编制好的网站投入实际运行前，用手工或编译程序等方法进行测试，修正语法错误和逻辑错误的过程。有经验的开发者都知道，这是保证网站系统能够正式应用的必要步骤。 Flask 自带的开发服务器使开发者在调试程序时无须再安装其他任何网络服务器，比如Tomcat、JBoss、Apache等。Flask默认处于调试状态，使得运行中的任何错误会同时向两个目标发送信息：一个是Python Console，即启动Python程序的控制台；另一个是HTTP客户端，即Flask开发服务器将调试信息传递给了客户端。 与Python单元测试功能无缝衔接： 单元测试是对最小软件开发单元的测试，其重点测试程序的内部结构，主要采用白盒测试方法，由开发人员负责。单元测试的主要目标是保证函数在给定的输入状态下，能够得到预想的输出，在不符合要求时能够提醒开发人员进行检查。 Flask提供了一个与Python自带的单元测试框架unitest无缝衔接的测试接口，即Flask对象的test_client()函数。通过test_client()函数，测试程序可以模拟进行HTTP访问的客户端来调用Flask路由处理函数，并且获取函数的输出来进行自定义的验证。 使用Jinja2模板 将HTML页面与后台应用程序联系起来一直是网站程序框架的一个重要目标。Flask通过使用Jinja2模板技术解决了这个问题。Jinja2是一个非常灵活的HTML模板技术，它是从Django模板发展而来的，但是比Django模板使用起来更加自由且更加高效。Jinja2模板使用配制的语义系统，提供灵活的模板继承技术，自动抗击XSS跨站攻击并且易于调试。 完全兼容WSGI 1.0标准 WSGI（Web Server Gateway Interface）具有很强的伸缩性且能运行于多线程或多进程环境下，因为Python线程全局锁的存在，使得WSGI的这个特性至关重要。WSGI已经是Python界的一个主要标准，各种大型网路服务器对其都有良好的支持。WSGI位于Web应用程序与Web服务器之间，与WSGI完全兼容使得Flask能够配置到各种大型网络服务器中。 基于Unicode编码 Flask是完全基于Unicode的。这对制作非纯ASCII字符集的网站来说非常方便。HTTP本身是基于字节的，也就是说任何编码格式都可以在HTTP中传输。但是，HTTP要求在HTTP Head中显式地声明在本次传输中所应用的编码格式。在默认情况下，Flask会自动添加一个UTF-8编码格式的HTTP Head，使程序员无须担心编码的问题。 自定义协议Twisted 以上讲到的3个Python Web框架都是围绕着应用层HTTP展开的，而Twisted是一个例外 Twisted是一个用Python语言编写的事件驱动的网络框架 对于追求服务器程序性能的应用，Twisted框架是一个很好的选择 Twisted是一个有着10多年历史的开源事件驱动框架 wisted支持很多种协议，包括传输层的UDP、TCP、TLS，以及应用层的HTTP、FTP等 对于所有这些协议，Twisted提供了客户端和服务器方面的开发工具。 Twisted框架的历史悠久，其主要发行版本都以Python 2为基础，最新的版本为基于Python 2.7的Twisted-15.4.0 Twisted社区正在开发基于Python 3的版本，但目前为止尚没有基于Python 3的Twisted稳定发行版 Twisted是一个高性能的编程框架 在不同的操作系统平台上，Twisted利用不同的底层技术实现了高效能通信 在Windows中，Twisted的实现基于I/O完成端口（IOCP，Input/Output Completion Port）技术，它保证了底层高效地将I/O事件通知给框架及应用程序 在Linux中，Twisted的实现基于epoll技术，epoll是Linux下多路复用I/O接口select/poll的增强版本，它能显著提高程序在大量并发连接中只有少量活跃的情况下的系统CPU利用率 在开发方法上，Twisted引导程序员使用异步编程模型。Twisted提供了丰富的Defer、Threading等特性来支持异步编程。 消息队列 RabbitMQ Python消息队列（RabbitMQ） Redis Python学习之Redis交互详解 相关疑问 多线程存在意义 多线程存在意义 协程常见问题 很多个协程一起运行有创建新的线程吗？ 协程运行时，都是在一个线程中运行的，没有创建新的线程。如下 import asyncio import time import threading a=time.time() async def hello1(): print(f\"Hello world 01 begin,my thread is:{threading.currentThread()}\") await asyncio.sleep(3) print(\"Hello again 01 end\") async def hello2(): print(f\"Hello world 02 begin,my thread is:{threading.currentThread()}\") await asyncio.sleep(2) print(\"Hello again 02 end\") async def hello3(): print(f\"Hello world 03 begin,my thread is:{threading.currentThread()}\") await asyncio.sleep(1) print(\"Hello again 03 end\") loop = asyncio.get_event_loop() tasks = [hello1(), hello2(),hello3()] loop.run_until_complete(asyncio.wait(tasks)) loop.close() b=time.time() print('---------------------------------------') print(b-a) '''运行结果为： Hello world 03 begin,my thread is: Hello world 02 begin,my thread is: Hello world 01 begin,my thread is: Hello again 03 end Hello again 02 end Hello again 01 end --------------------------------------- 2.994506597518921 ''' 从上面那个可以看出，三个不同的协程函数都是在一个线程完成的。但是并不是意味着，多个协程函数只能在一个线程中执行，同样可以创建新的线程，其实我们完全可以在新的线程中重新创建一个事件循环，具体的实例参见后面。 线程一定效率更高吗？ 也不是绝对的，当然在一般情况下，异步方式的执行效率是更高的，就比如上面的三个函数，如果按照同步的方式执行，则一共需要6秒的时间，但是采用协程则只需要最长的那个时间3秒，这自然是提高了工作效率，那是不是一定会提高呢？也不一定，这与协程的调用方式是由密切关系的。如下所示： import asyncio import time import threading a=time.time() async def hello1(): print(f\"Hello world 01 begin,my thread is:{threading.currentThread()}\") await asyncio.sleep(3) print(\"Hello again 01 end\") async def hello2(): print(f\"Hello world 02 begin,my thread is:{threading.currentThread()}\") await asyncio.sleep(2) print(\"Hello again 02 end\") async def hello3(): print(f\"Hello world 03 begin,my thread is:{threading.currentThread()}\") await hello2() await hello1() print(\"Hello again 03 end\") loop = asyncio.get_event_loop() tasks = [hello3()] loop.run_until_complete(asyncio.wait(tasks)) loop.close() b=time.time() print('---------------------------------------') print(b-a) '''运行结果为： Hello world 03 begin,my thread is: Hello world 02 begin,my thread is: Hello again 02 end Hello world 01 begin,my thread is: Hello again 01 end Hello again 03 end --------------------------------------- 5.008373498916626 ''' 我们发现一个问题，上面执行的顺序完全不是异步执行，执行的时间也没有得到改善，究其原因，是因为上面是通过hello3去调用hello1和hello2的，这和同步调用的方式完全是一样的，即使我定义的都是异步方法，它既没有提高执行效率，还会有阻塞。 结论：在有很多个异步方式的时候，一定要尽量避免这种异步函数的直接调用，这和同步是没什么区别的，一定要通过事件循环loop，“让事件循环在各个异步函数之间不停游走”，这样才不会造成阻塞。 协程会不会有阻塞呢？ 异步方式依然会有阻塞的，当我们定义的很多个异步方法彼此之间有一来的时候，比如，我必须要等到函数1执行完毕，函数2需要用到函数1的返回值，如上面的例子2所示，就会造成阻塞，这也是异步编程的难点之一，如何合理配置这些资源，尽量减少函数之间的明确依赖，这是很重要的。 异步方法假死（freezing） import asyncio import time import threading #定义一个异步操作 async def hello1(a,b): print(f\"异步函数开始执行\") await asyncio.sleep(3) print(\"异步函数执行结束\") return a+b #在一个异步操作里面调用另一个异步操作 async def main(): c=await hello1(10,20) print(c) print(\"主函数执行\") loop = asyncio.get_event_loop() tasks = [main()] loop.run_until_complete(asyncio.wait(tasks)) loop.close() '''运行结果为： 异步函数开始执行（在此处要等待3秒） 异步函数执行结束 30 主函数执行 ''' 注意一个问题：我们前面所讲的例子中，没有出现等待，是因为各个异步方法之间是“完全并列”关系，彼此之间没有依赖，所以，我可以将所有的异步操作“gather”起来，然后通过事件循环，让事件循环在多个异步方法之间来回调用，永不停止，故而没有出现等待。 但是，现实中不可能所有的异步方法都是完全独立的，没有任何关系的，在上面的这个例子中，就是很好的说明，hello1是一个耗时任务，耗时大约3秒，main也是一个异步方法，但是main中需要用到hello1中的返回结果，所以他必须要等待hello1运行结束之后再才能继续执行，这就是为什么会得到上面结果的原因。这也再一次说明，异步依然是会有阻塞的。 我们也可以这样理解，因为我给事件循环只注册了一个异步方法，那就是main，当在main里面遇到了await，事件循环挂起，转而寻找其他的异步方法，但是由于只注册了一个异步方法给事件循环，他没有其他的方法可执行了，所以只能等待，让hello1执行完了，再继续执行。 multithreading+asyncio总结 import tkinter as tk # 导入 Tkinter 库 import time import asyncio import threading class Form: def __init__(self): self.root=tk.Tk() self.root.geometry('500x300') self.root.title('窗体程序') #设置窗口标题 self.button=tk.Button(self.root,text=\"开始计算\",command=self.change_form_state) self.label=tk.Label(master=self.root,text=\"等待计算结果\") self.button.pack() self.label.pack() self.root.mainloop() async def calculate(self): await asyncio.sleep(3) self.label[\"text\"]=300 def get_loop(self,loop): self.loop=loop asyncio.set_event_loop(self.loop) self.loop.run_forever() def change_form_state(self): coroutine1 = self.calculate() new_loop = asyncio.new_event_loop() #在当前线程下创建时间循环，（未启用），在start_loop里面启动它 t = threading.Thread(target=self.get_loop,args=(new_loop,)) #通过当前线程开启新的线程去启动事件循环 t.start() asyncio.run_coroutine_threadsafe(coroutine1,new_loop) #这几个是关键，代表在新线程中事件循环不断“游走”执行 if __name__=='__main__': form=Form() 定义需要异步执行的一系列操作，及一系列协程函数； 在主线程中定义一个新的线程，然后在新线程中产生一个新的事件循环； 在主线程中，通过asyncio.run_coroutine_threadsafe(coroutine,loop)这个方法 将一系列异步方法注册到新线程的loop里面去，这样就是新线程负责事件循环的执行。 asyncio.run_coroutine_threadsafe(coroutine，loop)的意思很简单，就是我在新线程中创建一个事件循环loop 然后在新线程的loop中不断不停的运行一个或者是多个coroutine Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-12-01 00:57:11 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/21.python性能优化模块.html":{"url":"chapters/21.python性能优化模块.html","title":"python性能优化模块","keywords":"","body":"系统监控模块psutil模块例子时间监控cProfile介绍cProfile模块内存监控tracemalloc介绍tracemalloc模块 系统监控模块 psutil模块 psutil模块文档 psutil是一个跨平台库(http://pythonhosted.org/psutil/)能够轻松实现获取系统运行的进程和系统利用率（包括CPU、内存、磁盘、网络等）信息。它主要用来做系统监控，性能分析，进程管理。它实现了同等命令行工具提供的功能，如ps、top、lsof、netstat、ifconfig、who、df、kill、free、nice、ionice、iostat、iotop、uptime、pidof、tty、taskset、pmap等。 一、 安装psutil 　　pip install psutil 二、 监控cpu信息 import psutil psutil.cpu_times() #获取cpu（逻辑cpu的平均）占用时间的详细信息 psutil.cpu_times(percpu=True) #获取每个cpu占用时间的详细信息 psutil.cpt_times().user #获取用户进程占用cpu的时间（user+sys+idle+wait=total） 三、 监控内存信息 import psutil psutil.virtual_memory() #获取内存信息 psutil.virtual_memory().total #获取内存总量 psutil.swap_memory() #获取swap信息 psutil.swqp_memory() #获取swap总量 四、 监控磁盘信息 import psutil psutil.disk_partitions() #获取各分区的信息 psutil.disk_usage() #获取各分区的使用情况 psutil.disk_io_counters(perdisk=True) #获取各个分区的io情况 psutil.disk_io_counters(perdisk=True)['sda1'].read_count #获取sda1的io读取情况 五、 监控网络信息 import psutil psutil.net_io_counters() #获取所有网络接口io信息 psutil.net_io_counters(pernic=True) #获取每个网络接口的io信息 六、进程信息 import psutil psutil.Process(pid) #查看对应pid的进程信息 psutil.Process(pid).username() #查看是哪个用户创建的该进程 psutil.Process(pid).cmdline() #查看进程所在的路径 七、 登录用户信息 import psutil psutil.users() #查看目前登录用户信息 例子 import psutil import time def get_sys_rc(): # 获得cpu信息：核心数量、使用率 cpu_count = psutil.cpu_count() print(cpu_count) cpu_usage = psutil.cpu_percent(1) # 间隔1秒钟统计一次使用率 print(cpu_usage) # 获得内存大小和使用率 print(\"#\" * 50) mem = psutil.virtual_memory() mem_total = mem.total / 1024 / 1024 mem_usage = mem.percent print(mem_total, mem_usage) # 获得磁盘信息和分区信息 disk_info = psutil.disk_partitions() # 得到所有的分区信息 print(disk_info) for i in disk_info: # 遍历所有的分区，得到分区的名字 print(i.device) # 输出设备名字 part_info = psutil.disk_usage(i.device) # 得到每个分区的使用率 print(part_info) # 输出使用率 # round() 四舍五入的方法 print(f\"{i.device}总大小为{round(part_info.total / 1024 / 1024 / 1024)}G,使用率{part_info.percent}\") # 获得网卡信息 net_info = psutil.net_io_counters() step1 = net_info.bytes_sent time.sleep(3) step2 = psutil.net_io_counters().bytes_sent avg = (step2 - step1) / 3 / 1000 print(f\"当前平均的网络流量是{round(avg)}KB\") # 获得所有网卡的ip地址 # psutil.net_if_stats() return {\"cpu_count\": cpu_count, \"cpu_usage\": cpu_usage} # return {\"cpu_count\":cpu_count,\"cpu_usage\":cpu_usage,mem_total,mem_usage,avg} if __name__ == \"__main__\": print(get_sys_rc()) output 8 42.8 ################################################## 32685.93359375 23.9 [sdiskpart(device='C:\\\\', mountpoint='C:\\\\', fstype='NTFS', opts='rw,fixed'), sdiskpart(device='D:\\\\', mountpoint='D:\\\\', fstype='NTFS', opts='rw,fixed'), sdiskpart(device='E:\\\\', mountpoint='E:\\\\', fstype='NTFS', opts='rw,fixed'), sdiskpart(device='H:\\\\', mountpoint='H:\\\\', fstype='NTFS', opts='rw,fixed'), sdiskpart(device='Q:\\\\', mountpoint='Q:\\\\', fstype='NTFS', opts='rw,fixed'), sdiskpart(device='S:\\\\', mountpoint='S:\\\\', fstype='NTFS', opts='rw,fixed'), sdiskpart(device='U:\\\\', mountpoint='U:\\\\', fstype='NTFS', opts='rw,fixed')] C:\\ sdiskusage(total=116955537408, used=76401147904, free=40554389504, percent=65.3) C:\\总大小为109G,使用率65.3 D:\\ sdiskusage(total=174902472704, used=128004796416, free=46897676288, percent=73.2) D:\\总大小为163G,使用率73.2 E:\\ sdiskusage(total=107373129728, used=24370171904, free=83002957824, percent=22.7) E:\\总大小为100G,使用率22.7 H:\\ sdiskusage(total=214747312128, used=140843151360, free=73904160768, percent=65.6) H:\\总大小为200G,使用率65.6 Q:\\ sdiskusage(total=214748360704, used=67760369664, free=146987991040, percent=31.6) Q:\\总大小为200G,使用率31.6 S:\\ sdiskusage(total=214748360704, used=79423479808, free=135324880896, percent=37.0) S:\\总大小为200G,使用率37.0 U:\\ sdiskusage(total=1073741819904, used=507053940736, free=566687879168, percent=47.2) U:\\总大小为1000G,使用率47.2 当前平均的网络流量是1KB {'cpu_count': 8, 'cpu_usage': 42.8} 时间监控 cProfile介绍 cProfile官方文档 cProfile自python2.5以来就是标准版Python解释器默认的性能分析器。 其他版本的python，比如PyPy里没有cProfile的。 cProfile是一种确定性分析器，只测量CPU时间，并不关心内存消耗和其他与内存相关联的信息。 cProfile模块 时间监控装饰器 #!/usr/bin/env Python # -- coding: utf-8 -- \"\"\" @version: v1.0 @author: huangyc @file: profiler_helper.py @Description: 时间性能监控类，主要监控执行状态、频率和时长 @time: 2021/7/17 15:51 \"\"\" from cProfile import Profile from functools import wraps from pstats import Stats class ProfilerHelper: @staticmethod def profiler_wrap(sort_by: str = 'cumulative', print_stats: bool = True, print_callers: bool = True): \"\"\" 行状态、频率和时长装饰器 :param sort_by: 排序规则 可选参数： 准则 含义 升序/降序排列 calls 调用次数 降序 cumulative 累计时间 降序 cumtime 累计时间 降序 file 文件 升序 filename 文件名 升序 module 模块名 升序 ncalls 调用总次数 降序 pcalls 原始调用书 降序 line 行号 升序 name 函数名 升序 nfl 函数名/文件名/行号组合 降序 stdname 标准名称 升序 time 函数内部运行时间 降序 tottime 函数内部运行总时间 降序 :param print_stats: Create a Stats object based on the current profile and print the results to stdout. :param print_callers: 打印受测函数和调用函数的关系 :return: \"\"\" def wrapper(func): @wraps(func) def inner_func(*args, **kwargs): profiler = Profile() res = profiler.runcall(func, *args, **kwargs) stats = Stats(profiler) # strip_dirs()：删除报告中所有函数文件名的路径信息，这个方法改变stats实例内部的顺序，任何运行该方法的实例都将随机排列项目的顺序。 # 如果两个项目是相同的，那么这两个项目就可以合并。 stats.strip_dirs() # sort_stats(*keys)：通过一系列条件依次对所有项目进行排序，从而调整stats对象 stats.sort_stats(sort_by) if print_stats: stats.print_stats() if print_callers: stats.print_callers() return res return inner_func return wrapper @ProfilerHelper.profiler_wrap() def fn(h, a=5, b=8): import re print(h, a, b) return re.compile(\"aaa|bbb\") if __name__ == '__main__': print(fn(464646, 8, b=8)) 内存监控 tracemalloc介绍 tracemalloc模块官方文档 tracemalloc模块是跟踪python分配的内存块的调试工具。它提供以下信息： 回溯对象的分配位置 每个文件名和每个行号的已分配内存块的统计信息：已分配内存块的总大小、数量和平均大小 计算两个快照之间的差异以检测内存泄漏 tracemalloc模块 内存监控装饰器 #!/usr/bin/env Python # -- coding: utf-8 -- \"\"\" @version: v1.0 @author: huangyc @file: tracemalloc_helper.py @Description: 内存监控类 @time: 2021/7/17 15:53 \"\"\" import tracemalloc from functools import wraps class TracemallocHelper: @staticmethod def tracemalloc_wrap(key_type: str = 'lineno', monitor_type: str = 'statistics', top_k: int = 10, filter_self: bool = True): \"\"\" 内存监控装饰器 :param key_type: 比较的字段，'traceback', 'filename', 'lineno' :param monitor_type: 监控类型，'statistics', 'change' :param top_k: 关注top_k的内存占用,默认关注前10个 :param filter_self: 是否过滤掉监控程序本身的消耗 :return: \"\"\" def wrapper(func): @wraps(func) def inner_func(*args, **kwargs): tracemalloc.start() # ... start your application ... res = func(*args, **kwargs) snapshot1 = tracemalloc.take_snapshot() # ... call the function leaking memory ... snapshot2 = tracemalloc.take_snapshot() if monitor_type == 'change': top_stats = snapshot2.compare_to(snapshot1, key_type) elif monitor_type == 'statistics': top_stats = snapshot2.statistics(key_type) else: top_stats = snapshot2.statistics(key_type) print(f\"[ Top {top_k} differences ]\") for idx, stat in enumerate(top_stats[:top_k]): if filter_self and 'tracemalloc.py' in str(stat): continue print('\u0003d' % idx, stat) return res return inner_func return wrapper @TracemallocHelper.tracemalloc_wrap() def fn(h, a=5, b=8): print(h, a, b) d = [dict(zip('xy', (5, 6))) for i in range(100000)] t = [tuple(zip('xy', (5, 6))) for i in range(100000)] return d, t if __name__ == '__main__': fn(68, 7, b=78) Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-12-01 00:57:11 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/22.python网络编程.html":{"url":"chapters/22.python网络编程.html","title":"python网络编程","keywords":"","body":"网络编程socketTCP通讯流程基于UDP的socket粘包问题粘包的解决方案自定义报头解决粘包urllib基本概述重要模块urlopenrequesthandlerexcepturl parse相关urlparseurlunparsurljoinurlencodecookieurllib3requests 网络编程 socket和urllib的关系 提供对多种不同类型套接字的低级访问，您可以使用这些套接字通过任何端口和协议进行通信。例如，您可以将其用于电子邮件、SSH、远程桌面等，也可以用于侦听端口（对于服务器）。几乎所有Python网络库，包括urllib，都以某种方式使用socket。在urllib专门用于套接字的特定用途，即HTTP（和可选的TLS）和FTP协议的客户端，通常（但不总是）使用端口80、443或21。 Python urllib、urllib2、urllib3用法及区别 urllib、urllib2是老版本，urllib3是新版本，requests是基于urllib3写的。其中urllib和urllib2是内置库 模块urllib和urllib2的功能差不多，简单来说urllib2是urllib的增强——urllib2更好一些，但是urllib中有urllib2中所没有的函数。对于简单的下载， urllib绰绰有余。 如果需要实现HTTP身份验证或Cookie亦或编写扩展来处理自己的协议，urllib2可能是更好的选择。在Python2.x中主要为urllib和urllib2，这两个标准库是不可相互替代的。但是在Python3.x中将urllib2合并到了urllib，这一点值得注意。 urllib支持设置编码的函数urllib.urlencode，在模拟登陆的时候经常需要传递经过post编码之后的参数，如果不想使用第三方库完成模拟登录，就必须使用到标准库中的urllib。urllib提供一些比较原始基础的方法而urllib2并没有，比如urllib中的urlencode方法用来GET查询字符串的产生。 urllib2比较有优势的地方在于urllib2.openurl中可以接受一个Request类的实例来设置Request参数，来修改/设置Header头从而达到控制HTTP Request的header部分的目的，也可以修改用户代理,设置cookie等，但urllib仅可以接受URL。如果你访问一个网站想更改User Agent(可以伪装你的浏览器)，你就需要使用urllib2。urllib2模块没有加入urllib.urlretrieve函数以及urllib.quote等一系列quote和unquote功能，这个时候就需要urllib的辅助。 socket 网络7层协议，4层，5层？理清容易混淆的几个概念 爬虫遇到 Socket，莫慌，肝就完了！ Python3中的SocketServer socket是应用层与TCP/IP协议族通信的中间软件抽象层，它是一组接口。 在设计模式中，Socket其实就是一个门面模式，它把复杂的TCP/IP协议族隐藏在Socket接口后面，对用户来说，一组简单的接口就是全部，让Socket去组织数据，以符合指定的协议。 Socket 被称为套接字，是对 TCP/IP 协议的封装，它是传输层和应用层间的抽象层 相比 HTTP 的短连接通信方式，Socket 可实现客户端和服务器的长连接通信 Fiddler、Charles 只能抓取应用层的数据，如果你想抓其他层，比如：网络层、传输层、数据链路层的数据，强烈建议使用：Wireshark 在标准的OIS模型中并没有规定说必须有socket层，也就是说不使用socket也能完成通讯，是的，的确如此 那为什么需要socket呢？一个字懒，程序员都是懒的 我们发现还没有开始实现应用程序逻辑，就需要花大把时间来实现各种协议，太费事了，就有人专门把协议中一堆复杂的事情进行了封装，于是socket就诞生了 有了socket以后，无需自己编写代码实现三次握手，四次挥手，ARP请求，打包数据等等，socket已经封装好了，只需要遵循socket的规定去编程，写出的程序自然就是遵循tcp/udp标准的 socket的发展 套接字起源于20世纪70年代加利福尼亚大学伯克利分校版本的 Unix，即人们所说的 BSD Unix 因此，有时人们也把套接字称为“伯克利套接字”或“BSD 套接字” 一开始,套接字被设计用在同 一台主机上多个应用程序之间的通讯，这也被称进程间通讯或 IPC 套接字有两种(或者称为有两个种族)，分别是基于文件型的和基于网络型的 基于文件类型的套接字家族：AF_UNIX unix一切皆文件，基于文件的套接字调用的就是底层的文件系统来取数据，两个套接字进程运行在同一机器，可以通过访问同一个文件系统间接完成通信 基于网络类型的套接字家族：AF_INET 还有AF_INET6被用于ipv6，还有一些其他的地址家族，不过，他们要么是只用于某个平台，要么就是已经被废弃，或者是很少被使用，或者是根本没有实现，所有地址家族中，AF_INET是使用最广泛的一个，python支持很多种地址家族，但是由于大部通讯都是网络通讯，所以大部分时候使用AF_INET socket示例 # 1.导入socket模块 import socket # 2.创建socket对象 函数定义如下 socket.socket(socket_family,socket_type,protocal=0) #socket_family 可以是 AF_UNIX 或 AF_INET。 #socket_type 可以是 SOCK_STREAM表示TCP协议 或 SOCK_DGRAM表示UDP协议。 #protocol 一般不填,默认值为 0。 # 2.1获取TCP 套接字 tcpSock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) # 或者 后面的参数都有默认值,可以不写,默认创建的是TCP协议socket tcpSock = socket.socket() # 2.2获取udp/ip套接字 udpSock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) 服务端套接字函数 服务端套接字函数 含义 s.bind() 绑定(主机,端口号)到套接字 s.listen() 开始TCP监听 s.accept() 被动接受TCP客户的连接,(阻塞式)等待连接的到来 客户端套接字函数 客户端套接字函数 含义 s.connect() 主动初始化TCP服务器连接 s.connect_ex() connect()函数的扩展版本,出错时返回出错码,而不是抛出异常 公共用途的套接字函数 公共用途的套接字函数 含义 s.recv() 接收TCP数据 s.send() 发送TCP数据(send在待发送数据量大于己端缓存区剩余空间时,数据丢失,不会发完) s.sendall() 发送完整的TCP数据(本质就是循环调用sendsendall在待发送数据量大于己端缓存区剩余空间时，数据不丢失，循环调用send直到发完) s.recvfrom() 接收UDP数据 s.sendto() 发送UDP数据 s.getpeername() 连接到当前套接字的远端的地址 s.getsockname() 当前套接字的地址 s.getsockopt() 返回指定套接字的参数 s.setsockopt() 设置指定套接字的参数 s.close() 关闭套接字 面向锁的套接字方法 面向锁的套接字方法 含义 s.setblocking() 设置套接字的阻塞与非阻塞模式 s.settimeout() 设置阻塞套接字操作的超时时间 s.gettimeout() 得到阻塞套接字操作的超时时间 TCP通讯流程 TCP的通讯流程与打电话的过程非常相似 买手机 == socket() 装进手机卡 == bind() 待机 == listen() 电话来了、接受通话 == accept() 听 == read() 说 == write() 挂电话 == close() TCP服务端 import socket ip_port = ('127.0.0.1', 9000) # 电话卡 BUFSIZE = 1024 # 收发消息的尺寸 s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) # 买手机 s.bind(ip_port) # 手机插卡 s.listen(5) # 手机待机 conn, addr = s.accept() # 手机接电话 print('接到来自%s的电话' % addr[0]) msg = conn.recv(BUFSIZE) # 听消息,听话 print(msg, type(msg)) conn.send(msg.upper()) # 发消息,说话 conn.close() # 挂电话 s.close() # 手机关机 TCP客户端 import socket ip_port = ('127.0.0.1', 9000) BUFSIZE = 1024 s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) s.connect_ex(ip_port) # 拨电话 s.send('linhaifeng nb'.encode('utf-8')) # 发消息,说话(只能发送字节类型) feedback = s.recv(BUFSIZE) # 收消息,听话 print(feedback.decode('utf-8')) s.close() # 挂电话 注意TCP中必须先启动服务器再启动客户端,否则客户端由于无法链接服务器,直接报错! 如上就完成了一个最基本的TCP通讯，但是建立是为了传输数据，二传输数据很多时候并不是一次性就传输完成了，需要多次收发过程，所以需要给代码加上循环 改进版服务器端 import socket ip_port=('127.0.0.1',8081)#电话卡 BUFSIZE=1024 s=socket.socket(socket.AF_INET,socket.SOCK_STREAM) #买手机 s.bind(ip_port) #手机插卡 s.listen(5) #手机待机 while True: #新增接收链接循环,可以不停的接电话 conn,addr=s.accept() #手机接电话 # print(conn) # print(addr) print('接到来自%s的电话' �dr[0]) while True: #新增通信循环,可以不断的通信,收发消息 msg=conn.recv(BUFSIZE) #听消息,听话 print(msg,type(msg)) conn.send(msg.upper()) #发消息,说话 conn.close() #挂电话 s.close() #手机关机 改进版客户端 import socket ip_port=('127.0.0.1',8081) BUFSIZE=1024 s=socket.socket(socket.AF_INET,socket.SOCK_STREAM) s.connect_ex(ip_port) #拨电话 while True: #新增通信循环,客户端可以不断发收消息 msg=input('>>: ').strip() if len(msg) == 0:continue s.send(msg.encode('utf-8')) #发消息,说话(只能发送字节类型) feedback=s.recv(BUFSIZE) #收消息,听话 print(feedback.decode('utf-8')) s.close() #挂电话 基于UDP的socket UDP通讯流程与对讲机非常类似(由于不需要建立连接所以省去TCP的listen()和accept()这两步) 买传呼机 == socket() 固定对讲频道 == bind() 收信号 == recvfrom() 发信号 == sendto() UDP服务器端 import socket ip_port=('127.0.0.1',9000) # 固定通讯频道 BUFSIZE=1024 #在TCP中socket的初始化参数可以省略, 因为默认创建的就是TCP协议的socket #而UDP则必须手动指定相关参数 udp_server_client=socket.socket(socket.AF_INET,socket.SOCK_DGRAM) # 买对讲机 udp_server_client.bind(ip_port) while True: msg,addr=udp_server_client.recvfrom(BUFSIZE) #收信息 print(msg,addr) udp_server_client.sendto(msg.upper(),addr) # 发信息 UDP客户端 import socket ip_port=('127.0.0.1',9000) #确定通讯频道 BUFSIZE=1024 udp_server_client=socket.socket(socket.AF_INET,socket.SOCK_DGRAM) # 买对讲机 while True: msg=input('>>: ').strip() if not msg:continue udp_server_client.sendto(msg.encode('utf-8'),ip_port) # 发消息 back_msg,addr=udp_server_client.recvfrom(BUFSIZE) #收消息 udp是无链接的，先启动哪一端都不会报错，即使对方地址根本不存在也不会报错，强制关闭任何一方也没有任何问 另外，由于无连接的特点，服务器不需要针对摸个客户端进行循环，只要循环的接收即可，谁发来的消息都可以被处理，基于这个特点我们可以编写一个UDP程序，实现多个客户端同时与服务器通讯 UDP聊天服务器 import socket ip_port=('127.0.0.1',8081) udp_server_sock=socket.socket(socket.AF_INET,socket.SOCK_DGRAM) #买手机 udp_server_sock.bind(ip_port) while True: qq_msg,addr=udp_server_sock.recvfrom(1024) print('来自[%s:%s]的一条消息:\\033[1;44m%s\\033[0m' %(addr[0],addr[1],qq_msg.decode('utf-8'))) back_msg=input('回复消息: ').strip() udp_server_sock.sendto(back_msg.encode('utf-8'),addr) UDP聊天客户端 import socket BUFSIZE = 1024 udp_client_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) qq_name_dic = { '狗哥': ('127.0.0.1', 8081), '天线宝宝': ('127.0.0.1', 8081), '巴拉巴拉小魔女': ('127.0.0.1', 8081), '王尼玛': ('127.0.0.1', 8081), } while True: qq_name = input('请选择聊天对象: ').strip() while True: msg = input('请输入消息,回车发送: ').strip() if msg == 'quit': break if not msg or not qq_name or qq_name not in qq_name_dic: continue udp_client_socket.sendto(msg.encode('utf-8'), qq_name_dic[qq_name]) back_msg, addr = udp_client_socket.recvfrom(BUFSIZE) print('来自[%s:%s]的一条消息:\\033[1;44m%s\\033[0m' % (addr[0], addr[1], back_msg.decode('utf-8'))) udp_client_socket.close() 粘包问题 粘包指的是数据与数据之间没有明确的分界线，导致不能正确读取数据！ 要理解粘包问题，需要先了解TCP协议传输数据时的具体流程，TCP协议也称之为流式协议(UDP称为数据报协议) 应用程序无法直接操作硬件，应用程序想要发送数据则必须将数据交给操作系统，而操作系统需要需要同时为所有应用程序提供数据传输服务，也就意味着，操作系统不可能立马就能将应用程序的数据发送出去，就需要为应用程序提供一个缓冲区，用于临时存放数据，具体流程如下： 发送方：当应用程序调用send函数时，应用程序会将数据从应用程序拷贝到操作系统缓存，再由操作系统从缓冲区读取数据并发送出去 接收方：对方计算机收到数据也是操作系统先收到，至于应用程序何时处理这些数据，操作系统并不清楚，所以同样需要将数据先存储到操作系统的缓冲区中，当应用程序调用recv时，实际上是从操作系统缓冲区中将数据拷贝到应用程序的过程 上述过程对于TCP与UDP都是相同的不同之处在于： UDP: UDP在收发数据时是基于数据包的，即一个包一个包的发送，包与包之间有着明确的分界，到达对方操作系统缓冲区后也是一个一个独立的数据包，接收方从操作系统缓冲区中将数据包拷贝到应用程序 这种方式存在的问题： 发送方发送的数据长度每个操作系统会有不同的限制，数据超过限制则无法发送 接收方接收数据时如果应用程序的提供的缓存容量小于数据包的长度将造成数据丢失，而缓冲区大小不可能无限大 TCP: 当我们需要传输较大的数据，或需要保证数据完整性时，最简单的方式就是使用TCP协议了 与UDP不同的是，TCP增加了一套校验规则来保证数据的完整性，会将超过TCP包最大长度的数据拆分为多个TCP包 并在传输数据时为每一个TCP数据包指定一个顺序号，接收方在收到TCP数据包后按照顺序将数据包进行重组，重组后的数据全都是二进制数据，且每次收到的二进制数据之间没有明显的分界 基于这种工作机制TCP在三种情况下会发送粘包问题 当单个数据包较小时接收方可能一次性读取了多个包的数据 当整体数据较大时接收方可能一次仅读取了一个包的一部分内容 另外TCP协议为了提高效率，增加了一种优化机制，会将数据较小且发送间隔较短的数据合并发送，该机制也会导致发送方将两个数据包粘在一起发送 粘包的解决方案 解决方案：在发送数据前先发送数据长度 上述方案看起来解决了粘包问题，但是由于negle优化机制的存在，长度信息和数据还是有可能会粘包，而接受方并不知道长度信息具体几个字节，所以现在的问题是如何能够长度信息做成一个固定长度的bytes数据 我们可以将字符串拼接为一个固定长度的字符 但是这样太麻烦，struct模块为我们提供了一个功能，可以将整数类型转换为固定长度的bytes，此时就派上用场了 自定义报头解决粘包 上述方案已经完美解决了粘包问题，但是扩展性不高，例如我们要实现文件上传下载，不光要传输文件数据，还需要传输文件名字，md5值等等，如何能实现呢，具体思路： 发送端： 先将所有的额外信息打包到一个头中 然后先发送头部数据 最后发送真实数据 接收端： 接收固定长度的头部长度数据 根据长度数据获取头部数据 根据头部数据获取真实数据 客户端： import socket import struct import json c = socket.socket() c.connect((\"127.0.0.1\",8888)) while True: cmd = input(\">>>:\").strip() c.send(cmd.encode(\"utf-8\")) # 头部数据 data = c.recv(4) head_length = struct.unpack(\"i\",data)[0] head_data = c.recv(head_length).decode(\"utf-8\") head = json.loads(head_data) print(head) # 真实数据长度 data_length = head[\"data_size\"] #接收真实数据 size = 0 res = b\"\" while size 服务器： import socket import subprocess import struct import json server = socket.socket() server.bind((\"127.0.0.1\",8888)) server.listen() while True: client, addr = server.accept() while True: cmd = client.recv(1024).decode(\"utf-8\") p = subprocess.Popen(cmd,shell=True,stdout=-1,stderr=-1) # 真实数据 data = p.stdout.read() + p.stderr.read() # 头部数据 head = {\"data_size\":len(data),\"额外信息\":\"额外的值\"} head_data = json.dumps(head).encode(\"utf-8\") #头部长度 head_len = struct.pack(\"i\",len(head_data)) #逐个发送 client.send(head_len) client.send(head_data) client.send(data) urllib urllib—URL handling modules python爬虫从入门到放弃（三）之 Urllib库的基本使用 基本概述 urllib是python内置的HTTP请求库，包括以下模块 urllib.request: 最基本的HTTP请求模块，用来发起请求，就和人们在浏览器上输入网址来访问网页一样 urllib.error: 异常处理模块，如果在请求时出现错误，用这个模块来抓住异常，保证程序不会因为抛出异常而挂掉 urllib.parse url: 工具模块，提供了许多URL处理方法，比如URL的拆分、合并等等 urllib.robotparser: 用来识别目标网站的robot.txt文件(基本用不上) 重要模块 urlopen 关于urllib.request.urlopen参数的介绍： urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None) 请求 urlopen一般常用的有三个参数，它的参数如下：urllib.requeset.urlopen(url,data,timeout) import urllib.request # Get请求 response = urllib.request.urlopen('http://www.baidu.com') # response.read()可以获取到网页的内容 print(response.read().decode('utf-8')) # Post请求 import urllib.parse import urllib.request # 通过bytes(urllib.parse.urlencode())可以将post数据进行转换放到urllib.request.urlopen的data参数中。这样就完成了一次post请求 data = bytes(urllib.parse.urlencode({'word': 'hello'}), encoding='utf8') print(data) response = urllib.request.urlopen('http://httpbin.org/post', data=data) print(response.read()) # Timeout参数 # 在某些网络情况不好或者服务器端异常的情况会出现请求慢的情况，或者请求异常 # 所以这个时候我们需要给请求设置一个超时时间，而不是让程序一直在等待结果 response = urllib.request.urlopen('http://httpbin.org/get', timeout=1) print(response.read()) 如果我们添加data参数的时候就是以post请求方式请求，如果没有data参数就是get请求方式 响应 通过response.status、response.getheaders().response.getheader(\"server\")，获取状态码以及头部信息response.read()获得的是响应体的内容 print(response.status) 200 print(response.getheaders()) [('Connection', 'close'), ('Content-Length', '49589'), ('Server', 'nginx'), ('Content-Type', 'text/html; charset=utf-8'), ('X-Frame-Options', 'DENY'), ('Via', '1.1 vegur, 1.1 varnish, 1.1 varnish'), ('Accept-Ranges', 'bytes'), ('Date', 'Wed, 18 Aug 2021 07:37:14 GMT'), ('Age', '1496'), ('X-Served-By', 'cache-bwi5120-BWI, cache-nrt18321-NRT'), ('X-Cache', 'HIT, HIT'), ('X-Cache-Hits', '2, 1285'), ('X-Timer', 'S1629272234.003034,VS0,VE0'), ('Vary', 'Cookie'), ('Strict-Transport-Security', 'max-age=63072000; includeSubDomains')] print(response.getheader(\"server\")) nginx print(response.read()) 响应体的内容 request 上述的urlopen只能用于一些简单的请求，因为它无法添加一些header信息，如果后面写爬虫我们可以知道，很多情况下我们是需要添加头部信息去访问目标站的，这个时候就用到了urllib.request urllib中，request这个模块主要负责构造和发起网络请求，并在其中加入Headers、Proxy等。利用它可以模拟浏览器的一个请求发起过程 设置Headers 有很多网站为了防止程序爬虫爬网站造成网站瘫痪，会需要携带一些headers头部信息才能访问 最长见的有user-agent参数 from urllib import request, parse # 方式一 url = 'http://httpbin.org/post' headers = { 'User-Agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)', 'Host': 'httpbin.org' } dict = {'name': 'hyc'} data = bytes(parse.urlencode(dict), encoding='utf8') req = request.Request(url=url, data=data, headers=headers, method='POST') response = request.urlopen(req) # 方式二 data = bytes(parse.urlencode(dict), encoding='utf8') req = request.Request(url=url, data=data, method='POST') # 好处是自己可以定义一个请求头字典，然后循环进行添加 req.add_header('User-Agent', 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)') response = request.urlopen(req) urlretrieve下载文件 urlretrieve()方法直接将远程数据下载到本地。 urlretrieve(url, filename=None, reporthook=None, data=None) url：下载链接地址 filename：指定了保存本地路径(如果参数未指定，urllib会生成一个临时文件保存数据) reporthook：是一个回调函数，当连接上服务器、以及相应的数据块传输完毕时会触发该回调，我们可以利用这个回调函数来显示当前的下载进度 data：指post导服务器的数据，该方法返回一个包含两个元素的(filename, headers) 元组，filename 表示保存到本地的路径，header表示服务器的响应头 import os from six.moves import urllib import sys DATA_URL = 'http://www.python.org/ftp/python/2.7.5/Python-2.7.5.tar.bz2' filename = DATA_URL.split('/')[-1] def _progress(block_num, block_size, total_size): '''回调函数 @block_num: 已经下载的数据块 @block_size: 数据块的大小 @total_size: 远程文件的大小 ''' sys.stdout.write('\\r>> Downloading %s %.1f%%' % (filename, float(block_num * block_size) / float(total_size) * 100.0)) sys.stdout.flush() filepath, _ = urllib.request.urlretrieve(DATA_URL, filename, _progress) handler ProxyHandler 通过rulllib.request.ProxyHandler()可以设置代理,网站它会检测某一段时间某个IP 的访问次数，如果访问次数过多，它会禁止你的访问,所以这个时候需要通过设置代理来爬取数据 import urllib.request proxy_handler = urllib.request.ProxyHandler({ 'http': 'http://127.0.0.1:9743', 'https': 'https://127.0.0.1:9743' }) opener = urllib.request.build_opener(proxy_handler) response = opener.open('http://httpbin.org/get') print(response.read()) HTTPCookiProcessor cookie中保存中我们常见的登录信息，有时候爬取网站需要携带cookie信息访问,这里用到了http.cookijar，用于获取cookie以及存储cookie import http.cookiejar, urllib.request # 方式一：直接打印 cookie = http.cookiejar.CookieJar() handler = urllib.request.HTTPCookieProcessor(cookie) opener = urllib.request.build_opener(handler) response = opener.open('http://www.baidu.com') for item in cookie: print(item.name+\"=\"+item.value) # 方式二：MozillaCookieJar()方式 import http.cookiejar, urllib.request filename = \"cookie.txt\" cookie = http.cookiejar.MozillaCookieJar(filename) handler = urllib.request.HTTPCookieProcessor(cookie) opener = urllib.request.build_opener(handler) response = opener.open('http://www.baidu.com') cookie.save(ignore_discard=True, ignore_expires=True) # 方式三：LWPCookieJar()方式 import http.cookiejar, urllib.request filename = 'cookie.txt' cookie = http.cookiejar.LWPCookieJar(filename) handler = urllib.request.HTTPCookieProcessor(cookie) opener = urllib.request.build_opener(handler) response = opener.open('http://www.baidu.com') cookie.save(ignore_discard=True, ignore_expires=True) # 读取cookie import http.cookiejar, urllib.request cookie = http.cookiejar.LWPCookieJar() cookie.load('cookie.txt', ignore_discard=True, ignore_expires=True) handler = urllib.request.HTTPCookieProcessor(cookie) opener = urllib.request.build_opener(handler) response = opener.open('http://www.baidu.com') print(response.read().decode('utf-8')) except urllib库实现了三个异常类： URLError里只有一个属性：reason,即抓异常的时候只能打印错误信息 HTTPError里有三个属性：code,reason,headers ContentTooShortError里有三个属性：reason,content from urllib import request,error try: response = request.urlopen(\"http://pythonsite.com/1111.html\") except error.HTTPError as e: print(e.reason) print(e.code) print(e.headers) except error.URLError as e: print(e.reason) # e.reason其实也可以在做深入的判断 if isinstance(e.reason,socket.timeout): print(\"time out\") else: print(\"reqeust successfully\") url parse相关 urlparse # urllib.parse.urlparse(urlstring, scheme='', allow_fragments=True) from urllib.parse import urlparse result = urlparse(\"http://www.baidu.com/index.html;user?id=5#comment\") print(result) urlunpars 功能和urlparse的功能相反，它是用于拼接 from urllib.parse import urlunparse data = ['http','www.baidu.com','index.html','user','a=123','commit'] print(urlunparse(data)) urljoin 这个的功能其实是做拼接的 from urllib.parse import urljoin print(urljoin('http://www.baidu.com', 'FAQ.html')) print(urljoin('http://www.baidu.com', 'https://pythonsite.com/FAQ.html')) print(urljoin('http://www.baidu.com/about.html', 'https://pythonsite.com/FAQ.html')) print(urljoin('http://www.baidu.com/about.html', 'https://pythonsite.com/FAQ.html?question=2')) print(urljoin('http://www.baidu.com?wd=abc', 'https://pythonsite.com/index.php')) print(urljoin('http://www.baidu.com', '?category=2#comment')) print(urljoin('www.baidu.com', '?category=2#comment')) print(urljoin('www.baidu.com#comment', '?category=2')) # 从拼接的结果我们可以看出，拼接的时候后面的优先级高于前面的url http://www.baidu.com/FAQ.html https://pythonsite.com/FAQ.html https://pythonsite.com/FAQ.html https://pythonsite.com/FAQ.html?question=2 https://pythonsite.com/index.php http://www.baidu.com?category=2#comment www.baidu.com?category=2#comment www.baidu.com?category=2 urlencode 这个方法可以将字典转换为url参数 from urllib.parse import urlencode params = {\"name\":\"hyc\", \"age\":28} base_url = \"http://www.baidu.com?\" url = base_url+urlencode(params) print(url) cookie 登录后，得到cookie，带着这个cookie可以访问登录后的内容 #!/usr/bin/env Python # -- coding: utf-8 -- \"\"\" @version: v1.0 @author: huangyc @file: cookie_test.py @Description: @time: 2022/11/3 11:34 \"\"\" import urllib.parse import urllib.parse import urllib.request import urllib.request from http import cookiejar def cookie_test(): # 定义请求头 headers = {\"content-type\": \"application/x-www-form-urlencoded\", \"Users-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36\"} # 登录的网址 login_url = \"https://xx.hycbook.com/xx.php\" # 登录的参数 login_form_data = {\"fm_usr\": \"xxx\", \"fm_pwd\": \"xxx\"} # 使用urllib.request时 post方法所携带的参数不能是字典形式 login_form_data_final = urllib.parse.urlencode(login_form_data) # 发送登录请求POST, 自动保存cookie cook_jar = cookiejar.MozillaCookieJar() # 定义有添加 cookie功能的处理器 cook_handler = urllib.request.HTTPCookieProcessor(cook_jar) # 根据处理器生成opener opener = urllib.request.build_opener(cook_handler) # 此时headers里面还没有cookie, 还没有登录 login_request = urllib.request.Request(login_url, headers=headers, data=login_form_data_final.encode(\"utf-8\")) # 如果登录成功. cookjar自动保存cookie, opener里面有cookjar, 所以opener里有cookie resp = opener.open(login_request) res_text = resp.read().decode('utf-8') cook_jar.save(\"cookie.txt\") # 创建管理器 cookie_handler = urllib.request.HTTPCookieProcessor(cook_jar) http_handler = urllib.request.HTTPHandler() https_handler = urllib.request.HTTPSHandler() # 创建请求求管理器, 此处将登录过的cookie带进去 opener = urllib.request.build_opener(cookie_handler, http_handler, https_handler) look_url = f\"https://xx.hycbook.com\" req = urllib.request.Request(look_url) # 发起请求 response = opener.open(req) print(\"结束\") urllib3 requests Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-12-01 00:57:11 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/5.python装饰器.html":{"url":"chapters/5.python装饰器.html","title":"python装饰器","summary":"python装饰器学习以及函数式编程的库介绍","keywords":"","body":"函数基础函数定义函数和方法可调用对象函数内省参数传递args、*kwargs用法args、*kwargs用法有限个参数可变参数函数注解函数式编程包operator模块reduceitemgetterattrgettermethodcaller其他模块functools模块partialpartialmethodtotal_orderingcmp_to_keylru_cachesingledispatchwrapsupdate_wrapper装饰器(Dercrator)globals关键字闭包global关键字自由变量nonlocal声明函数装饰函数functools.wrap装饰器带参数的装饰器(3层)带有不定参数的装饰器多个装饰器函数装饰类类装饰函数类装饰类内置装饰器命令行神器Click@property和@classmethodfunctools模块的内置装饰器柯里化案例时间装饰器 函数基础 函数定义 在 Python 中，函数是一等对象，编程语言理论家把“一等对象”定义为满足下述条件的程序实体： 在运行时创建 能赋值给变量或数据结构中的元素 能作为参数传给函数 能作为函数的返回结果 有了一等函数，就可以使用函数式风格编程。 函数式编程的特点之一是使用高阶函数——接受函数为参数，或者把函数作为结果返回的函数是高阶函数（higher-order function）。 在函数式编程范式中，最为人熟知的高阶函数有 map、filter、reduce 和 apply。 在 Python 3 中，map 和 filter 还是内置函数，但是由于引入了列表推导和生成器表达式，它们变得没那么重要了。 sum 和 reduce 的通用思想是把某个操作连续应用到序列的元素上，累计之前的结果，把一系列值归约成一个值。 all 和 any 也是内置的归约函数。 all(iterable): 如果 iterable 的每个元素都是真值，返回 True；all([]) 返回 True。 any(iterable): 只要 iterable 中有元素是真值，就返回 True；any([]) 返回 False。 lambda 关键字在 Python 表达式内创建匿名函数。 lambda 函数的定义体中不能赋值，也不能使用 while 和 try 等 Python 语句。 lambda 句法只是语法糖：与 def 语句一样，lambda 表达式会创建函数对象。 函数和方法 从几个维度来介绍下python中函数和方法的区别： 分类角度分析 (1) 函数的分类： 内置函数：python内嵌的一些函数。 匿名函数：一行代码实现一个函数功能。 递归函数 自定义函数：根据自己的需求，来进行定义函数。 (2) 方法的分类： 普通方法：直接用self调用的方法。 私有方法：__函数名，只能在类中被调用的方法。 属性方法：@property，将方法伪装成为属性，让代码看起来更合理。 特殊方法(双下划线方法)：以__init__为例，是用来封装实例化对象的属性，只要是实例化对象就一定会执行__init__方法，如果对象子类中没有则会寻找父类（超类），如果父类（超类）也没有，则直接继承object(python 3.x)类，执行类中的__init__方法。 类方法：通过类名的调用去操作公共模板中的属性和方法。 静态方法：不用传入类空间、对象的方法， 作用是保证代码的一致性，规范性，可以完全独立类外的一个方法，但是为了代码的一致性统一的放到某个模块(py文件)中。 作用域角度分析 (1) 函数作用域：从函数调用开始至函数执行完成，返回给调用者后，在执行过程中开辟的空间会自动释放，也就是说函数执行完成后，函数体内部通过赋值等方式修改变量的值不会保留，会随着返回给调用者后，开辟的空间会自动释放。 (2) 方法作用域：通过实例化的对象进行方法的调用，调用后开辟的空间不会释放，也就是说调用方法中对变量的修改值会一直保留。 调用方式分析 (1) 函数：通过函数名()的方式进行调用。 (2) 方法：通过对象.方法名()的方式进行调用。 class Foo(object): def func(self): pass #实例化 obj = Foo() # 执行方式一:调用的func是方法 obj.func() #func 方法 # 执行方式二：调用的func是函数 Foo.func(123) # 函数 可调用对象 除了用户定义的函数，调用运算符（即 ()）还可以应用到其他对象上。如果想判断对象能否调用，可以使用内置的 callable() 函数。 如果类定义了 __call__ 方法，那么它的实例可以作为函数调用。 判断对象能否调用，最安全的方法是使用内置的 callable() 函数 任何 Python 对象都可以表现得像函数。为此，只需实现实例方法 __call__。 函数内省 除了 __doc__，函数对象还有很多属性。使用 dir 函数可以探知 factorial 具有下述属性： >>> dir(factorial) ['__annotations__', '__call__', '__class__', '__closure__', '__code__', '__defaults__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__','__format__', '__ge__', '__get__', '__getattribute__', '__globals__', '__gt__', '__hash__', '__init__', '__kwdefaults__', '__le__', '__lt__', '__module__', '__name__', '__ne__', '__new__', '__qualname__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__'] >>> 用户定义的函数的属性 名称 类型 说明 __annotations__ dict 参数和返回值的注解 __call__ method-wrapper 实现 () 运算符；即可调用对象协议 __closure__ tuple 函数闭包，即自由变量的绑定（通常是 None） __code__ code 编译成字节码的函数元数据和函数定义体 __defaults__ tuple 形式参数的默认值 __get__ method-wrapper 实现只读描述符协议（参见第 20 章） __globals__ dict 函数所在模块中的全局变量 __kwdefaults__ dict 仅限关键字形式参数的默认值 __name__ str 函数名称 __qualname__ str 函数的限定名称，如 Random.choice（ 参阅PEP 3155） __defaults__、__code__ 和 __annotations__ 属性，IDE 和框架使用它们提取关于函数签名的信息。 参数传递 args、*kwargs用法 # *args是用来发送一个非键值对的可变数量的参数列表给一个函数 def test_var_args(f_arg, *argv): print(\"first normal arg:\", f_arg) for arg in argv: print(\"another arg through *argv:\", arg) test_var_args('yasoob', 'python', 'eggs', 'test') first normal arg: yasoob another arg through *argv: python another arg through *argv: eggs another arg through *argv: test # ** kwargs允许您将keyworded可变长度的参数传递给函数。如果要在函数中处理命名参数，则应使用** kwargs def greet_me(**kwargs): for key, value in kwargs.items(): print(\"{0} = {1}\".format(key, value)) # 必须含有key greet_me(name=\"yasoob\",age=\"19\",sex=\"girl\") name = yasoob age = 19 sex = girl args、*kwargs用法 def foo(*args, **kwargs): print('args = ', args) print('kwargs = ', kwargs) print('---------------------------------------') if __name__ == '__main__': foo(1,2,3,4) foo(a=1,b=2,c=3) foo(1,2,3,4, a=1,b=2,c=3) foo('a', 1, None, a=1, b='2', c=3) args = (1, 2, 3, 4) kwargs = {} --------------------------------------- args = () kwargs = {'a': 1, 'b': 2, 'c': 3} --------------------------------------- args = (1, 2, 3, 4) kwargs = {'a': 1, 'b': 2, 'c': 3} --------------------------------------- args = ('a', 1, None) kwargs = {'a': 1, 'b': '2', 'c': 3} --------------------------------------- *args`表示任何多个无名参数，它是一个tuple **kwargs表示关键字参数，它是一个dict 同时使用*args和**kwargs时，必须*args参数列要在**kwargs前 有限个参数 # -*- coding: UTF-8 -*- def log(func): def wrapper(a,b): print(\"call test(%d，%d)\" %(a,b)) return func(a,b) return wrapper @log def test(a,b): print(\"sum = %d\" % (a+b)) test(2,4) call test(2，4) sum = 6 可变参数 # 当装饰器不知道业务函数到底有多少个参数时，用*args 来代替 def log(func): def wrapper(*args): print(\"call test()\" ) return func(*args) return wrapper @log def test(a,b,c): print(\"sum = %d\" % (a+b+c)) test(2,4,5) call test() sum = 11 函数注解 函数声明中的各个参数可以在 : 之后增加注解表达式。如果参数有默认值，注解放在参数名和 = 号之间。如果想注解返回值，在 ) 和函数声明末尾的 : 之间添加 -> 和一个表达式。 from typing import List def func(num: int = 0, type: str = 'default') -> List[int]: return [num, num] func(num=2) Out[3]: [2, 2] 注解不会做任何处理，只是存储在函数的 __annotations__ 属性（一个字典） Python 对注解所做的唯一的事情是，把它们存储在函数的 __annotations__ 属性里。仅此而已，Python 不做检查、不做强制、不做验证，什么操作都不做。换句话说，注解对 Python 解释器没有任何意义。注解只是元数据，可以供 IDE、框架和装饰器等工具使用。 标准库中还没有什么会用到这些元数据，唯有 inspect.signature() 函数知道怎么提取注解 signature 函数返回一个 Signature 对象，它有一个 return_annotation 属性和一个 parameters 属性，后者是一个字典，把参数名映射到 Parameter 对象上。每个 Parameter 对象自己也有 annotation 属性 inspect.signature 函数返回一个 inspect.Signature 对象，它有一个 parameters 属性，这是一个有序映射，把参数名和 inspect.Parameter 对象对应起来。 各个 Parameter 属性也有自己的属性，例如 name 、 default 和 kind 。特殊的 inspect._empty 值表示没有默认值，考虑到 None 是有效的默认值（也经常这么做），而且这么做是合理的。 kind 属性的值是 _ParameterKind 类中的 5 个值之一，列举如下。 POSITIONAL_OR_KEYWORD 可以通过定位参数和关键字参数传入的形参（多数 Python 函数的参数属于此类）。 VAR_POSITIONAL 定位参数元组。 VAR_KEYWORD 关键字参数字典。 KEYWORD_ONLY 仅限关键字参数（Python 3 新增）。 POSITIONAL_ONLY 仅限定位参数；目前，Python 声明函数的句法不支持，但是有些使用 C 语言实现且不接受关键字参数的函数（如 divmod ）支持。 除了 name 、 default 和 kind ， inspect.Parameter 对象还有一个 annotation （注解）属性，它的值通常是 inspect._empty inspect.Signature 对象有个 bind 方法，它可以把任意个参数绑定到签名中的形参上，所用的规则与实参到形参的匹配方式一样。 函数式编程包 operator模块 Python 的目标不是变成函数式编程语言，但是得益于 operator 和 functools 等包的支持，函数式编程风格也可以信手拈来。 reduce 在函数式编程中，经常需要把算术运算符当作函数使用。例如，不使用递归计算阶乘。求和可以使用 sum 函数，但是求积则没有这样的函数。我们可以使用 reduce operator 模块为多个算术运算符提供了对应的函数，从而避免编写 lambda a, b: a*b 这种平凡的匿名函数 from functools import reduce from operator import mul def fact(n): return reduce(mul, range(1, n+1)) operator 模块中还有一类函数，能替代从序列中取出元素或读取对象属性的 lambda 表达式：因此，itemgetter 和 attrgetter 其实会自行构建函数。 itemgetter itemgetter 的常见用途：根据元组的某个字段给元组列表排序。 itemgetter(1) 的作用与 lambda fields: fields[1] 一样：创建一个接受集合的函数，返回索引位 1 上的元素。 >>> metro_data = [ ... ('Tokyo', 'JP', 36.933, (35.689722, 139.691667)), ... ('Delhi NCR', 'IN', 21.935, (28.613889, 77.208889)), ... ('Mexico City', 'MX', 20.142, (19.433333, -99.133333)), ... ('New York-Newark', 'US', 20.104, (40.808611, -74.020386)), ... ('Sao Paulo', 'BR', 19.649, (-23.547778, -46.635833)), ... ] >>> >>> from operator import itemgetter >>> for city in sorted(metro_data, key=itemgetter(1)): ... print(city) ... ('Sao Paulo', 'BR', 19.649, (-23.547778, -46.635833)) ('Delhi NCR', 'IN', 21.935, (28.613889, 77.208889)) ('Tokyo', 'JP', 36.933, (35.689722, 139.691667)) ('Mexico City', 'MX', 20.142, (19.433333, -99.133333)) ('New York-Newark', 'US', 20.104, (40.808611, -74.020386)) d = {\"a\":1,\"b\":2,\"c\":3} itemgetter(\"a\",'b')(d) Out[67]: (1, 2) attrgetter attrgetter 与 itemgetter 作用类似，它创建的函数根据名称提取对象的属性。如果把多个属性名传给 attrgetter，它也会返回提取的值构成的元组。 >>> for city in sorted(metro_areas, key=attrgetter('coord.lat')): ... print(name_lat(city)) methodcaller methodcaller。它的作用与 attrgetter 和 itemgetter 类似，它会自行创建函数。methodcaller 创建的函数会在对象上调用参数指定的方法 >>> from operator import methodcaller >>> s = 'The time has come' >>> upcase = methodcaller('upper') >>> upcase(s) 'THE TIME HAS COME' >>> hiphenate = methodcaller('replace', ' ', '-') >>> hiphenate(s) 'The-time-has-come' 其他模块 下面是 operator 模块中定义的部分函数（省略了以 _ 开头的名称，因为它们基本上是实现细节）：Python 3.5 中增加了 imatmul 和 matmul。 >>> [name for name in dir(operator) if not name.startswith('_')] ['abs', 'add', 'and_', 'attrgetter', 'concat', 'contains', 'countOf', 'delitem', 'eq', 'floordiv', 'ge', 'getitem', 'gt', 'iadd', 'iand', 'iconcat', 'ifloordiv', 'ilshift', 'imod', 'imul', 'index', 'indexOf', 'inv', 'invert', 'ior', 'ipow', 'irshift', 'is_', 'is_not', 'isub', 'itemgetter', 'itruediv', 'ixor', 'le', 'length_hint', 'lshift', 'lt', 'methodcaller', 'mod', 'mul', 'ne', 'neg', 'not_', 'or_', 'pos', 'pow', 'rshift', 'setitem', 'sub', 'truediv', 'truth', 'xor'] 这 52 个名称中大部分的作用不言而喻。以 i 开头、后面是另一个运算符的那些名称（如 iadd、iand 等），对应的是增量赋值运算符（如 +=、&= 等）。如果第一个参数是可变的，那么这些运算符函数会就地修改它；否则，作用与不带 i 的函数一样，直接返回运算结果 以下表格显示了抽象运算是如何对应于 Python 语法中的运算符和 operator 模块中的函数的。 运算 语法 函数 加法 a + b add(a, b) 字符串拼接 seq1 + seq2 concat(seq1, seq2) 包含测试 obj in seq contains(seq, obj) 除法 a / b truediv(a, b) 整除法 a // b floordiv(a, b) 按位与 a & b and_(a, b) 按位异或 a ^ b xor(a, b) 按位取反 ~ a invert(a) 按位或 a | b or_(a, b) 取幂 a ** b pow(a, b) 一致标识 a is b is_(a, b) 一致标识 a is not b is_not(a, b) 索引赋值 obj[k] = v setitem(obj, k, v) 索引删除 del obj[k] delitem(obj, k) 索引取值 obj[k] getitem(obj, k) 左移 a lshift(a, b) 取模 a % b mod(a, b) 乘法 a * b mul(a, b) 矩阵乘法 a @ b matmul(a, b) 取反（算术） - a neg(a) 取反（逻辑） not a not_(a) 正数 + a pos(a) 右移 a >> b rshift(a, b) 切片赋值 seq[i:j] = values setitem(seq, slice(i, j), values) 切片删除 del seq[i:j] delitem(seq, slice(i, j)) 切片取值 seq[i:j] getitem(seq, slice(i, j)) 字符串格式化 s % obj mod(s, obj) 减法 a - b sub(a, b) 真值测试 obj truth(obj) 比较 a lt(a, b) 比较 a le(a, b) 相等 a == b eq(a, b) 不等 a != b ne(a, b) 比较 a >= b ge(a, b) 比较 a > b gt(a, b) functools模块 每周一个 Python 模块 | functools Python functools 模块 说到高阶函数，这是函数式编程范式中很重要的一个概念，简单地说， 就是一个可以接受函数作为参数或者以函数作为返回值的函数，因为 Python 中函数是一类对象， 因此很容易支持这样的函数式特性。 functools 模块中函数只有 cmp_to_key、partial、reduce、total_ordering、update_wrapper、wraps、lru_cache 这几个： partial 用于创建一个偏函数，它用一些默认参数包装一个可调用对象，返回结果是可调用对象，并且可以像原始对象一样对待，这样可以简化函数调用。 from functools import partial def add(x, y): return x + y add_y = partial(add, 3) # add_y 是一个函数 add_y(4) # 结果是 7 partialmethod partialmethod是 Python 3.4 中新引入的装饰器，作用基本类似于 partial， 不过仅作用于方法(函数和方法的异同见上方)。 class Cell(object): def __init__(self): self._alive = False @property def alive(self): return self._alive def set_state(self, state): self._alive = bool(state) set_alive = partialmethod(set_state, True) set_dead = partialmethod(set_state, False) c = Cell() c.alive # False c.set_alive() c.alive # True total_ordering total_ordering 同样是 Python 2.7 中新增函数，用于简化比较函数的写法。如果你已经定义了 __eq__ 方法，以及 __lt__、__le__、__gt__ 或者 __ge__() 其中之一， 即可自动生成其它比较方法。官方示例： @total_ordering class Student: def __eq__(self, other): return ((self.lastname.lower(), self.firstname.lower()) == (other.lastname.lower(), other.firstname.lower())) def __lt__(self, other): return ((self.lastname.lower(), self.firstname.lower()) cmp_to_key cmp_to_key 是 Python 2.7 中新增的函数，用于将比较函数转换为 key 函数， 这样就可以应用在接受 key 函数为参数的函数中。比如 sorted()、min()、 max()、 heapq.nlargest()、 itertools.groupby() 等。 sorted(range(5), key=cmp_to_key(lambda x, y: y-x)) # [4, 3, 2, 1, 0] lru_cache 一个装饰器是在 Python3 中新加的，在 Python2 中如果想要使用可以安装第三方库 functools32。该装饰器用于缓存函数的调用结果，对于需要多次调用的函数，而且每次调用参数都相同，则可以用该装饰器缓存调用结果，从而加快程序运行。 from functools import lru_cache @lru_cache(None) def add(x, y): print(\"calculating: %s + %s\" % (x, y)) return x + y print(add(1, 2)) print(add(1, 2)) # 直接返回缓存信息 print(add(2, 3)) calculating: 1 + 2 3 3 calculating: 2 + 3 5 由于该装饰器会将不同的调用结果缓存在内存中，因此需要注意内存占用问题，避免占用过多内存，从而影响系统性能。 lru_cache 可以使用两个可选的参数来配置 maxsize 参数指定存储多少个调用的结果。缓存满了之后，旧的结果会被扔掉，腾出空间。为了得到最佳性能，maxsize 应该设为 2 的幂。typed 参数如果设为 True，把不同参数类型得到的结果分开保存，即把通常认为相等的浮点数和整数参数（如 1 和 1.0）区分开。顺便说一下，因为 lru_cache 使用字典存储结果，而且键根据调用时传入的定位参数和关键字参数创建，所以被 lru_cache 装饰的函数，它的所有参数都必须是可散列的。 singledispatch 见本页-单分派泛函数singledispatch wraps 使用装饰器极大地复用了代码，但是他有一个缺点就是原函数的元信息不见了，比如函数的docstring、name、参数列表 functools.wraps，wraps本身也是一个装饰器，它能把原函数的元信息拷贝到装饰器里面的 func 函数中，这使得装饰器里面的log函数也有和原函数test一样的元信息了 说到“接受函数为参数，以函数为返回值”，在 Python 中最常用的当属装饰器了。 functools 库中装饰器相关的函数是 update_wrapper、wraps，还搭配 WRAPPER_ASSIGNMENTS 和 WRAPPER_UPDATES 两个常量使用，作用就是消除 Python 装饰器的一些负面作用。 def log(func): def wrapper(): print(\"call test()\") return func() return wrapper @log def test(): print(\"this is what I want\") test() print(test.__name__) call test() this is what I want wrapper import functools def log(func): @functools.wraps(func) def wrapper(): print(\"call test()\") return func() return wrapper @log def test(): print(\"this is what I want\") test() print(test.__name__) call test() this is what I want test update_wrapper update_wrapper 的作用与 wraps 类似，不过功能更加强大，换句话说，wraps 其实是 update_wrapper 的特殊化，实际上 wraps(wrapped) 相当于 partial(update_wrapper, wrapped=wrapped, **kwargs)。 def decorator(func): def wrapper(*args, **kwargs): return func(*args, **kwargs) return update_wrapper(wrapper, func) 装饰器(Dercrator) 必须会 Python 装饰器的五个理由 Python装饰器 用于有切面需求的场景，比如：插入日志、性能测试、事务处理、缓存、权限校验等场景 用装饰器抽离出大量与函数功能本身无关的雷同代码到装饰器中并继续重用 严格来说，装饰器只是语法糖，装饰器的两大特性是： 能把被装饰的函数替换成其他函数 函数装饰器在加载模块时立即执行(Python 加载模块时)，被装饰的函数只有在明确调用时运行，这突出了导入时和运行时的区别。 装饰器的4种类型：函数装饰函数、函数装饰类、类装饰函数、类装饰类 globals关键字 返回一个字典，表示当前的全局符号表。这个符号表始终针对当前模块（对函数或方法来说，是指定义它们的模块，而不是调用它们的模块）。 方式一 globals 函数帮助 best_promo 自动找到其他可用的 *_promo 函数 promos = [globals()[name] for name in globals() ➊ if name.endswith('_promo') ➋ and name != 'best_promo'] ➌ 方式二 收集所有可用促销的另一种方法是，在一个单独的模块中保存所有策略函数，把 best_promo 排除在外。 最大的变化是内省名为 promotions 的独立模块，构建策略函数列表。 promos = [func for name, func in inspect.getmembers(promotions, inspect.isfunction)] 闭包 创建保有内部状态的函数，还有一种截然不同的方式——使用闭包。 在表达式中引用变量时，Python解释器将按如下顺序遍历各作用域，以解析该引用: 当前函数的作用域 任何外围作用域(例如，包含当前函数的其他函数) 包含当前代码的那个模块的作用域(也叫全局作用域，global scope) 内置作用域(也就是包含len及str等函数的那个作用域) 如果上面这些地方都没有定义过名称相符的变量，那就抛出NameError异常。 nonlocal语句清楚地表明: 如果在闭包内给该变量赋值，那么修改的其实是闭包外那个作用域中的变量。 这与global语句互为补充，global 用来表示对该变量的赋值操作，将会直接修改模块作用域里的那个变量。 ps：按照我们正常的认知，一个函数结束的时候，会把自己的临时变量都释放还给内存，之后变量都不存在了。一般情况下，确实是这样的。 但是闭包是一个特别的情况，外部函数发现，自己的临时变量会在将来的内部函数中用到，自己在结束的时候，返回内函数的同时，会把外函数的临时变量送给内函数绑定在一起。 所以外函数已经结束了，调用内函数的时候仍然能够使用外函数的临时变量。 global关键字 如果在函数中赋值时想让解释器把 b 当成全局变量，要使用 global 声明： b = 6 def func(a): global b print(a) print(b) b = 9 自由变量 闭包：只有涉及嵌套函数时才有闭包问题 闭包指延伸了作用域的函数，其中包含函数定义体中引用、但是不在定义体中定义的非全局变量。 函数是不是匿名的没有关系，关键是它能访问定义体之外定义的非全局变量。 在 averager 函数中，series 是自由变量（free variable），指未在本地作用域中绑定的变量 averager 的闭包延伸到那个函数的作用域之外，包含自由变量 series 的绑定 只有嵌套在其他函数中的函数才可能需要处理不在全局作用域中的外部变量 nonlocal声明 但是对数字、字符串、元组等不可变类型来说，只能读取，不能更新。 如果尝试重新绑定，例如 count = count + 1，其实会隐式创建局部变量 count。 这样，count 就不是自由变量了，因此不会保存在闭包中，为了解决这个问题，Python 3 引入了 nonlocal 声明。 它的作用是把变量标记为自由变量，即使在函数中为变量赋予新值了，也会变成自由变量。 如果为 nonlocal 声明的变量赋予新值，闭包中保存的绑定会更新。 def make_averager(): count, total = 0, 0 def averager(new_value): nonlocal count, total count += 1 total += new_value return total / count return averager 小结：外函数内部定义可变类型的变量可以在内函数使用，对数字、字符串、元组等不可变类型来说，需要使用 nonlocal 声明变量为自由变量，内函数才可以访问到。 函数装饰函数 装饰器的典型行为：把被装饰的函数替换成新函数，二者接受相同的参数，而且（通常）返回被装饰的函数本该返回的值，同时还会做些额外操作。 def decorator(func): def inner(*args, **kwargs): print('before...........') res = func(*args, **kwargs) print('after............') return res return inner @decorator def run(): print('run...............') return 0 if __name__ == \"__main__\": run() run.__name__ # 此时decorator叫做装饰器 ------------------------------------------ before........... run............... after............ inner --------------------- functools.wrap装饰器 inner的返回值要与func的一致，并且inner与func参数相同 为了不改变被装饰函数或类的性质，添加functools.wrap装饰器 from functools import wraps def decorator(func): @wraps(func) def inner(): print('before...........') res = func() print('after............') return res return inner @decorator def run(): print('run...............') return 0 if __name__ == \"__main__\": run() print(run.__name__) ------------------------------------------ before........... run............... after............ run --------------------- 带参数的装饰器(3层) from functools import wraps from datetime import datetime def start(): return datetime.now() def end(): return datetime.now() def Filter(start_time, end_time): def decorator(func): @wraps(func) def inner(*args, **kwargs): s = start_time() res = func(*args,**kwargs) e = end_time() print(\"耗时 {} s\".format((e-s).total_seconds())) return res return inner return decorator @Filter(start, end) def run(): for i in range(2): for j in range(3): print(j) return 0 if __name__ == \"__main__\": run() 0 1 2 0 1 2 耗时 0.003987 s 带有不定参数的装饰器 # 带有不定参数的装饰器 # 拓展的函数好多可是有参数，有的参数还是个数不定的那种 import time def deco(func): def wrapper(*args, **kwargs): startTime = time.time() func(*args, **kwargs) endTime = time.time() msecs = (endTime - startTime) * 1000 print(\"time is %d ms\" % msecs) return wrapper @deco def func(a, b): print(\"hello,here is a func for add:\") time.sleep(1) print(\"result is %d\" % (a + b)) @deco def func2(a, b, c): print(\"hello,here is a func for add:\") time.sleep(1) print(\"result is %d\" % (a + b + c)) if __name__ == '__main__': f = func func2(3, 4, 5) f(3, 4) hello,here is a func for add: result is 12 time is 1000 ms hello,here is a func for add: result is 7 time is 1000 ms 多个装饰器 # 一个函数需要加入很多功能，一个装饰器怕是搞不定，装饰器能支持多个嘛 # 多个装饰器执行的顺序就是从最后一个装饰器开始，执行到第一个装饰器，再执行函数本身。 # 多个装饰器 import time def deco01(func): def urapper(*args, **kwargs): print(\"this is decoe1\") startTime = time.time() func(*args, **kwargs) endTime = time.time() msecs = (endTime - startTime) * 1000 print(\"time is %d ms\" % msecs) print(\"decoe1 end here\") return urapper def deco02(func): def wrapper(*args, **kwargs): print(\"this is decoe2\") func(*args, **kwargs) print(\"decoe2 end here\") return wrapper @deco01 @deco02 def func(a, b): print(\"hello,here is a func for add:\") time.sleep(1) print(\"result is %d\" % (a + b)) if __name__ == '__main__': f = func f(3, 4) this is decoe1 this is decoe2 hello,here is a func for add: result is 7 decoe2 end here time is 1003 ms decoe1 end here 函数装饰类 def wrapClass(cls): def inner(a): print('class name:', cls.__name__) return cls(a) return inner @wrapClass class Foo(): def __init__(self, a): self.a = a def fun(self): print('self.a =', self.a) m = Foo('xiemanR') m.fun() class name: Foo self.a = xiemanR 定义 装饰器不仅可以是函数，还可以是类 相比函数装饰器，类装饰器具有灵活度大、高内聚、封装性等优点 像__call__这样前后都带下划线的方法在Python中被称为内置(魔法)方法。重载这些魔法方法一般会改变对象的内部行为 用法 让类的构造函数__init__()接受一个函数 重载call()并 返回一个函数 使用@类形式将装饰器附加到业务函数上 类装饰函数 class ShowFunName(): def __init__(self, func): self._func = func def __call__(self, a): print('function name:', self._func.__name__) return self._func(a) @ShowFunName def Bar(a): return a print(Bar('xiemanR')) function name: Bar xiemanR 类装饰类 class ShowClassName(object): def __init__(self, cls): self._cls = cls def __call__(self, a): print('class name:', self._cls.__name__) return self._cls(a) @ShowClassName class Foobar(object): def __init__(self, a): self.value = a def fun(self): print(self.value) a = Foobar('xiemanR') a.fun() class name: Foobar xiemanR 内置装饰器 命令行神器Click 命令行神器 Click教程A篇 # -*- coding: utf-8 -* import click @click.command() @click.option('--count', default=1, help='Number of greetings.') @click.option('--name', prompt='Your name', help='The person to greet.') def hello(count, name): \"\"\"Simple program that greets NAME for a total of COUNT times.\"\"\" for x in range(count): click.echo('Hello %s!' % name) if __name__ == '__main__': hello() Q:\\pyCharmWS>python ./tempTest.py --count=3 --name=Ethan Hello Ethan! Hello Ethan! Hello Ethan! @property和@classmethod python中常用的内置装饰器 # @property # 使调用类中的方法像引用类中的字段属性一样。被修饰的特性方法，内部可以实现处理逻辑，但对外提供统一的调用方式。遵循了统一访问的原则。 # @classmethod # 类方法的第一个参数是类，将类本身作为操作的方法。类方法被哪个类调用，就传入哪个类作为第一个参数进行操作。 # 注意，静态方法和类方法是为类操作准备的。虽然通过实例也能调用，但是不建议 # -*- coding: utf-8 -*- # coding: utf-8 class TestClass: name = \"test\" def __init__(self, name): self.name = name @property def sayHello(self): print(\"hello\", self.name) @staticmethod def fun(self, x, y): return x + y cls = TestClass(\"felix\") print(f\"通过实例引用属性: {cls.name}\") print(f\"像引用属性一样调用@property修饰的方法: {cls.sayHello}\") print(f\"类名直接引用静态方法: {TestClass.fun(None, 2, 3)}\") 通过实例引用属性: felix hello felix 像引用属性一样调用@property修饰的方法: None 类名直接引用静态方法: 5 functools模块的内置装饰器 更详细的见上方的functools模块 lru_cache做备忘 functools.lru_cache 是非常实用的装饰器，它实现了备忘（memoization）功能。 这是一项优化技术，它把耗时的函数的结果保存起来，避免传入相同的参数时重复计算。 LRU 三个字母是“Least Recently Used”的缩写，表明缓存不会无限制增长，一段时间不用的缓存会被扔掉。 生成第 n 个斐波纳契数这种慢速递归函数适合使用 lru_cache import functools from clockdeco import clock @functools.lru_cache() # ➊ @clock # ➋ def fibonacci(n): if n 0 [0.00000119s] fibonacci(1) -> 1 [0.00010800s] fibonacci(2) -> 1 [0.00000787s] fibonacci(3) -> 2 [0.00016093s] fibonacci(4) -> 3 [0.00001216s] fibonacci(5) -> 5 [0.00025296s] fibonacci(6) -> 8 除了优化递归算法之外，lru_cache 在从 Web 中获取信息的应用中也能发挥巨大作用。 特别要注意，lru_cache 可以使用两个可选的参数来配置。它的签名是： functools.lru_cache(maxsize=128, typed=False) maxsize 参数指定存储多少个调用的结果。缓存满了之后，旧的结果会被扔掉，腾出空间。为了得到最佳性能，maxsize 应该设为 2 的幂。typed 参数如果设为 True，把不同参数类型得到的结果分开保存，即把通常认为相等的浮点数和整数参数（如 1 和 1.0）区分开。 顺便说一下，因为 lru_cache 使用字典存储结果，而且键根据调用时传入的定位参数和关键字参数创建，所以被 lru_cache 装饰的函数，它的所有参数都必须是可散列的。 单分派泛函数singledispatch PEP 443 -- Single-dispatch generic functions functools.singledispatch 是 Python 3.4 增加的，PyPI 中的 singledispatch包可以向后兼容 Python 2.6 到 Python 3.3。 假设我们在开发一个调试 Web 应用的工具，我们想生成 HTML，显示不同类型的 Python 对象。 我们可能会编写这样的函数： import html def htmlize(obj): content = html.escape(repr(obj)) return '{}'.format(content) 这个函数适用于任何 Python 类型，但是现在我们想做个扩展，让它使用特别的方式显示某些类型。 str：把内部的换行符替换为 ' \\n'；不使用 ，而是使用 int：以十进制和十六进制显示数字 list：输出一个 HTML 列表，根据各个元素的类型进行格式化 因为 Python 不支持重载方法或函数，所以我们不能使用不同的签名定义 htmlize 的变体，也无法使用不同的方式处理不同的数据类型。在 Python 中，一种常见的做法是把 htmlize 变成一个分派函数，使用一串 if/elif/elif，调用专门的函数，如 htmlize_str、htmlize_int，等等。这样不便于模块的用户扩展，还显得笨拙：时间一长，分派函数 htmlize 会变得很大，而且它与各个专门函数之间的耦合也很紧密。 Python 3.4 新增的 functools.singledispatch 装饰器可以把整体方案拆分成多个模块，甚至可以为你无法修改的类提供专门的函数。使用 @singledispatch 装饰的普通函数会变成泛函数（generic function）：根据第一个参数的类型，以不同方式执行相同操作的一组函数 如果根据多个参数选择专门的函数，那就是多分派了。 singledispatch 创建一个自定义的 htmlize.register 装饰器，把多个函数绑在一起组成一个泛函数 from functools import singledispatch from collections import abc import numbers import html @singledispatch ➊ def htmlize(obj): content = html.escape(repr(obj)) return '{}'.format(content) @htmlize.register(str) ➋ def _(text): ➌ content = html.escape(text).replace('\\n', '\\n') return '{0}'.format(content) @htmlize.register(numbers.Integral) ➍ def _(n): return '{0} (0x{0:x})'.format(n) @htmlize.register(tuple) ➎ @htmlize.register(abc.MutableSequence) def _(seq): inner = '\\n'.join(htmlize(item) for item in seq) return '\\n' + inner + '\\n' ❶ @singledispatch 标记处理 object 类型的基函数。 ❷ 各个专门函数使用 @«base_function».register(«type») 装饰。 ❸ 专门函数的名称无关紧要；_ 是个不错的选择，简单明了。 ❹ 为每个需要特殊处理的类型注册一个函数。numbers.Integral 是 int 的虚拟超类。 ❺ 可以叠放多个 register 装饰器，让同一个函数支持不同类型。 只要可能，注册的专门函数应该处理抽象基类（如 numbers.Integral 和 abc.MutableSequence），不要处理具体实现（如 int 和 list）。这样，代码支持的兼容类型更广泛。例如，Python 扩展可以子类化 numbers.Integral，使用固定的位数实现 int 类型。 singledispatch 机制的一个显著特征是，你可以在系统的任何地方和任何模块中注册专门函数。如果后来在新的模块中定义了新的类型，可以轻松地添加一个新的专门函数来处理那个类型。 此外，你还可以为不是自己编写的或者不能修改的类添加自定义函数。 @singledispatch 不是为了把 Java 的那种方法重载带入 Python。在一个类中为同一个方法定义多个重载变体，比在一个函数中使用一长串 if/elif/elif/elif 块要更好。但是这两种方案都有缺陷，因为它们让代码单元（类或函数）承担的职责太多。@singledispath 的优点是支持模块化扩展：各个模块可以为它支持的各个类型注册一个专门函数。 柯里化 柯里化（Currying） 将原来接受两个参数的函数变成新的接受一个参数的函数的过程。 新的函数返回一个以原有第二个参数为参数的函数。 # pip install simplecurry -i https://pypi.tuna.tsinghua.edu.cn/simple from simplecurry import curried @curried def add2(a,b,c): return c * a + b add2(2)(5)(8) >>> 21 curry化最大的意义在于把多个参数的function等价转化成多个单参数function的级联，这样所有的函数就都统一了，方便做lambda演算。 在scala里，curry化对类型推演也有帮助，scala的类型推演是局部的，在同一个参数列表中后面的参数不能借助前面的参数类型进行推演，curry化以后，放在两个参数列表里，后面一个参数列表里的参数可以借助前面一个参数列表里的参数类型进行推演。这就是为什么 foldLeft这种函数的定义都是curry的形式。 案例 时间装饰器 计算函数运行时间 from functools import wraps # 作为装饰器使用，返回函数执行需要花费的时间 def time_this_function(func): @wraps(func) def wrapper(*args, **kwargs): start = time.time() result = func(*args, **kwargs) end = time.time() print(f'{func.__name__}, 耗时{round(end - start, 4)}s') return result return wrapper Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-12-01 00:57:11 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/20.python进阶问题.html":{"url":"chapters/20.python进阶问题.html","title":"python进阶问题","keywords":"","body":"高级概念与包函数式编程包operator模块查找策略闭包closure和偏函数Partial怪异问题避坑扩展阅读机器学习8大算法比较今年GitHub排名前20的Python机器学习开源项目Python基础网站列表(有空再摘录)其他 高级概念与包 函数式编程包operator form operator import mul 可以替代 reduce(lambda a,b:a*b, range(1,1+n)) 为 reduce(mul, range(1,1+n)) itemgetter(1) 可以替代 lambda fields: fields[1]：创建一个接受集合的函数，返回索引位1上的元素 attrgetter与itemgetter类似，它创建的函数根据名称提取对象的属性。如果把多个属性名传给attrgetter，它也会返回提取的值构成的元组。 methodcaller会自行创建函数，创建的函数会在对象上调用参数指定的方法。 from operator import methodcaller s = 'The time is come' upcase = methodcaller('upper') upcase(s) Out[11]: 'THE TIME IS COME' 模块查找策略 目标：自动查找其他可用的*_promo函数 方式一： promos = [globals()[name] for name in globals() if name.endswith('_promo') and name != 'best_promo'] 方式二：将所有促销放在promotions模块 import inspect promos = [func for name,func in inspect.getmembers(promotions, inspect.isfunction)] inspect.getmembers用于捕获对象(这里是promotions模块)的属性，第二个参数是可选的判断条件(一个布尔值函数)，这里只提取模块中的函数 闭包closure和偏函数Partial 内层函数+所引用的外层变量，称为闭包 # 闭包就是函数里面嵌套一个函数 # 因为函数的出现，导致变量出现了分化：全局变量、局部变量 # 全局变量：当前模块中任意地方都能访问！ # 局部变量：当前函数中可以操作 # 全局变量-局部变量：变量-在函数外部调用访问： def outer(): msg = \"这是一个局部变量\" print(\"这是一个函数\", msg) def inner(): print(\"可以访问外部变量msg\", msg) return msg return inner 在函数嵌套的前提下 内层函数引用了外层函数的变(包括参数) 外层函数又把内层函数当做返回值进行返回 闭包中，如果要修改引用的外层变量 需要使用 nonlocal 变量声明，表示非局部的 否则会被当做是闭包内，新定义的变量 # PYTHON中的偏函数，就是一个语法糖 # 为原来没有默认值的函数，创建带常用值的偏函数 from functools import partial def show2(name, msg): print(name, \": \" , msg) s = partial(show2, msg=\"HI\") s(\"tom\") show2(\"jerry\", \"hello\") tom : HI jerry : hello python模块:profile,pstats 怪异问题避坑 class Teas: def __init__(self, lst=[]): self.lst = lst def hyc_print(self): logger.info(self.lst) def __del__(self): pass if __name__ == '__main__': # multiprocessing_queue_test() te1 = Teas() te1.lst.append(1) te1.hyc_print() te2 = Teas() te2.lst.append(2) te2.hyc_print() te1.hyc_print() logger.info(te1.lst is te2.lst) 2020-05-29 17:14:40 multiprocessing_queue_iterator INFO: [1] 2020-05-29 17:14:40 multiprocessing_queue_iterator INFO: [1, 2] 2020-05-29 17:14:40 multiprocessing_queue_iterator INFO: [1, 2] 2020-05-29 17:14:40 multiprocessing_queue_iterator INFO: True class Teas: def __init__(self, lst=None): self.lst = lst if lst is not None else [] def hyc_print(self): logger.info(self.lst) def __del__(self): pass 2020-05-29 17:15:43 multiprocessing_queue_iterator INFO: [1] 2020-05-29 17:15:43 multiprocessing_queue_iterator INFO: [2] 2020-05-29 17:15:43 multiprocessing_queue_iterator INFO: [1] 2020-05-29 17:15:43 multiprocessing_queue_iterator INFO: False 扩展阅读 机器学习8大算法比较 https://mp.weixin.qq.com/s/0dT4BN01g0anVwyfjS-RVA 今年GitHub排名前20的Python机器学习开源项目 https://mp.weixin.qq.com/s/-WJ_S6CV7Cc14f4YzthPAQ Python基础网站列表(有空再摘录) 1. 元组 https://www.jianshu.com/p/b728648501a8 2. 字典 https://www.jianshu.com/p/8b51c9bf6d12 3. 集合 https://www.jianshu.com/p/75eb228b638e 4. 列表 https://www.jianshu.com/p/636314cf0126 6.作用域 https://www.jianshu.com/p/d8271c03a0f3 8. 对象(属性限制-公有私有) https://www.jianshu.com/p/c7f6ecf07fbc 9. 对象(属性限制-只读) https://www.jianshu.com/p/dd0e1487a4d6 10. 对象(属性限制-只读优化) https://www.jianshu.com/p/f54e0a5af635 11. 对象(系统内置方法-遍历操作) https://www.jianshu.com/p/bd87cca40d8b 12. 内存管理机制-引用计数/垃圾回收/循环引用/弱引用 https://www.jianshu.com/p/ef8a218c6b89 13. 对象思想 https://www.jianshu.com/p/0347ba667667 14. 综合案例：封装/继承/多态 https://www.jianshu.com/p/b35043c76f50 15. 异常处理(错误和异常) https://www.jianshu.com/p/507d677e74a4 16. 包/模块(概念及导入语法) https://www.jianshu.com/p/1d9100f8292a 17. 包/模块(导入及其底层逻辑) https://www.jianshu.com/p/6a99e5e4c1b5 18. 包/模块(三方包安装) https://www.jianshu.com/p/68477d5625fc 19. 包/模块(创建和发布) https://www.jianshu.com/p/ee48fde9afd6 20. 其他 ​ 比如： 你要去做一个电商后台，存储着每件产品的 ID、名称和价格。现在需要根据商品 ID 找出价格，如何使用最合适的数据结构呢？ 在 Python 中字典、集合都是经过高度性能优化的数据结构，如果采用列表来存储数据并进行查找，时间复杂度是多少？ 换成字典呢？哪个更高效？事实上，采用不同数据结构存储十万数据，查找速度差异就有可能差出几千倍。 再比如： Python 中的协程和线程有什么区别？ 生成器如何进化成协程？ 并发编程中的 future 和 asyncio 有什么关系？ 如何写出线程安全的高性能代码呢？ Python 基础入门 必学知识：【Python 基础数据结构】【Python 基础语法】【文件操作】【错误与异常处理】【Python 面向对象】【模块化】 第一步，你需要掌握 Python 的核心基础知识。当然，不同于其他基础教材，我不仅仅只讲基础概念、操作，同时也为你整理了很多进阶难度的知识，或是一些重难点、易错点等需要注意的地方。不仅可以让入门级的程序员查漏补缺，打捞基础，也能让有经验的程序员，重新从工程角度认识基础，升华理解。 Python 进阶核心知识 必学知识：【Python 协议】【Python 高级语法】【Python 正则表达式】【Python 并发编程】【垃圾回收机制】【项目实战】 第二步，进阶 Python 核心知识点，比如装饰器、并发编程等等。如果你的工作只是写 100 行以下的脚本程序，可能不怎么会用得到。但如果你做的是大型程序的开发，则非常有必要。 规范：编写高质量的 Python 程序 这部分着重于教你把程序写得更加规范、更加稳定。我在实际工作中见过不少程序员，会写程序，但写得实在有点“惨不忍睹”，导致最后调试起来错误不断，修改非常费劲儿。因此，我觉得用单独一个版块讲解这个问题非常有必要。 当然，我不会用一些似是而非的规范来说教，而是会用具体的编程操作和技巧，教你提高代码质量。比如，如何合理地分解代码、运用 assert，如何写单元测试等等。 Python 实战，串联整个知识体系：带你搭建量化交易系统 必学知识点：【RESTful】【Socket】【Pandas】【Numpy】【Kafka】【RabbitMQ】【MySQL】【Django】 Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-12-01 00:57:11 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/6.python魔法函数.html":{"url":"chapters/6.python魔法函数.html","title":"python魔法函数","keywords":"","body":"魔法函数基本分类构造方法操作符比较操作符数值操作符类的表示自动化__repr__实现访问控制自定义序列容器相关反射抽象基类ABC定义抽象类注册具体类通过派生实现ABC中的具体方法抽象属性metaclass可调用的对象上下文管理器创建描述符对象拷贝PicklingPickling : 小试牛刀Pickle你的对象一个例子如何调用魔法方法 (译)Python魔法方法指南 原文 Python进阶：实例讲解Python中的魔法函数（Magic Methods） 魔法函数 所谓魔法函数（Magic Methods），是Python的一种高级语法，允许你在类中自定义函数（函数名格式一般为__xx__），并绑定到类的特殊方法中。比如在类A中自定义__str__()函数，则在调用str(A())时，会自动调用__str__()函数，并返回相应的结果。在我们平时的使用中，可能经常使用__init__函数和__del\\函数，其实这也是魔法函数的一种。 魔术方法（magic method）是特殊方法的昵称。特殊方法也叫双下方法 通过实现特殊方法来利用 Python 数据模型的两个好处。 作为你的类的用户，他们不必去记住标准操作的各式名称（“怎么得到元素的总数？是 .size() 还是 .length() 还是别的什么？”）。 可以更加方便地利用 Python 的标准库，比如 random.choice 函数，从而不用重新发明轮子。 特殊方法的存在是为了被 Python 解释器调用的，你自己并不需要调用它们。 除非有大量的元编程存在，直接调用特殊方法的频率应该远远低于你去实现它们的次数。唯一的例外可能是 __init__ 方法 Python 之禅”中的另外一句话：“不能让特例特殊到开始破坏既定规则。” python的魔法函数总被双下划线包围，它们可以给你的类增加特殊的方法。如果你的对象实现了这些方法中的一个，那么这个方法就会在特殊情况下被调用，你可以定义想要的行为，而这一切都是自动发生的。 什么是魔法函数？ 基本分类 构造方法 最为熟知的基本的魔法方法就是__init__ ，我们可以用它来指明一个对象初始化的行为。然而，当我们调用 x = SomeClass() 的时候， __init__ 并不是第一个被调用的方法。事实上，第一个被调用的是 __new__ ，这个方法才真正地创建了实例。当这个对象的生命周期结束的时候，__del__ 会被调用。 __new__(cls,[...]) __new__ 是对象实例化时第一个调用的方法，它只取下 cls 参数，并把其他参数传给 __init__ 。 __new__ 很少使用，但是也有它适合的场景，尤其是当类继承自一个像元组或者字符串这样不经常改变的类型的时候。 __init__(self,[...]) 类的初始化方法。它获取任何传给构造器的参数（比如我们调用 x = SomeClass(10, ‘foo’) ， __init__ 就会接到参数 10 和 ‘foo’ 。 __del__(self) __new__ 和 __init__ 是对象的构造器， __del__ 是对象的销毁器。它并非实现了语句 del x (因此该语句不等同于 x.__del__())。而是定义了当对象被垃圾回收时的行为。 当对象需要在销毁时做一些处理的时候这个方法很有用，比如 socket 对象、文件对象。但是需要注意的是，当Python解释器退出但对象仍然存活的时候， __del__ 并不会 执行。 所以养成一个手工清理的好习惯是很重要的，比如及时关闭连接。 操作符 使用Python魔法方法的一个巨大优势就是可以构建一个拥有Python内置类型行为的对象。这意味着你可以避免使用非标准的、丑陋的方式来表达简单的操作。 在一些语言中，这样做很常见: if instance.equals(other_instance): # do something 运用魔法方法的魔力，我们可以定义方法 eq if instance == other_instance: # do something 这是魔法力量的一部分，这样我们就可以创建一个像内建类型那样的对象了！ 比较操作符 Python包含了一系列的魔法方法，用于实现对象之间直接比较，而不需要采用方法调用。同样也可以重载Python默认的比较方法，改变它们的行为。 __cmp__(self, other) __cmp__ 是所有比较魔法方法中最基础的一个，它实际上定义了所有比较操作符的行为（ other 时返回正整数。最好只定义你所需要的比较形式，而不是一次定义全部。 如果你需要实现所有的比较形式，而且它们的判断标准类似，那么 __cmp__ 是一个很好的方法，可以减少代码重复，让代码更简洁。3.x版本被移除。 __eq__(self, other) 定义等于操作符(==)的行为。 __ne__(self, other) 定义不等于操作符(!=)的行为。 __lt__(self, other) 定义小于操作符( __gt__(self, other) 定义大于操作符(>)的行为。 __le__(self, other) 定义小于等于操作符( __ge__(self, other) 定义大于等于操作符(>)的行为。 数值操作符 五类：一元操作符，常见算数操作符，反射算数操作符，增强赋值操作符，和类型转换操作符。 一元操作符 一元操作符只有一个操作符。 __pos__(self) 实现取正操作，例如 +some_object。 __neg__(self) 实现取负操作，例如 -some_object。 __abs__(self) 实现内建绝对值函数 abs() 操作。 __invert__(self) 实现取反操作符 ~。 __round__(self， n) 实现内建函数 round() ，n 是近似小数点的位数。 __floor__(self) 实现 math.floor() 函数，即向下取整。 __ceil__(self) 实现 math.ceil() 函数，即向上取整。 __trunc__(self) 实现 math.trunc() 函数，即距离零最近的整数。 常见算数操作符 __add__(self, other) 实现加法操作。 __sub__(self, other) 实现减法操作。 __mul__(self, other) 实现乘法操作。 __floordiv__(self, other) 实现使用 // 操作符的整数除法。 __div__(self, other) 实现使用 / 操作符的除法。 __truediv__(self, other) 实现 true 除法，这个函数只有使用 from __future__ import division 时才有作用。 __mod__(self, other) 实现 % 取余操作。 __divmod__(self, other) 实现 divmod 内建函数。 __pow__(self, other[, modulo]) 实现 ** 操作符。 __lshift__(self, other) 实现左移位运算符 __rshift__(self, other) 实现右移位运算符 >> 。 __and__(self, other) 实现按位与运算符 & 。 __or__(self, other) 实现按位或运算符 | 。 __xor__(self, other) 实现按位异或运算符 ^ 。 反射算数运算符 反射只不过是运算符交换了一下位置： some_object + other -> other + some_object 所有反射运算符魔法方法和它们的常见版本做的工作相同，只不过是处理交换连个操作数之后的情况。绝大多数情况下，反射运算和正常顺序产生的结果是相同的，所以很可能你定义 __radd__ 时只是调用一下 __add__。注意一点，操作符左侧的对象（也就是上面的 other ）一定不要定义（或者产生 NotImplemented 异常） 操作符的非反射版本。例如，在上面的例子中，只有当 other 没有定义 __add__ 时 someobject.\\_radd__ 才会被调用。 __radd__(self, other) 实现反射加法操作。 __rsub__(self, other) 实现反射减法操作。 __rmul__(self, other) 实现反射乘法操作。 __rfloordiv__(self, other) 实现使用 // 操作符的整数反射除法。 __rdiv__(self, other) 实现使用 / 操作符的反射除法。 __rtruediv__(self, other) 实现 true 反射除法，这个函数只有使用 from __future__ import division 时才有作用。 __rmod__(self, other) 实现 % 反射取余操作符。 __rdivmod__(self, other) 实现调用 divmod(other, self) 时 divmod 内建函数的操作。 __rpow__(self, other[, modulo]) 实现 ** 反射操作符。 __rlshift__(self, other) 实现反射左移位运算符 __rshift__(self, other) 实现反射右移位运算符 >> 的作用。 __rand__(self, other) 实现反射按位与运算符 & 。 __ror__(self, other) 实现反射按位或运算符 | 。 __rxor__(self, other) 实现反射按位异或运算符 ^ 。 增强赋值运算符 Python同样提供了大量的魔法方法，可以用来自定义增强赋值操作的行为。增强赋值融合了“常见”的操作符和赋值操作，看下面的例子: x = 5 x += 1 # 也就是 x = x + 1 这些方法都应该返回左侧操作数应该被赋予的值。 __iadd__(self, other) 实现加法赋值操作。 __isub__(self, other) 实现减法赋值操作。 __imul__(self, other) 实现乘法赋值操作。 __ifloordiv__(self, other) 实现使用 //= 操作符的整数除法赋值操作。 __idiv__(self, other) 实现使用 /= 操作符的除法赋值操作。 __itruediv__(self, other) 实现 true 除法赋值操作，这个函数只有使用 from __future__ import division 时才有作用。 __imod__(self, other) 实现 %= 取余赋值操作。 __ipow__ 实现 **= 操作。 __ilshift__(self, other) 实现左移位赋值运算符 __irshift__(self, other) 实现右移位赋值运算符 >>= 。 __iand__(self, other) 实现按位与运算符 &= 。 __ior__(self, other) 实现按位或赋值运算符 |= 。 __ixor__(self, other) 实现按位异或赋值运算符 ^= 。 类型转换操作符 __int__(self) 实现到int的类型转换。 __long__(self) 实现到long的类型转换。 __float__(self) 实现到float的类型转换。 __complex__(self) 实现到complex的类型转换。 __oct__(self) 实现到八进制数的类型转换。 __hex__(self) 实现到十六进制数的类型转换。 __index__(self) 实现当对象用于切片表达式时到一个整数的类型转换。如果你定义了一个可能会用于切片操作的数值类型，你应该定义 __index__。 __trunc__(self) 当调用 math.trunc(self) 时调用该方法， __trunc__ 应该返回 self 截取到一个整数类型（通常是long类型）的值。 __coerce__(self) 该方法用于实现混合模式算数运算，如果不能进行类型转换， __coerce__ 应该返回 None 。反之，它应该返回一个二元组 self 和 other ，这两者均已被转换成相同的类型。因为和其他魔法方法有功能上的重复，以及本身行为令人迷惑。在python3.x版本已被移除。 类的表示 使用字符串来表示类是一个相当有用的特性。在Python中有一些内建方法可以返回类的表示，相对应的，也有一系列魔法方法可以用来自定义在使用这些内建函数时类的行为。 __str__(self) 定义对类的实例调用 str() 时的行为。 __repr__(self) 定义对类的实例调用 repr() 时的行为。 str() 和 repr() 最主要的差别在于“目标用户”。 repr() 的作用是产生机器可读的输出（大部分情况下，其输出可以作为有效的Python代码），而 str() 则产生人类可读的输出。 __repr__ 所返回的字符串应该准确、无歧义，并且尽可能表达出如何用代码创建出这个被打印的对象。 __repr__ 和 __str__ 的区别在于，后者是在 str() 函数被使用，或是在用 print 函数打印一个对象的时候才被调用的，并且它返回的字符串对终端用户更友好。 如果你只想实现这两个特殊方法中的一个，__repr__ 是更好的选择，因为如果一个对象没有 __str__ 函数，而 Python 又需要调用它的时候，解释器会用 __repr__作为替代。 自动化__repr__实现 首先定义一个元类来获取 __init_\\_()函数的参数列表 #!/usr/bin/env Python # -- coding: utf-8 -- \"\"\" @version: v0.1 @author: narutohyc @file: meta_interface.py @Description: @time: 2020/6/15 20:29 \"\"\" import collections from abc import (ABC, abstractmethod, ABCMeta) import inspect class DicMetaClass(ABCMeta): def __new__(cls, name, bases, attrs, **kwargs): if name == 'DicMeta': return super().__new__(cls, name, bases, attrs, **kwargs) # 获取__init__函数的 默认值 argspec = inspect.getfullargspec(attrs[\"__init__\"]) init_defaults = dict(zip(argspec.args[-len(argspec.defaults):], argspec.defaults)) cls.__init_defaults = init_defaults attrs['__init_defaults__'] = init_defaults attrs['__init_args__'] = argspec.args[1:] return super().__new__(cls, name, bases, attrs, **kwargs) 这里定义一个抽象类来实现__repr__方法 class DicMeta(ABC, metaclass=DicMetaClass): def __init__(self): super(DicMeta, self).__init__() @abstractmethod def to_dict(self): ''' 返回字典 ''' pass def __repr__(self): ''' __repr__() 生成的文本字符串标准做法是需要让 eval(repr(x)) == x 为真。 如果实在不能这样子做，应该创建一个有用的文本表示，并使用 括起来。 ''' return f\"{self.__class__.__name__}({', '.join(['%s=%r' % (k, type(self.__dict__[k])(self.__dict__[k])) for k in self.__init_args__ if k in self.__dict__.keys()])})\" def __str__(self): self.__repr__() @classmethod def load_from_mapping(cls, mapping_datas): ''' 用字典来构建实例对象 ''' assert isinstance(mapping_datas, collections.abc.Mapping) obj = cls.__new__(cls) [setattr(obj, k, v) for k, v in mapping_datas.items()] return obj 这样的话，继承自DicMeta的子类都有了__repr__方法的默认实现了，执行eval(repr(sample))，就可以得到一个sample了。 __unicode__(self) 定义对类的实例调用 unicode() 时的行为。 unicode() 和 str() 很像，只是它返回unicode字符串。注意，如果调用者试图调用 str() 而你的类只实现了 __unicode__() ，那么类将不能正常工作。所有你应该总是定义 __str__() ，以防有些人没有闲情雅致来使用unicode。Python 3中string和unicode的区别不复存在，因此 __unicode__ 被取消了， __bytes__ 加入进来（与Python 2.7 中的 __str__ 和 __unicode__ 行为类似），用于新的创建字节数组的内建方法。 __format__(self) 定义当类的实例用于新式字符串格式化时的行为，例如， “Hello, 0:abc!”.format(a) 会导致调用 a.__format__(“abc”) 。当定义你自己的数值类型或字符串类型时，你可能想提供某些特殊的格式化选项，这种情况下这个魔法方法会非常有用。 __hash__(self) 定义对类的实例调用 hash() 时的行为。它必须返回一个整数，其结果会被用于字典中键的快速比较。同时注意一点，实现这个魔法方法通常也需要实现 __eq__ ，并且遵守如下的规则： a == b 意味着 hash(a) == hash(b)。 __nonzero__(self) 定义对类的实例调用 bool() 时的行为，根据你自己对类的设计，针对不同的实例，这个魔法方法应该相应地返回True或False。3.x中被重命名成 __bool__ 。 bool(x) 的背后是调用 x.__bool__() 的结果；如果不存在 __bool__ 方法，那么 bool(x) 会尝试调用 x.__len__()。若返回 0，则 bool 会返回 False；否则返回 True。 __dir__(self) 定义对类的实例调用 dir() 时的行为，这个方法应该向调用者返回一个属性列表。一般来说，没必要自己实现 __dir__ 。但是如果你重定义了 __getattr__ 或者 __getattribute__ （下个部分会介绍），乃至使用动态生成的属性，以实现类的交互式使用，那么这个魔法方法是必不可少的。 访问控制 很多从其他语言转向Python的人都抱怨Python的类缺少真正意义上的封装（即没办法定义私有属性然后使用公有的getter和setter）。然而事实并非如此。实际上Python不是通过显式定义的字段和方法修改器，而是通过魔法方法实现了一系列的封装。 __getattr__(self, name) 当用户试图访问一个根本不存在（或者暂时不存在）的属性时，你可以通过这个魔法方法来定义类的行为。这个可以用于捕捉错误的拼写并且给出指引，使用废弃属性时给出警告（如果你愿意，仍然可以计算并且返回该属性），以及灵活地处理AttributeError。只有当试图访问不存在的属性时它才会被调用，所以这不能算是一个真正的封装的办法。 __setattr__(self, name, value) 和 __getattr__ 不同， __setattr__ 可以用于真正意义上的封装。它允许你自定义某个属性的赋值行为，不管这个属性存在与否，也就是说你可以对任意属性的任何变化都定义自己的规则。然后，一定要小心使用 __setattr__ ，这个列表最后的例子中会有所展示。 __delattr__(self, name) 这个魔法方法和 __setattr__ 几乎相同，只不过它是用于处理删除属性时的行为。和 setattr\\_ 一样，使用它时也需要多加小心，防止产生无限递归（在 __delattr__ 的实现中调用 del self.name 会导致无限递归）。 __getattribute__(self, name) __getattribute__ 看起来和上面那些方法很合得来，但是最好不要使用它。 __getattribute__ 只能用于新式类。在最新版的Python中所有的类都是新式类，在老版Python中你可以通过继承 object 来创建新式类。 __getattribute__ 允许你自定义属性被访问时的行为，它也同样可能遇到无限递归问题（通过调用基类的 __getattribute__ 来避免）。 __getattribute__ 基本上可以替代 __getattr__ 。只有当它被实现，并且显式地被调用，或者产生 AttributeError 时它才被使用。 这个魔法方法可以被使用（毕竟，选择权在你自己），我不推荐你使用它，因为它的使用范围相对有限（通常我们想要在赋值时进行特殊操作，而不是取值时），而且实现这个方法很容易出现Bug。 自定义这些控制属性访问的魔法方法很容易导致问题，考虑下面这个例子: def __setattr__(self, name. value): self.name = value # 因为每次属性幅值都要调用 __setattr__()，所以这里的实现会导致递归 def __setattr__(self, name, value): self.__dict__[name] = value # 使用 __dict__ 进行赋值 # 定义自定义行为 再次重申，Python的魔法方法十分强大，能力越强责任越大，了解如何正确的使用魔法方法更加重要。 到这里，我们对Python中自定义属性存取控制有了什么样的印象？它并不适合轻度的使用。实际上，它有些过分强大，而且违反直觉。然而它之所以存在，是因为一个更大的原则：Python不指望让杜绝坏事发生，而是想办法让做坏事变得困难。自由是至高无上的权利，你真的可以随心所欲。下面的例子展示了实际应用中某些特殊的属性访问方法（注意我们之所以使用 super 是因为不是所有的类都有 __dict__ 属性）: 自定义序列 有许多办法可以让你的Python类表现得像是内建序列类型（字典，元组，列表，字符串等）。这些魔法方式是目前为止我最喜欢的。它们给了你难以置信的控制能力，可以让你的类与一系列的全局函数完美结合。在Python中，协议完全是非正式的，也不需要显式的声明，事实上，它们更像是一种参考标准。 为什么我们要讲协议？因为在Python中实现自定义容器类型需要用到一些协议。首先，不可变容器类型有如下协议：想实现一个不可变容器，你需要定义 __len__ 和 __getitem__ (后面会具体说明）。可变容器的协议除了上面提到的两个方法之外，还需要定义 __setitem__ 和 __delitem__ 。最后，如果你想让你的对象可以迭代，你需要定义 __iter__ ，这个方法返回一个迭代器。迭代器必须遵守迭代器协议，需要定义 __iter__ （返回它自己）和 next 方法。 容器相关 __len__(self) 返回容器的长度，可变和不可变类型都需要实现。 __getitem__(self, key) 定义对容器中某一项使用 self[key] 的方式进行读取操作时的行为。这也是可变和不可变容器类型都需要实现的一个方法。它应该在键的类型错误式产生 TypeError 异常，同时在没有与键值相匹配的内容时产生 KeyError 异常。 __setitem__(self, key) 定义对容器中某一项使用 self[key] 的方式进行赋值操作时的行为。它是可变容器类型必须实现的一个方法，同样应该在合适的时候产生 KeyError 和 TypeError 异常。 __iter__(self, key) 它应该返回当前容器的一个迭代器。迭代器以一连串内容的形式返回，最常见的是使用 iter() 函数调用，以及在类似 for x in container: 的循环中被调用。迭代器是他们自己的对象，需要定义 __iter__ 方法并在其中返回自己。特殊方法的调用是隐式的，比如 for i in x: 这个语句，背后其实用的是 iter(x)，而这个函数的背后则是 x.__iter__() 方法。当然前提是这个方法在 x 中被实现了。 __reversed__(self) 定义了对容器使用 reversed() 内建函数时的行为。它应该返回一个反转之后的序列。当你的序列类是有序时，类似列表和元组，再实现这个方法， __contains__(self, item) __contains__ 定义了使用 in 和 not in 进行成员测试时类的行为。你可能好奇为什么这个方法不是序列协议的一部分，原因是，如果 __contains__ 没有定义，Python就会迭代整个序列，如果找到了需要的一项就返回 True 。迭代通常是隐式的，譬如说一个集合类型没有实现 __contains__ 方法，那么 in运算符就会按顺序做一次迭代搜索。 __missing__(self ,key) __missing__ 在字典的子类中使用，它定义了当试图访问一个字典中不存在的键时的行为（目前为止是指字典的实例，例如我有一个字典 d ， “george” 不是字典中的一个键，当试图访问 d[“george’] 时就会调用 d.__missing__(“george”) ）。 下面是实现了一些函数式结构的列表，展示了如何实现自己的序列： class FunctionalList: '''一个列表的封装类，实现了一些额外的函数式 方法，例如head, tail, init, last, drop和take。''' def __init__(self, values=None): if values is None: self.values = [] else: self.values = values def __len__(self): return len(self.values) def __getitem__(self, key): # 如果键的类型或值不合法，列表会返回异常 return self.values[key] def __setitem__(self, key, value): self.values[key] = value def __delitem__(self, key): del self.values[key] def __iter__(self): return iter(self.values) def __reversed__(self): return reversed(self.values) def append(self, value): self.values.append(value) def head(self): # 取得第一个元素 return self.values[0] def tail(self): # 取得除第一个元素外的所有元素 return self.valuse[1:] def init(self): # 取得除最后一个元素外的所有元素 return self.values[:-1] def last(self): # 取得最后一个元素 return self.values[-1] def drop(self, n): # 取得除前n个元素外的所有元素 return self.values[n:] def take(self, n): # 取得前n个元素 return self.values[:n] 反射 你可以通过定义魔法方法来控制用于反射的内建函数 isinstance 和 issubclass 的行为。下面是对应的魔法方法： __instancecheck__(self, instance) 检查一个实例是否是你定义的类的一个实例（例如 isinstance(instance, class) ）。 __subclasscheck__(self, subclass) 检查一个类是否是你定义的类的子类（例如 issubclass(subclass, class) ）。 这几个魔法方法的适用范围看起来有些窄，事实也正是如此。bu'yong在反射魔法方法上花费太多时间，因为相比其他魔法方法它们显得不是很重要。但是它们展示了在Python中进行面向对象编程（或者总体上使用Python进行编程）时很重要的一点：不管做什么事情，都会有一个简单方法，不管它常用不常用。 抽象基类ABC python抽象基类abc python中并没有提供抽象类与抽象方法，但是提供了内置模块abc(abstract base class)来模拟实现抽象类。 抽象基类的使用： 直接继承 直接继承抽象基类的子类就没有这么灵活，抽象基类中可以声明”抽象方法“和“抽象属性”，只有完全覆写（实现）了抽象基类中的“抽象”内容后，才能被实例化，而虚拟子类则不受此影响。 虚拟子类 将其他的类”注册“到抽象基类下当虚拟子类（调用register方法），虚拟子类的好处是你实现的第三方子类不需要直接继承自基类，可以实现抽象基类中的部分API接口，也可以根本不实现，但是issubclass(), issubinstance()进行判断时仍然返回真值。 定义抽象类 首先在abc_base.py中定义一个抽象基类PluginBase，这个基类用于保存和加载数据。 import abc class PluginBase(object): __metaclass__ = abc.ABCMeta @abc.abstractmethod def load(self, input): \"\"\"Retrieve data from the input source and return an object.\"\"\" return @abc.abstractmethod def save(self, output, data): \"\"\"Save the data object to the output.\"\"\" return 通过@abc.abstractmethod将方法声明为抽象方法。 注册具体类 然后在abc_register.py中定义一个具体的类： import abc from abc_base import PluginBase class RegisteredImplementation(object): def load(self, input): return input.read() def save(self, output, data): return output.write(data) PluginBase.register(RegisteredImplementation) if __name__ == '__main__': print 'Subclass:', issubclass(RegisteredImplementation, PluginBase) print 'Instance:', isinstance(RegisteredImplementation(), PluginBase) 在上面的例子中，RegisteredImplementation并没有继承自PluginBase，而是将其注册为PluginBase的一个实现。 运行结果如下： Subclass: True Instance: True 通过派生实现 也可以在abc_subclass.py中直接继承抽象类： import abc from abc_base import PluginBase class SubclassImplementation(PluginBase): def load(self, input): return input.read() def save(self, output, data): return output.write(data) if __name__ == '__main__': print 'Subclass:', issubclass(SubclassImplementation, PluginBase) print 'Instance:', isinstance(SubclassImplementation(), PluginBase) 这样做有一个副作用，当查询基类的子类时，会输出所有继承自改类的子类。 import abc from abc_base import PluginBase import abc_subclass import abc_register for sc in PluginBase.__subclasses__(): print sc.__name__ 输出结果如下： SubclassImplementation 直接从抽象基类派生子类有一个好处，除非子类完全抽象基类的抽象方法，否则子类不能实例化。 import abc from abc_base import PluginBase class IncompleteImplementation(PluginBase): def save(self, output, data): return output.write(data) if __name__ == '__main__': print 'Subclass:', issubclass(IncompleteImplementation, PluginBase) print 'Instance:', isinstance(IncompleteImplementation(), PluginBase) ABC中的具体方法 尽管具体子类必须实现抽象类中的所有抽象方法，但是，抽象类中也可以包含具体方法。在子类中可以通过super()来调用。 import abc from cStringIO import StringIO class ABCWithConcreteImplementation(object): __metaclass__ = abc.ABCMeta @abc.abstractmethod def retrieve_values(self, input): print 'base class reading data' return input.read() class ConcreteOverride(ABCWithConcreteImplementation): def retrieve_values(self, input): base_data = super(ConcreteOverride, self).retrieve_values(input) print 'subclass sorting data' response = sorted(base_data.splitlines()) return response input = StringIO(\"\"\"line one line two line three \"\"\") reader = ConcreteOverride() print reader.retrieve_values(input) print 输出结果如下： base class reading data subclass sorting data ['line one', 'line three', 'line two'] 抽象属性 可以通过@abstractproperty定义抽象属性： import abc class Base(object): __metaclass__ = abc.ABCMeta @abc.abstractproperty def value(self): return 'Should never get here' class Implementation(Base): @property def value(self): return 'concrete property' try: b = Base() print 'Base.value:', b.value except Exception, err: print 'ERROR:', str(err) i = Implementation() print 'Implementation.value:', i.value 输出结果如下： ERROR: Can't instantiate abstract class Base with abstract methods value Implementation.value: concrete property metaclass > 可调用的对象 Python中一个特殊的魔法方法允许你自己类的对象表现得像是函数，然后你就可以“调用”它们，把它们传递到使用函数做参数的函数中， __call__(self, [args...]) 允许类的一个实例像函数那样被调用。本质上这代表了 x() 和 x.__call__() 是相同的。注意 __call__ 可以有多个参数，这代表你可以像定义其他任何函数一样，定义 __call__ ，喜欢用多少参数就用多少。 __call__ 在某些需要经常改变状态的类的实例中显得特别有用。“调用”这个实例来改变它的状态，是一种更加符合直觉，也更加优雅的方法。一个表示平面上实体的类是一个不错的例子: class Entity: '''表示一个实体的类，调用它的实例 可以更新实体的位置''' def __init__(self, size, x, y): self.x, self.y = x, y self.size = size def __call__(self, x, y): '''改变实体的位置''' self.x, self.y = x, y 上下文管理器 在Python 2.5中引入了一个全新的关键词，随之而来的是一种新的代码复用方法—— with 声明。上下文管理的概念在Python中并不是全新引入的（之前它作为标准库的一部分实现），直到PEP 343被接受，它才成为一种一级的语言结构。 当对象使用 with 声明创建时，上下文管理器允许类做一些设置和清理工作。上下文管理器的行为由下面两个魔法方法所定义： __enter__(self) 定义使用 with 声明创建的语句块最开始上下文管理器应该做些什么。注意 __enter__ 的返回值会赋给 with 声明的目标，也就是 as 之后的东西。 __exit__(self, exception_type, exception_value, traceback) 定义当 with 声明语句块执行完毕（或终止）时上下文管理器的行为。它可以用来处理异常，进行清理，或者做其他应该在语句块结束之后立刻执行的工作。如果语句块顺利执行， exceptiontype , exception_value 和 traceback 会是 None 。否则，你可以选择处理这个异常或者让用户来处理。如果你想处理异常，确保 \\_exit__ 在完成工作之后返回 True 。如果你不想处理异常，那就让它发生吧。 对一些具有良好定义的且通用的设置和清理行为的类，__enter__ 和 __exit__ 会显得特别有用。你也可以使用这几个方法来创建通用的上下文管理器，用来包装其他对象。下面是一个例子: class Closer: ''' 一个上下文管理器，可以在with语句中 使用close()自动关闭对象 ''' def __init__(self, obj): self.obj = obj def __enter__(self, obj): return self.obj # 绑定到目标 def __exit__(self, exception_type, exception_value, traceback): try: self.obj.close() except AttributeError: # obj不是可关闭的 print 'Not closable.' return True # 成功地处理了异常 这是一个 Closer 在实际使用中的例子，使用一个FTP连接来演示（一个可关闭的socket): >>> from magicmethods import Closer >>> from ftplib import FTP >>> with Closer(FTP('ftp.somesite.com')) as conn: ... conn.dir() ... # 为了简单，省略了某些输出 >>> conn.dir() # 很长的 AttributeError 信息，不能使用一个已关闭的连接 >>> with Closer(int(5)) as i: ... i += 1 ... Not closable. >>> i 6 这就是上下文管理器和魔法方法的力量。Python标准库包含一个 contextlib 模块，里面有一个上下文管理器 contextlib.closing() 基本上和我们的包装器完成的是同样的事情（但是没有包含任何当对象没有close()方法时的处理）。 创建描述符对象 描述符是一个类，当使用取值，赋值和删除 时它可以改变其他对象。描述符不是用来单独使用的，它们需要被一个拥有者类所包含。描述符可以用来创建面向对象数据库，以及创建某些属性之间互相依赖的类。描述符在表现具有不同单位的属性，或者需要计算的属性时显得特别有用（例如表现一个坐标系中的点的类，其中的距离原点的距离这种属性）。 要想成为一个描述符，一个类必须具有实现 __get__ , __set__ 和 __delete__ 三个方法中至少一个。 让我们一起来看一看这些魔法方法： __get__(self, instance, owner) 定义当试图取出描述符的值时的行为。 instance 是拥有者类的实例， owner 是拥有者类本身。 __set__(self, instance, owner) 定义当描述符的值改变时的行为。 instance 是拥有者类的实例， value 是要赋给描述符的值。 __delete__(self, instance, owner) 定义当描述符的值被删除时的行为。 instance 是拥有者类的实例 现在，来看一个描述符的有效应用：单位转换: class Meter(object): '''米的描述符。''' def __init__(self, value=0.0): self.value = float(value) def __get__(self, instance, owner): return self.value def __set__(self, instance, owner): self.value = float(value) class Foot(object): '''英尺的描述符。''' def __get(self, instance, owner): return instance.meter * 3.2808 def __set(self, instance, value): instance.meter = float(value) / 3.2808 class Distance(object): '''用于描述距离的类，包含英尺和米两个描述符。''' meter = Meter() foot = Foot() 拷贝 有些时候，特别是处理可变对象时，你可能想拷贝一个对象，改变这个对象而不影响原有的对象。这时就需要用到Python的 copy 模块了。然而（幸运的是），Python模块并不具有感知能力， 因此我们不用担心某天基于Linux的机器人崛起。但是我们的确需要告诉Python如何有效率地拷贝对象。 __copy__(self) 定义对类的实例使用 copy.copy() 时的行为。 copy.copy() 返回一个对象的浅拷贝，这意味着拷贝出的实例是全新的，然而里面的数据全都是引用的。也就是说，对象本身是拷贝的，但是它的数据还是引用的（所以浅拷贝中的数据更改会影响原对象）。 __deepcopy__(self, memodict=) 定义对类的实例使用 copy.deepcopy() 时的行为。 copy.deepcopy() 返回一个对象的深拷贝，这个对象和它的数据全都被拷贝了一份。 memodict 是一个先前拷贝对象的缓存，它优化了拷贝过程，而且可以防止拷贝递归数据结构时产生无限递归。当你想深拷贝一个单独的属性时，在那个属性上调用 copy.deepcopy() ，使用 memodict 作为第一个参数。 这些魔法方法有什么用武之地呢？像往常一样，当你需要比默认行为更加精确的控制时。例如，如果你想拷贝一个对象，其中存储了一个字典作为缓存（可能会很大），拷贝缓存可能是没有意义的。如果这个缓存可以在内存中被不同实例共享，那么它就应该被共享。 Pickling 如果你和其他的Python爱好者共事过，很可能你已经听说过Pickling了。Pickling是Python数据结构的序列化过程，当你想存储一个对象稍后再取出读取时，Pickling会显得十分有用。 Pickling是如此的重要，以至于它不仅仅有自己的模块（ pickle ），还有自己的协议和魔法方法。首先，我们先来简要的介绍一下如何pickle已存在的对象类型。 Pickling : 小试牛刀 我们一起来pickle吧。假设你有一个字典，你想存储它，稍后再取出来。你可以把它的内容写入一个文件，小心翼翼地确保使用了正确地格式，要把它读取出来，你可以使用 exec() 或处理文件输入。但是这种方法并不可靠：如果你使用纯文本来存储重要数据，数据很容易以多种方式被破坏或者修改，导致你的程序崩溃，更糟糕的情况下，还可能在你的计算机上运行恶意代码。因此，我们要pickle它: import pickle data = {'foo': [1,2,3], 'bar': ('Hello', 'world!'), 'baz': True} jar = open('data.pkl', 'wb') pickle.dump(data, jar) # 将pickle后的数据写入jar文件 jar.close() 过了几个小时，我们想把它取出来，我们只需要反pickle它: import pickle pkl_file = open('data.pkl', 'rb') # 与pickle后的数据连接 data = pickle.load(pkl_file) # 把它加载进一个变量 print data pkl_file.close() 将会发生什么？正如你期待的，它就是我们之前的 data 。 现在，还需要谨慎地说一句： pickle并不完美。Pickle文件很容易因为事故或被故意的破坏掉。Pickling或许比纯文本文件安全一些，但是依然有可能被用来运行恶意代码。而且它还不支持跨Python版本，所以不要指望分发pickle对象之后所有人都能正确地读取。然而不管怎么样，它依然是一个强有力的工具，可以用于缓存和其他类型的持久化工作。 Pickle你的对象 Pickle不仅仅可以用于内建类型，任何遵守pickle协议的类都可以被pickle。Pickle协议有四个可选方法，可以让类自定义它们的行为（这和C语言扩展略有不同，那不在我们的讨论范围之内）。 __getinitargs__(self) 如果你想让你的类在反pickle时调用 __init__ ，你可以定义 __getinitargs__(self) ，它会返回一个参数元组，这个元组会传递给 __init__ 。注意，这个方法只能用于旧式类。 __getnewargs__(self) 对新式类来说，你可以通过这个方法改变类在反pickle时传递给 __new__ 的参数。这个方法应该返回一个参数元组。 __getstate__(self) 你可以自定义对象被pickle时被存储的状态，而不使用对象的 __dict__ 属性。 这个状态在对象被反pickle时会被 __setstate__ 使用。 __setstate__(self) 当一个对象被反pickle时，如果定义了 __setstate__ ，对象的状态会传递给这个魔法方法，而不是直接应用到对象的 __dict__ 属性。这个魔法方法和 __getstate__ 相互依存：当这两个方法都被定义时，你可以在Pickle时使用任何方法保存对象的任何状态。 __reduce__(self) 当定义扩展类型时（也就是使用Python的C语言API实现的类型），如果你想pickle它们，你必须告诉Python如何pickle它们。 __reduce__ 被定义之后，当对象被Pickle时就会被调用。它要么返回一个代表全局名称的字符串，Pyhton会查找它并pickle，要么返回一个元组。这个元组包含2到5个元素，其中包括：一个可调用的对象，用于重建对象时调用；一个参数元素，供那个可调用对象使用；被传递给 __setstate__ 的状态（可选）；一个产生被pickle的列表元素的迭代器（可选）；一个产生被pickle的字典元素的迭代器（可选）； __reduceex\\_(self) __reduceex\\_ 的存在是为了兼容性。如果它被定义，在pickle时 __reduceex\\_ 会代替 __reduce__ 被调用。 __reduce__ 也可以被定义，用于不支持 __reduceex\\_ 的旧版pickle的API调用。 一个例子 Slate 它会记住它的值曾经是什么，以及那些值是什么时候赋给它的。然而 每次被pickle时它都会变成空白，因为当前的值不会被存储: import time class Slate: '''存储一个字符串和一个变更日志的类 每次被pickle都会忘记它当前的值''' def __init__(self, value): self.value = value self.last_change = time.asctime() self.history = {} def change(self, new_value): # 改变当前值，将上一个值记录到历史 self.history[self.last_change] = self.value self.value = new_value) self.last_change = time.asctime() def print_change(self): print 'Changelog for Slate object:' for k,v in self.history.items(): print '%s\\t %s' % (k,v) def __getstate__(self): # 故意不返回self.value或self.last_change # 我们想在反pickle时得到一个空白的slate return self.history def __setstate__(self): # 使self.history = slate，last_change # 和value为未定义 self.history = state self.value, self.last_change = None, None 如何调用魔法方法 一些魔法方法直接和内建函数对应，这种情况下，如何调用它们是显而易见的。然而，另外的情况下，调用魔法方法的途径并不是那么明显。这个附录旨在展示那些不那么明显的调用魔法方法的语法。 魔法方法 什么时候被调用 解释 __new__(cls [,...]) instance = MyClass(arg1, arg2) __new__在实例创建时调用 __init__(self [,...]) instance = MyClass(arg1,arg2) __init__在实例创建时调用 __cmp__(self) self == other, self > other 等 进行比较时调用，3.x移除 __pos__(self) +self 一元加法符号 __neg__(self) -self 一元减法符号 __invert__(self) ~self 按位取反 __index__(self) x[self] 当对象用于索引时 __nonzero__(self) bool(self) 对象的布尔值 __getattr__(self, name) self.name #name不存在 访问不存在的属性 __setattr__(self, name) self.name = val 给属性赋值 __delattr__(self, name) del self.name 删除属性 __getattribute__(self,name) self.name 访问任意属性 __getitem__(self, key) self[key] 使用索引访问某个元素 __setitem__(self, key) self[key] = val 使用索引给某个元素赋值 __delitem__(self, key) del self[key] 使用索引删除某个对象 __iter__(self) for x in self 迭代 __contains__(self, value) value in self, value not in self 使用in进行成员测试 __call__(self [,...]) self(args) “调用”一个实例 __enter__(self) with self as x: with声明的上下文管理器 __exit__(self, exc, val, trace) with self as x: with声明的上下文管理器 __getstate__(self) pickle.dump(pkl_file, self) Pickling __setstate__(self) data = pickle.load(pkl_file) Pickling Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-12-01 00:57:11 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/10.数据库相关操作.html":{"url":"chapters/10.数据库相关操作.html","title":"数据库相关操作","keywords":"","body":"Elasticsearch数据库环境配置EsDao包装类使用案例Oracle数据库环境配置sql基础建表查询相关操作Postgresql数据库离线安装数据库环境配置sql语法数据库连接数据库信息数据备份与恢复表空间锁表处理表结构修改数据更新和查询数据和结构复制视图分页查询删除重复记录索引实用sql其他语法ORM框架ORM框架比较SQLAlchemysession和scoped_session几种操作方式 Elasticsearch数据库 环境配置 安装环境 pip install elasticsearch==7.6.0 EsDao包装类 # -- coding: utf-8 -- \"\"\" @version: v1.0 @author: huangyc @file: EsDao.py @Description: Es统一操作类 @time: 2020/4/27 10:22 \"\"\" from elasticsearch.helpers import bulk from elasticsearch import Elasticsearch import pandas as pd class EsDao(object): \"\"\" ElasticSearch的数据操作类 \"\"\" # 查询批次大小 DEFAULT_BATCH_SIZE = 1000 # 写入批次大小 BULK_BATCH_SIZE = 10000 def __init__(self, hosts, timeout=3600*24): self.hosts = hosts self.timeout = timeout self.es = Elasticsearch(hosts, timeout=self.timeout) def save_data_list(self, index_name, data_list): \"\"\" 保存数据列表到es的指定索引中 :param index_name: 索引名称 :param data_list: 数据列表，列表元素代表一行数据，元素类型为dict :return: \"\"\" bulk_data_lst = [ data_list[i:i + self.BULK_BATCH_SIZE] for i in range(0, len(data_list), self.BULK_BATCH_SIZE) ] if len(data_list) > 0 and '_id' in data_list[0]: for bulk_data in bulk_data_lst: actions = [{ \"_index\": index_name, \"_type\": index_name, \"_id\": data.pop(\"_id\"), \"_source\": data } for data in bulk_data ] bulk(self.es, actions, index=index_name, raise_on_error=True) else: for bulk_data in bulk_data_lst: actions = [{ \"_index\": index_name, \"_type\": index_name, \"_source\": data } for data in bulk_data ] bulk(self.es, actions, index=index_name, raise_on_error=True) def is_index_exists(self, index_name): \"\"\" 判断指定索引是否存在 :param index_name: 索引名称 :return: \"\"\" return self.es.indices.exists(index=index_name) def delete_by_query(self, index_name, query_body): \"\"\" 按查询结果删除数据 :param index_name: :param query_body: :return: \"\"\" return self.es.delete_by_query(index_name, query_body) def clear_index_data(self, index_name): \"\"\" 清空指定索引的数据 :param index_name: :return: \"\"\" return self.delete_by_query( index_name=index_name, query_body={ \"query\": { \"match_all\": {} } } ) def save_df_data(self, index_name, df): \"\"\" 保存pandas的DataFrame到es的指定索引中 :param index_name: 索引名称 :param df: 要保存的dataframe :return: \"\"\" col_lst = df.columns.tolist() dic_lst = [dict([(c, v) for c, v in zip(col_lst, r)]) for r in df.values.tolist()] self.save_data_list(index_name=index_name, data_list=dic_lst) def create_index(self, index_name, mapping_properties): \"\"\" 创建索引 :param index_name: 索引名称 :param mapping_properties: 索引mapping中的属性列表 :return: \"\"\" if not self.es.indices.exists(index=index_name): mapping = { \"mappings\": { index_name: { \"properties\": mapping_properties } } } res = self.es.indices.create(index=index_name, body=mapping) if res is not None and 'acknowledged' in res: return res.get('acknowledged') return False def _search_with_scroll(self, index_name, query_body): if \"size\" not in query_body: query_body[\"size\"] = self.DEFAULT_BATCH_SIZE response = self.es.search( index=index_name, body=query_body, search_type=\"dfs_query_then_fetch\", scroll=\"120m\", timeout=\"60m\" ) scroll_id = response[\"_scroll_id\"] while True: sources = [doc[\"_source\"] for doc in response[\"hits\"][\"hits\"]] if len(sources) == 0: break yield sources response = self.es.scroll(scroll_id=scroll_id, scroll=\"60m\") def query_for_df(self, index_name, query_body): \"\"\" 执行查询并获取pandas.DataFrame格式的返回值 :param index_name: 索引名称 :param query_body: 查询条件 :return: \"\"\" sources = [] for sub_source in self._search_with_scroll(index_name=index_name, query_body=query_body): sources.extend(sub_source) return pd.DataFrame(sources) def query_for_df_with_batch(self, index_name, query_body, batch_size=DEFAULT_BATCH_SIZE): \"\"\" 按批次大小查询并返回pandas.DataFrame的generator格式的返回值 :param index_name: 索引名称 :param query_body: 查询条件 :param batch_size: 批次大小 :return: \"\"\" if \"size\" not in query_body: query_body[\"size\"] = batch_size for sub_source in self._search_with_scroll(index_name=index_name, query_body=query_body): yield pd.DataFrame(sub_source) def get_first_row_with_df(self, index_name): \"\"\" 获取指定索引的首行数据，格式为pandas.DataFrame 可用于获取索引的元信息 :param index_name: 索引名称 :return: \"\"\" query_body = { \"size\": 1, \"query\": { \"match_all\": {} } } for sub_source in self._search_with_scroll(index_name=index_name, query_body=query_body): return pd.DataFrame(sub_source) 使用案例 class TaskMeta: ''' 数据元类 ''' def __init__(self, text, doc_id, sentence_id, reg_lst, flag, has_reg, text_source=\"primitive\"): self.text = text self.doc_id = doc_id self.sentence_id = sentence_id self.reg_lst = reg_lst self.flag = flag self.has_reg = has_reg self.text_source = text_source def __repr__(self): return f'{self.text} {self.doc_id} {self.sentence_id} {self.reg_lst} {self.flag} {self.has_reg} {self.text_source}' def to_dict(self): return {\"text\": self.text, \"doc_id\": self.doc_id, \"sentence_id\": self.sentence_id, \"reg_lst\": self.reg_lst, \"flag\": self.flag, \"has_reg\": self.has_reg, \"text_source\": self.text_source} def create_index(target_es_dao, index_name, mapping): ''' 创建es索引 :return: 是否创建成功 ''' if not target_es_dao.is_index_exists(index_name): target_es_dao.create_index(index_name, mapping) else: target_es_dao.clear_index_data(index_name) print(f\"索引{index_name}已存在, 已清除数据\") def writer_fun(target_es_dao, target_index, sample_lst): ''' 写数据到es库 ''' df_sample_lst = [] [df_sample_lst.append(sample.to_dict()) for sample in sample_lst] df_sample_lst = pd.DataFrame(df_sample_lst) target_es_dao.save_df_data(target_index, df_sample_lst) print(f'写入数据{len(sample_lst)}条') def es_cal_test(): # 获取连接 source_es_dao = EsDao(f\"http://{aug_config.SOURCE_IP}:{aug_config.SOURCE_PORT}/\") query_condition = { \"query_string\": { \"default_field\": \"has_reg\", \"query\": \"true\" } } query_body = { \"query\": query_condition } # 查询数据 datas = source_es_dao.query_for_df(index_name=aug_config.SOURCE_INDEX, query_body=query_body) records = datas.to_dict(orient='record') sample_lst = [] for record in records: sample_lst.append( TaskMeta( text=record[\"text\"], doc_id=record[\"doc_id\"], sentence_id=record[\"sentence_id\"], reg_lst=record[\"reg_lst\"], flag=record[\"flag\"], has_reg=record[\"has_reg\"] ) ) # 创建索引 create_index(target_es_dao, aug_config.TARGET_INDEX, aug_config.MAPPING) # 写入数据 writer_fun(target_es_dao, aug_config.TARGET_INDEX, sample_lst=sample_lst) if __name__ == '__main__': es_cal_test() Oracle数据库 Python操作Oracle数据库：cx_Oracle 环境配置 Linux上Python连接Oracle解决报错cx_Oracle.DatabaseError: DPI-1047 安装库 pip install cx-Oracle 链接库准备，需要将oci.dll、oraocci11.dll、oraociei11.dll复制到sitepackages路径下，oracle client下载链接，并配置到系统环境变量，链接中没有的自己去官网（win64、所有平台、linux64）注册一个账号下载对应的版本 -- 查看oracle版本 SELECT * FROM v$version; 没有配置会报如下的错： # Windows下报错 cx_Oracle.DatabaseError: DPI-1047: Cannot locate a 64-bit Oracle Client library: \"D:\\software\\win_or # Linux下报错 cx_Oracle.DatabaseError: DPI-1047: Cannot locate a 64-bit Oracle Client library: \"libclntsh.so: cannot open shared object file: No such file or directory\". See https://cx-oracle.readthedocs.io/en/latest/user_guide/installation.html for help windows下安装完客户端后，配置oracle客户端的环境变量 D:\\software\\win_oracle_dlls\\instantclient_11_2 linux下可以使用rpm安装包安装 sudo rpm -ivh oracle-instantclient11.2-basic-11.2.0.4.0-1.x86_64.rpm 然后将环境变量配置到/etc/profile # 配置oracle客户端 export ORACLE_CLIENT_HOME=/lib/oracle/11.2/client64 export TNS_ADMIN=$ORACLE_CLIENT_HOME export LD_LIBRARY_PATH=$ORACLE_CLIENT_HOME/lib export ORABIN=$ORACLE_CLIENT_HOME/bin PATH=$PATH:$ORABIN export PATH export PATH=$ORACLE_HOME:$PATH export PATH=$PATH:$HOME/bin:$ORACLE_CLIENT_HOME/bin 其他类似找不到libclntsh.sod的错误，如果出现这个错误，请进行软连接挂载文件，让系统的路径能正确的获取到该文件，操作如下： sudo sh -c \"/usr/lib/oracle/instantclient_11_1 > /etc/ld.so.conf.d/oracle-instantclient.conf\" sudo ldconfig sql基础 建表 --blob字段插入实例 create table blob_table_tmp( id number primary key, blob_cl blob not null, clob_cl clob not null ); insert into blob_table_tmp values(1,rawtohex('11111000011111'),'增加一条记录时，碰到插入blob类型数据出错'); insert into blob_table_tmp values(3,rawtohex('4561888'),'增加一条记录时，碰到插入blob类型数据出错'); insert into blob_table_tmp values(4,rawtohex('增加一条记录时333'),'增加一条记录时，碰到插入blob类型数据出错'); 查询 获取连接 FINANCE_DB_HOST = \"192.168.x.x\" FINANCE_DB_PORT = 1521 FINANCE_DB_USER = \"hyc\" FINANCE_DB_PASSWORD = \"123456\" FINANCE_DB_DB = \"ORCL\" class OracleConn(): config_path = '' @staticmethod def get_conn(conn_name, encoding=\"UTF-8\"): conn_str = str(eval(\"%s_DB_USER\" % (OracleConn.config_path, conn_name))) + \"/\" + str(eval(\"%s.%s_DB_PASSWORD\" % (OracleConn.config_path, conn_name))) conn_str += \"@\" + str(eval(\"%s_DB_HOST\" % (OracleConn.config_path, conn_name))) conn_str += \":\" + str(eval(\"%s_DB_PORT\" % (OracleConn.config_path, conn_name))) conn_str += \"/\" + str(eval(\"%s_DB_DB\" % (OracleConn.config_path, conn_name))) return ora.connect(conn_str, encoding=encoding, nencoding=encoding) 读写数据库 def oracle_test(): # 获取数据库连接 conn = OracleConn.get_conn(\"FINANCE\") cur = conn.cursor() # 查询数据 sql = \"select id,blob_cl,clob_cl from FINANCE.blob_table_tmp\" datas = [] r = cur.execute(sql) # 假设name是clob字段类型 [datas.append((gg[0], gg[1].read().decode('utf-8'), gg[2].read())) for gg in r] # 写入数据 insert_sql = \"INSERT INTO new_table(id,new_name) VALUES (:ID,:NEW_NAME)\" res = [] [res.append((data[0], data[1])) for data in datas] cur.executemany(insert_sql, res) cur.execute('commit') cur.close() conn.close() print(\"写入结束\") if __name__ == '__main__': oracle_test() 相关操作 关于数据库的连接，查询和写入 import cx_Oracle class Setting: DB_USER = 'narutohyc' DB_PASSWORD = 'hyc' DB_IP = '192.168.0.1' DB_PORT = '' DB_SERVICE = 'dataBaseName' setting = Setting() def oracle_test(): # 获取数据库连接 conn = cx_Oracle.connect('%s/%s@%s/%s' % (setting.DB_USER, setting.DB_PASSWORD, setting.DB_IP, setting.DB_SERVICE), encoding='utf-8') cur = conn.cursor() # 查询数据 sql = \"select ID, name from hyc_database\" datas = [] r = cur.execute(sql) # 假设name是clob字段类型 [datas.append((gg[0], gg[1].read())) for gg in r] # 写入数据 insert_sql = \"INSERT INTO new_table(id,new_name) VALUES (:ID,:NEW_NAME)\" res = [] [res.append((data[0], data[1])) for data in datas] cur.executemany(insert_sql, res) cur.execute('commit') cur.close() conn.close() print(\"写入结束\") if __name__ == '__main__': oracle_test() Postgresql数据库 官方文档Documentation → PostgreSQL 16 查询 数据类型 我终于学会了使用python操作postgresql 保姆级 CentOS 7离线安装PostgreSQL 14教程 易百_PostgreSQL教程 离线安装数据库 先从centos7-pg_14.2下载下载rpm包(微云下载centos7.6_PostgreSQL14.2)，或者直接官方下载安装教程安装，如果离线安装就下载rpm包 # 离线安装执行以下命令安装 rpm -ivh postgresql14-libs-14.2-1PGDG.rhel7.x86_64.rpm rpm -ivh postgresql14-14.2-1PGDG.rhel7.x86_64.rpm rpm -ivh postgresql14-server-14.2-1PGDG.rhel7.x86_64.rpm rpm -ivh postgresql14-contrib-14.2-1PGDG.rhel7.x86_64.rpm 出现OSError: Python library not found: libpython3.6mu.so.1.0, libpython3.6m.so.1.0, libpython3.6.so.1.0, libpython3.6m.so的解决办法 yum install python3-devel 创建数据库data和log文件夹 # 创建数据库data和log文件夹 mkdir -p /home/postgres/pgsql_data mkdir -p /home/postgres/pgsql_log # 创建日志文件 touch /home/postgres/pgsql_log/pgsql.log 授权给安装数据时自动创建的postgres用户 chown -R postgres:postgres /home/postgres/pgsql_data chown -R postgres:postgres /home/postgres/pgsql_log 切换到安装数据时自动创建的postgres用户 su - postgres 初始化数据库到新建数据目录 /usr/pgsql-14/bin/initdb -D /home/postgres/pgsql_data 启动服务器(初始化数据库日志文件) /usr/pgsql-14/bin/pg_ctl -D /home/postgres/pgsql_data/ -l /home/postgres/pgsql_log/pgsql.log start # 查看状态 /usr/pgsql-14/bin/pg_ctl -D /home/postgres/pgsql_data/ -l /home/postgres/pgsql_log/pgsql.log status 切换到管理员开启端口并重启防火墙 su root firewall-cmd --zone=public --add-port=5432/tcp --permanent firewall-cmd --reload 修改配置文件实现远程访问vi /home/postgres/pgsql_data/postgresql.conf # 修改监听地址 listen_addresses = '*' # 修改最大连接数（按需） max_connections = 1000 # 修改密码认证 password_encryption = md5 修改可访问的用户IP段 vi /home/pgsql_data/pg_hba.conf（a进入编辑模式，esc退出编辑模式，:wq并按回车保存） IPV4下修改为或新增 host all all 0.0.0.0/0 trust postgres用户重启数据库服务 su - postgres /usr/pgsql-14/bin/pg_ctl -D /home/postgres/pgsql_data/ -l /home/postgres/pgsql_log/pgsql.log restart 数据库安装结束，管理员postgres，默认密码123456 使用navicat连接pg库后新建数据库 环境配置 pip install psycopg2 sql语法 数据库连接 -- 获取数据库实例连接数 select count(*) from pg_stat_activity; -- 获取数据库最大连接数 show max_connections; -- 查询当前连接数详细信息 select * from pg_stat_activity; -- 查询数据库中各个用户名对应的数据库连接数 select usename, count(*) from pg_stat_activity group by usename; 数据库信息 -- 查询数据库大小 select pg_size_pretty (pg_database_size('pg_fac_stk')); -- 查询各表磁盘占用 SELECT table_schema || '.' || table_name AS table_full_name, pg_size_pretty(pg_total_relation_size('\"' || table_schema || '\".\"' || table_name || '\"')) AS size FROM information_schema.tables where table_name like 'finance_%' ORDER BY pg_total_relation_size('\"' || table_schema || '\".\"' || table_name || '\"') DESC; -- 获取各个表中的数据记录数 select relname as TABLE_NAME, reltuples as rowCounts from pg_class where relkind = 'r' order by rowCounts desc; -- 查看数据库表对应的数据文件 select pg_relation_filepath('product'); -- 查看数据库实例的版本 select version(); -- 分析评估SQL执行情况 EXPLAIN ANALYZE SELECT * FROM t_cfg_opinfo; -- 获取数据库当前的回滚事务数以及死锁数 select datname,xact_rollback,deadlocks from pg_stat_database; 数据备份与恢复 使用pgdump备份数据库 pgdump是PostgreSQL官方提供的备份工具，可以将数据库的数据和架构保存到一个文件中，使用pgdump备份的优点包括： 备份数据可以保持原有的结构和特性，还原时可以保证数据准确性 备份文件可以跨平台传输，方便进行远程备份 备份文件可以进行压缩，减小文件大小，方便传输和存储 可以新建数据库，建几张表做测试 # 学生表 CREATE TABLE students ( id SERIAL PRIMARY KEY, name VARCHAR(100) NOT NULL, gender VARCHAR(10) NOT NULL, age INTEGER NOT NULL, class VARCHAR(20) NOT NULL ); # 学科表 CREATE TABLE subjects ( id SERIAL PRIMARY KEY, name VARCHAR(100) NOT NULL ); # 成绩表 CREATE TABLE scores ( id SERIAL PRIMARY KEY, student_id INTEGER NOT NULL, subject_id INTEGER NOT NULL, score INTEGER NOT NULL, FOREIGN KEY (student_id) REFERENCES students (id), FOREIGN KEY (subject_id) REFERENCES subjects (id) ); # 插入一些测试数据 INSERT INTO students (name, gender, age, class) VALUES ('Alice', 'Female', 18, 'Class A'), ('Bob', 'Male', 17, 'Class B'), ('Charlie', 'Male', 19, 'Class A'), ('Diana', 'Female', 18, 'Class B'); # 插入学科表数据 INSERT INTO subjects (name) VALUES ('Mathematics'), ('English'), ('Science'); -- Alice 的成绩 INSERT INTO scores (student_id, subject_id, score) VALUES (1, 1, 90), (1, 2, 85), (1, 3, 92); -- Bob 的成绩 INSERT INTO scores (student_id, subject_id, score) VALUES (2, 1, 78), (2, 2, 80), (2, 3, 75); -- Charlie 的成绩 INSERT INTO scores (student_id, subject_id, score) VALUES (3, 1, 88), (3, 2, 92), (3, 3, 90); -- Diana 的成绩 INSERT INTO scores (student_id, subject_id, score) VALUES (4, 1, 95), (4, 2, 88), (4, 3, 92); 备份 使用pgdump备份数据库非常简单，只需要在终端中输入相应的命令即可 备份整个数据库 pg_dump -h -p -U -F c -b -v -f # 示例 /usr/pgsql-14/bin/pg_dump -h 127.0.0.1 -U postgres -p 5432 -F t -b -v -f build_hyc_test.sql.tar hyc_test 备份指定表或数据 pg_dump -h -p -U -F c -b -v -t -t -f # 示例 -- 备份指定表到sql文件 -- '-c --if-exists' 会生成 'drop table if exist' 命令 -- '--no-owner' 是一个选项，用于指定在导出数据库时不包括拥有者信息 pg_dump --verbose --host=192.168.xx.xx --port=5432 --username=postgres --file /home/huangyc/pg_bak_test/bak_hyc.sql --encoding=UTF-8 -t \"public.tushare_wz_index\" -t \"public.tushare_us_basic\" -t \"public.dim_fund\" -t \"public.dim_index\" -c --if-exists --no-owner pg_fac_stk 具体参数的含义如下： -h：数据库服务所在主机地址，可以是本地地址localhost或者IP地址 -p：数据库服务的监听端口，一般为默认端口5432 -U：连接数据库的用户名 -F：备份文件的格式，包括自定义格式c，纯文本格式p和归档格式t -b：在备份文件中包含备份的数据库的模式信息 -v：备份过程中输出详细的信息 -f：备份文件的保存路径和文件名 -t：只备份指定的表和数据 -- 备份postgres库并tar打包 pg_dump -h 127.0.0.1 -p 5432 -U postgres -f postgres.sql.tar -Ft; -- 备份postgres库，转储数据为带列名的INSERT命令 pg_dumpall -d postgres -U postgres -f postgres.sql --column-inserts; 还原 使用备份文件进行恢复也非常简单，只需要在终端中输入相应的命令即可 恢复整个库 pg_restore -h -p -U -d # 示例 /usr/pgsql-14/bin/pg_restore -h 127.0.0.1 -U postgres -p 5432 -d hyc_test_bak build_hyc_test.sql.tar 恢复指定数据 pg_restore -h -p -U -t -t -d # 示例 -- 对于pg_dump备份出来的sql文件，直接执行sql文件即可恢复 -- 还原指定sql文件到bak_test库(需要自己建库) psql --host=192.168.xx.xx --port=5432 --username=postgres -d bak_test --file /home/huangyc/pg_bak_test/bak_hyc.sql.tar 具体参数的含义如下： -h：数据库服务所在主机地址，可以是本地地址localhost或者IP地址 -p：数据库服务的监听端口，一般为默认端口5432 -U：连接数据库的用户名 -d：恢复数据的目标数据库名称 -t：只恢复指定的表和数据 命令详解 [postgres@pg01 ~]$ pg_dump --help 用法: pg_dump [选项]... [数据库名字] **一般选项**: -f, --file=FILENAME 输出文件或目录名 -F, --format=c|d|t|p 输出文件格式 (c=custom, d=directory, t=tar,p=plain,plain就是sql纯文本 (默认值)) -j, --jobs=NUM 执行多个并行任务进行备份转储工作 -v, --verbose 详细模式 -V, --version 输出版本信息，然后退出 -Z, --compress=0-9 被压缩格式的压缩级别，0表示不压缩 --lock-wait-timeout=TIMEOUT 在等待表锁超时后操作失败 --no-sync 不用等待变化安全写入磁盘 -?, --help 显示此帮助, 然后退出 **控制输出内容选项(常用)**: -a, --data-only 只转储数据,不包括模式,只对纯文本输出有意义 -s, --schema-only 只转储模式, 不包括数据 -c, --clean 在重新创建之前，先清除（删除）数据库对象，如drop table。只对纯文本输出有意义 -C, --create 指定输出文件中是否生成create database语句,只对纯文本输出有意义 -n, --schema=PATTERN 指定要导出的schema，不指定则导出所有的非系统schema -N, --exclude-schema=PATTERN 排除导出哪些schema -O, --no-owner 在明文格式中, 忽略恢复对象所属者 -t, --table=PATTERN 指定导出的表、视图、序列，可以使用多个-t匹配多个表，使用-t之后，-n和-N就失效了 -T, --exclude-table=PATTERN 排除表 -x, --no-privileges 不要转储权限 (grant/revoke) --disable-triggers 在只恢复数据的过程中禁用触发器 --exclude-table-data=PATTERN do NOT dump data for the specified table(s) --if-exists 当删除对象时使用IF EXISTS --inserts 以INSERT命令，而不是COPY命令的形式转储数据，使用该选项可以把数据加载到非pg数据库，会使恢复非常慢 该选项为每行生成1个单独的insert命令，?在恢复过程中遇到错误，将会丢失1行而不是全部表数据 --column-inserts 以带有列名的INSERT命令形式转储数据，例如insert into table_name(column,...) values(value1,...) --load-via-partition-root 通过根表加载分区 --no-comments 不转储注释 --no-tablespaces 不转储表空间分配信息 --no-unlogged-table-data 不转储没有日志的表数据 --on-conflict-do-nothing 将ON CONFLICT DO NOTHING添加到INSERT命令 **控制输出内容选项(不常用)**: -S, --superuser=NAME 指定关闭触发器时需要用到的超级用户名。 它只有在使用了--disable-triggers时才有影响。一般情况下，最好不要输入该参数，而是用 超级用户启动生成的脚本。 -b, --blobs 在转储中包括大对象 -B, --no-blobs 排除转储中的大型对象 -E, --encoding=ENCODING 转储以ENCODING形式编码的数据 --binary-upgrade 只能由升级工具使用 --enable-row-security 启用行安全性（只转储用户能够访问的内容） --extra-float-digits=NUM 覆盖extra_float_digits的默认设置 --disable-dollar-quoting 取消美元 (符号) 引号, 使用 SQL 标准引号 --no-publications 不转储发布 --no-security-labels 不转储安全标签的分配 --no-subscriptions 不转储订阅 --no-synchronized-snapshots 在并行工作集中不使用同步快照 --quote-all-identifiers 所有标识符加引号，即使不是关键字 --rows-per-insert=NROWS 每个插入的行数；意味着--inserts --section=SECTION 备份命名的节 (数据前, 数据, 及 数据后) --serializable-deferrable 等到备份可以无异常运行 --snapshot=SNAPSHOT 为转储使用给定的快照 --strict-names 要求每个表和(或)schema包括模式以匹配至少一个实体 --use-set-session-authorization 使用 SESSION AUTHORIZATION 命令代替 ALTER OWNER 命令来设置所有权 **联接选项**: -d, --dbname=DBNAME 对数据库 DBNAME备份 -h, --host=主机名 数据库服务器的主机名或套接字目录 -p, --port=端口号 数据库服务器的端口号 -U, --username=名字 以指定的数据库用户联接 -w, --no-password 永远不提示输入口令 -W, --password 强制口令提示 (自动) --role=ROLENAME 在转储前运行SET ROLE 对于pg_dump的自定义备份custom和tar类型的备份，需要使用pg_restore进行恢复，pg_restore语法如下 [postgres@pg01 pg_backup]$ pg_restore --help pg_restore 从一个归档中恢复一个由 pg_dump 创建的 PostgreSQL 数据库. 用法: pg_restore [选项]... [文件名] 一般选项: -d, --dbname=名字 连接数据库名字 -f, --file=文件名 输出文件名(- 对于stdout) -F, --format=c|d|t 备份文件格式(应该自动进行) -l, --list 打印归档文件的 TOC 概述 -v, --verbose 详细模式 -V, --version 输出版本信息, 然后退出 -?, --help 显示此帮助, 然后退出 恢复控制选项: -a, --data-only 只恢复数据, 不包括模式 -c, --clean 在重新创建之前，先清除（删除）数据库对象 -C, --create 创建目标数据库 -e, --exit-on-error 发生错误退出, 默认为继续 -I, --index=NAME 恢复指定名称的索引 -j, --jobs=NUM 执行多个并行任务进行恢复工作 -L, --use-list=FILENAME 从这个文件中使用指定的内容表排序 输出 -n, --schema=NAME 在这个模式中只恢复对象 -N, --exclude-schema=NAME 不恢复此模式中的对象 -O, --no-owner 不恢复对象所属者 -P, --function=NAME(args) 恢复指定名字的函数 -s, --schema-only 只恢复模式, 不包括数据 -S, --superuser=NAME 使用指定的超级用户来禁用触发器 -t, --table=NAME 恢复命名关系（表、视图等） -T, --trigger=NAME 恢复指定名字的触发器 -x, --no-privileges 跳过处理权限的恢复 (grant/revoke) -1, --single-transaction 作为单个事务恢复 --disable-triggers 在只恢复数据的过程中禁用触发器 --enable-row-security 启用行安全性 --if-exists 当删除对象时使用IF EXISTS --no-comments 不恢复注释 --no-data-for-failed-tables 对那些无法创建的表不进行 数据恢复 --no-publications 不恢复发行 --no-security-labels 不恢复安全标签信息 --no-subscriptions 不恢复订阅 --no-tablespaces 不恢复表空间的分配信息 --section=SECTION 恢复命名节 (数据前、数据及数据后) --strict-names 要求每个表和(或)schema包括模式以匹配至少一个实体 --use-set-session-authorization 使用 SESSION AUTHORIZATION 命令代替 ALTER OWNER 命令来设置所有权 联接选项: -h, --host=主机名 数据库服务器的主机名或套接字目录 -p, --port=端口号 数据库服务器的端口号 -U, --username=名字 以指定的数据库用户联接 -w, --no-password 永远不提示输入口令 -W, --password 强制口令提示 (自动) --role=ROLENAME 在恢复前执行SET ROLE操作 选项 -I, -n, -N, -P, -t, -T, 以及 --section 可以组合使用和指定 多次用于选择多个对象. 如果没有提供输入文件名, 则使用标准输入. 表空间 新建表空间 # 新建表空间目录 t_fac_ts mkdir /home/huangyc/t_fac_ts # 修改表空间的用户权限 chown postgres /home/huangyc/t_fac_ts pg库新建表空间 create tablespace t_fac_ts owner postgres location '/home/huangyc/t_fac_ts'; 表空间有关的一些语法 # 删除表空间 (需要先drop表空间所有的表, 或者将该空间下所有的表移除才能drop表空间) DROP TABLESPACE t_fac_ts; # 修改具体的表到指定表空间下 ALTER TABLE t_fac_tushare_stock_basic SET TABLESPACE t_fac_ts; # 修改指定库到指定表空间下 ALTER DATABASE name SET TABLESPACE new_tablespace; 锁表处理 pg锁表解锁 查看被锁的表 select a.locktype,a.database,a.pid,a.mode,a.relation,b.relname from pg_locks a join pg_class b on a.relation = b.oid where relname='t_opt_strhdk_blsj'; 杀死被锁的pid select pg_terminate_backend(pid); 表结构修改 -- 修改表名 alter table \"user\" rename to \"ts_user\"; -- 添加新字段 alter table table_name add column col_name varchar(50); -- 丢弃某列 alter table table_name drop column col_name; -- 添加主键 alter table table_name add primary key(\"col_name\"); -- 修改字段名 alter table table_name rename column old_col_name to new_col_name; 数据更新和查询 设置某字段的值 -- 设置某字段的值 update table_name set col_name=new_value; -- 更新某个字段并关联其他表 UPDATE table1 SET field_to_update = table2.new_value FROM table2 WHERE table1.common_column = table2.common_column; 删除表中重复数据 -- 查询[旧表]数据的重复情况 select col1,col2,count(*) from old_table group by col1,col2; -- 所有字段都一样的情况 create table bak_table as select distinct * from table_name; -- 查询[新表]数据的重复情况 select col1,col2,count(*) from bak_table group by col1,col2; truncate table old_table; insert into old_table (col1,col2) select col1,col2 from bak_table; 不存在插入，存在更新 insert into ... on conflict(column_name) do update set ... conflict(column_name): column_name字段是判断要查找的数据是否存在，作为判断条件 column_name必须是主键或者其他具有唯一性的字段(如唯一键或排他键) insert into user(id,username,address,create_date,create_by) values('1','刘德华','香港',now(),'system') on conflict(id) do update set address='中国',update_date=now(),update_by='system'; # 批量的方式 insert into testunnest(id, age, name) values (unnest(array[1,3]), unnest(array[18,10]), unnest(array['valupdated', 'val3'])) on conflict (id) do update set age = excluded.age, name = excluded.name; 数据和结构复制 -- [复制表和数据] 复制表结构和数据 自动建表，不会复制主键什么的 create table new_table as select * from old_table; -- [复制数据] 复制数据到 新表 表需要提前建，并且表字段要一致，不会复制主键什么的 insert into new_table (col_0, col_1) select col_0, col_1 from old_table; 视图 普通视图 视图是一个虚拟表，它是根据一个或多个基本表的查询结果动态生成的，每次查询视图时都会执行相应的查询 CREATE VIEW view_name AS SELECT column1, column2, ... FROM table_name WHERE condition; drop view view_name; 物化视图 物化视图是一个实际存储数据的表，它的数据定期刷新，不像普通视图那样每次查询都重新计算。 CREATE MATERIALIZED VIEW materialized_view_name AS SELECT column1, column2, ... FROM table_name WHERE condition; drop MATERIALIZED VIEW materialized_view_name 需要注意的是，物化视图需要定期手动或自动刷新以更新数据，你可以使用 REFRESH MATERIALIZED VIEW 命令来进行刷新 分页查询 select * from table_name limit 10000 offset 20000; 删除重复记录 postgresql 常用的删除重复数据方法 -- 初始化数据 create table hyc_tmp_del_test(id int, name varchar(255)); create table hyc_tmp_del_test_bk (like hyc_tmp_del_test); insert into hyc_tmp_del_test select generate_series(1, 10000), 'huangyc'; insert into hyc_tmp_del_test select generate_series(1, 10000), 'huangyc'; insert into hyc_tmp_del_test_bk select * from hyc_tmp_del_test; -- 最容易想到的方法就是判断数据是否重复，对于重复的数据只保留ctid最小（或最大）的数据，删除其他的 -- id相同的数据，保留ctid最小的，其他的删除 explain analyse delete from hyc_tmp_del_test_bk a where a.ctid <> (select min(t.ctid) from hyc_tmp_del_test_bk t where a.id=t.id); -- 17.112s -- group by方法通过分组找到ctid最小的数据，然后删除其他数据 explain analyse delete from hyc_tmp_del_test_bk a where a.ctid not in (select min(ctid) from hyc_tmp_del_test_bk group by id); -- 0.052s -- 高效删除方法 explain analyze delete from hyc_tmp_del_test_bk a where a.ctid = any(array (select ctid from (select row_number() over (partition by id), ctid from hyc_tmp_del_test_bk) t where t.row_number > 1)); -- 0.055s 第二种和第三种感觉差不多，原文说是第三种快不少，这里pg库是14.x版本 关键 pg中每个表都有几个系统隐藏列：tableoid， xmin， xmax，cmin，cmax，ctid 其中tableoid表示表的oid，cmin、cmax、xmin和xmax是mvcc的实现有关 ctid表示行版本在表中的物理位置: 它属于对象标识符类型(oid，Object Identifier Types)，是一种行标识符，它的数据使用的元组标识符(tid，tuple identifier)。元组ID是一对(块号，块内的元组索引)，用于标识当前行的物理位置。 索引 -- 获取数据库表中的索引 select * from pg_indexes where tablename = 't_cfg_opinfo'; -- 创建索引 create index index_name on table_name (col_0, col_1); -- 查询索引 select * from pg_indexes where tablename='table_name'; -- 删除索引 drop index index_name; 什么情况下要避免使用索引？ 虽然索引的目的在于提高数据库的性能，但这里有几个情况需要避免使用索引 使用索引时，需要考虑下列准则： 索引不应该使用在较小的表上 索引不应该使用在有频繁的大批量的更新或插入操作的表上 索引不应该使用在含有大量的 NULL 值的列上 索引不应该使用在频繁操作的列上 实用sql -- 查询库中的最大版本 SELECT (CASE WHEN MAX(version) IS NULL THEN -1 ELSE MAX(version) END) + 1 AS version FROM table_name 其他语法 筛选某列，逗号拼接 select string_agg(bs_org_id,',') as bs_org_ids from bs_org where par_org_id ='100' 日期转换 select to_char(col_name,'yyyyMMDD')-interval '2 day' from table_name -- -interval '2 day' 表示往前2天 转时间戳 select '2011-01-06 09:57:59'::timestamp; TO_TIMESTAMP('2011-01-06 09:57:59', 'YYYY-MM-DD HH24:MI:S') postgresql 获取分组第一条数据 窗口函数 给数据分组并排名，使用 row_number() over (partition by 分组的字段名 order by 排序规则) as 排名 从上述第一步中取出，排名为第一的数据，即为第一条数据 select * from 上述第一步 where 排名=1 获取前N名的数据，将一中第二步的条件换成where 排名 distributed key alter table table_name set distributed by (id); alter table table_name add primary key (id); ORM框架 ORM框架比较 一文了解 Python 的三种数据源架构模式 SQLAlchemy 和其他的 ORM 框架 SQLObject 优点： 采用了易懂的ActiveRecord 模式 一个相对较小的代码库 缺点： 方法和类的命名遵循了Java 的小驼峰风格 不支持数据库session隔离工作单元 Storm 优点： 清爽轻量的API，短学习曲线和长期可维护性 不需要特殊的类构造函数，也没有必要的基类 缺点： 迫使程序员手工写表格创建的DDL语句，而不是从模型类自动派生 Storm的贡献者必须把他们的贡献的版权给Canonical公司 Django's ORM 优点： 易用，学习曲线短 和Django紧密集合，用Django时使用约定俗成的方法去操作数据库 缺点： 不好处理复杂的查询，强制开发者回到原生SQL 紧密和Django集成，使得在Django环境外很难使用 peewee 优点： Django式的API，使其易用 轻量实现，很容易和任意web框架集成 缺点： 不支持自动化 schema 迁移 多对多查询写起来不直观 SQLAlchemy 优点： 企业级API，使得代码有健壮性和适应性 灵活的设计，使得能轻松写复杂查询 缺点： 工作单元概念不常见 重量级API，导致长学习曲线 相比其他的ORM， SQLAlchemy 意味着，无论你何时写SQLAlchemy代码， 都专注于工作单元的前沿概念 。DB Session 的概念可能最初很难理解和正确使用，但是后来你会欣赏这额外的复杂性，这让意外的时序提交相关的数据库bug减少到0。在SQLAlchemy中处理多数据库是棘手的， 因为每个DB session 都限定了一个数据库连接。但是，这种类型的限制实际上是好事， 因为这样强制你绞尽脑汁去想在多个数据库之间的交互， 从而使得数据库交互代码很容易调试。 SQLAlchemy SQLAlchemy 1.4 Documentation sqlalchemy操作数据库 sqlalchemy外键和relationship查询 SQLALlchemy数据查询小集合 SQLAlchemy 的连接池机制 SQLAlchemy 中的 Session、sessionmaker、scoped_session Contextual/Thread-local Sessions SQLAlchemy(常用的SQLAlchemy列选项) 查询官网例子Object Relational Tutorial (1.x API) sqlalchemy外键和relationship查询 session和scoped_session session用于创建程序和数据库之间的会话，所有对象的载入和保存都需通过session对象 。 通过sessionmaker调用创建一个工厂，并关联Engine以确保每个session都可以使用该Engine连接资源 scoped_session 实现了一个线程的隔离，保证不同的线程拿到不同的session, 同一个线程拿到的session 是同一个值 s1 = Session() s2 = Session() s1.add(person) s1.commit() # 必须先close，s2才能继续操作person s1.close() s2.add(person) session 和scoped_session本质上都是用来操作数据库的，只是session 只适合在单线程下面使用 官方文档提到了scoped_session的正确使用方法。request结束后要调用scoped_session.remove() Engine Configuration 使用 create_engine创建我们需要的DB starting point from sqlalchemy import create_engine scheme = 'mysql+pymysql://root:123456@localhost:3306/dev_shopping?charset=utf8' engine = create_engine(scheme, pool_size=10 , max_overflow=-1, pool_recycle=1200) create_engine 函数常用参数： pool_size=10 # 连接池的大小，0表示连接数无限制 pool_recycle=-1 # 连接池回收连接的时间，如果设置为-1，表示没有no timeout, 注意，mysql会自动断开超过8小时的连接，所以sqlalchemy沿用被mysql断开的连接会抛出MySQL has gone away max_overflow=-1 # 连接池中允许‘溢出’的连接个数，如果设置为-1，表示连接池中可以创建任意数量的连接 pool_timeout=30 # 在连接池获取一个空闲连接等待的时间 echo=False # 如果设置True, Engine将会记录所有的日志，日志默认会输出到sys.stdout 创建Engine之后，接下来的问题，就是如何使用Engine 在单进程中，建议在在初始化的模块的时候创建Engine, 使Engine成为全局变量， 而不是为每个调用Engine的对象或者函数中创建, Engine不同于connect, connect函数会创建数据库连接的资源，Engine是管理connect创建的连接资源 在多进程中，为每个子进程都创建各自的Engine, 因为进程之间是不能共享Engine 几种操作方式 Working with Engines and Connections SqlAlchemy的Engine，Connection和Session 区别？适合什么时候用？ Engine方式 Engine是SQLAlchemy中连接数据库最底层级别的对象，它维护了一个连接池，可以在应用程序需要和数据库对话时使用。在Engine.execute(close_with_result=True) close_with_result=True 表示连接自动关闭； result = engine.execute('SELECT * FROM tablename;') conn = engine.connect(close_with_result=True) result = conn.execute('SELECT * FROM tablename;') for row in result: print(result['columnname'] result.close() Connection方式 Connection，实际上是执行SQL查询的工作，每当你想更好的控制连接的属性，如何时关闭等都建议使用这个操作；比如在一个事务中，要控制它提交commit的时间，在connection控制中就可以运行多个不同的SQL语句，如果其中一个出现问题，则其他所有的语句都会撤销更改； connection = engine.connect() trans = connection.begin() try: connection.execute(\"INSERT INTO films VALUES ('Comedy', '82 minutes');\") connection.execute(\"INSERT INTO datalog VALUES ('added a comedy');\") trans.commit() except: trans.rollback() raise Session方式 Session，一般都是用于ORM中，因为在ORM中，会自动生成SQL语句以及自动连接数据库（自己配置），使用session.execute（）也是个编辑的方法，可以将会话绑定到任何对象；如果你确定使用ORM，就建议使用session来处理execute(),否则还是使用connection更好方便； 总结: 从应用角度来看，可以把这三类分为两种： 直接使用Engine.execute() 或Connection.execute()，更加灵活，可以使用原生SQL语句 使用Session处理交易类型的数据，因为方便使用session.add(), session.rollback(), session.commit(), session.close()等，它是使用ORM时推荐的一种和数据库交互的方式 Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-12-01 00:57:11 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/12.文件和目录访问.html":{"url":"chapters/12.文件和目录访问.html","title":"文件和目录访问","keywords":"","body":"文件基本使用文件打开和关闭文件内容的读取数据的文件写入File对象的属性高阶使用临时文件和目录入门示例常用的函数数据解压缩gzip 格式ZIP格式tar格式os库基本概念path子库导入文件操作增删文件[夹]目录遍历目录操作高阶文件操作复制和删除磁盘使用统计os库之环境参数其他使用 文件 文件是数据的抽象和集合 文件是存储在辅助存储器上的数据序列 文件是数据存储的一种形式 文件展现形态：文本文件和二进制文件 文本文件 vs. 二进制文件 文件文件和二进制文件只是文件的展示方式 本质上，所有文件都是二进制形式存储 形式上，所有文件采用两种方式展示 文本文件 由单一特定编码组成的文件，如UTF-8编码 由于存在编码，也被看成是存储着的长字符串 适用于例如：.txt文件、.py文件等 二进制文件 直接由比特0和1组成，没有统一字符编码 一般存在二进制0和1的组织结构，即文件格式 适用于例如：.png文件、.avi文件等 基本使用 文件打开和关闭 基本使用 = open(, , encoding='utf-8') 文件句柄 文件路径和名称 文本 or 二进制 .close() 打开模式 文件的打开模式 描述 'r' 只读模式，默认值，如果文件不存在，返回FileNotFoundError 'w' 覆盖写模式，文件不存在则创建，存在则完全覆盖 'x' 创建写模式，文件不存在则创建，存在则返回FileExistsError 'a' 追加写模式，文件不存在则创建，存在则在文件最后追加内容 'b' 二进制文件模式 't' 文本文件模式，默认值 '+' 与r/w/x/a一同使用，在原功能基础上增加同时读写功能 示例 a = open(\"f.txt\", \"rt\") a.read(size) # - 一次读入，统一处理 a.readline(size) # - 按数量读入，逐步处理 a.readlines(hint) # - 一次读入，分行处理 a.write(s) a.writelines(lines) a.seek(offset) f = open(\"f.txt\") # - 文本形式、只读模式、默认值 f = open(\"f.txt\", \"rt\") # - 文本形式、只读模式、同默认值 f = open(\"f.txt\", \"w\") # - 文本形式、覆盖写模式 f = open(\"f.txt\", \"a+\") # - 文本形式、追加写模式+ 读文件 f = open(\"f.txt\", \"x\") # - 文本形式、创建写模式 f = open(\"f.txt\", \"b\") # - 二进制形式、只读模式 f = open(\"f.txt\", \"wb\") # - 二进制形式、覆盖写模式 文件内容的读取 # 读入全部内容，如果给出参数，读入前size长度 # .read(size=-1) >>>s = f.read(2) 中国 # 读入一行内容，如果给出参数，读入该行前size长度 # .readline(size=-1) >>>s = f.readline() 中国是一个伟大的国家！ # 读入文件所有行，以每行为元素形成列表 # 如果给出参数，读入前hint行 # .readlines(hint=-1) while True: lines = f.readlines(100000) if not lines: break for line in lines: pass # do something 数据的文件写入 # 1）将嵌套列表或字典转为json格式数据 import json f=open('a1.txt','w',encoding='utf-8') dic={1:'张三',2:'李四'} dic2={11:'张三三',2:'李思思'} a=json.dumps(dic) f.write(a+'\\n') b=json.dumps(dic2) f.write(b+'\\n') f.close() # 2)将文件中的列表或字典json字符串读取出来 import json f=open('a1.txt','r',encoding='utf-8') for i in f: print(json.loads(i)) f.close() # 改变当前文件操作指针的位置，offset含义如下： # 0 – 文件开头； 1 – 当前位置； 2 – 文件结尾 # seek（offset [,from]）方法改变当前文件的位置。Offset变量表示要移动的字节数。From变量指定开始移动字节的参考位置。 # 如果from被设为0，这意味着将文件的开头作为移动字节的参考位置。如果设为1，则使用当前的位置作为参考位置。如果它被设为2，那么该文件的末尾将作为参考位置。 >>>f.seek(0) #回到文件开头 # 查看指针位置 f.tell() ls = [\"中国\", \"法国\", \"美国\"]s fo.writelines(ls) File对象的属性 属性 描述 file.closed 返回true如果文件已被关闭，否则返回false。 file.mode 返回被打开文件的访问模式。 file.name 返回文件的名称。 file.softspace 如果用print输出后，必须跟一个空格符，则返回false。否则返回true。 高阶使用 临时文件和目录 一日一技：在Python中创建临时文件用于记录临时数据 tempfile --- 生成临时文件和目录 当我们在做数据分析的时候，可能会由于数据量过大导致内存不足。如果我们没有条件使用更高配置的电脑，也没有办法优化数据，那么我们可以先把计算的中间值存放在一个文本文件中。 这种方案虽然有效，但是中间数据写成的临时文件如果不清理，时间一长就会占用大量硬盘空间。当然你也可以每一次都覆盖临时文件，这样它虽然不会堆积，但当你的分析程序已经停止的时候，临时文件还在硬盘上占用空间。 tempfile 模块专门用于创建临时文件和临时目录 既可以在 UNIX 平台上运行良好，也可以在 Windows 平台上运行良好 入门示例 from tempfile import TemporaryFilewith TemporaryFile('w+t', encoding='utf-8') as f: # 生成中间数据 f.write('中间数据') f.write('另一部分中间数据') # 其他计算过程 # 下面开始读取临时文件 f.seek(0) f.read() # 退出with上下文，临时文件自动被删除 由于临时文件被关闭就会被删除，所以需要实现同时读写文件，因此文件模式为 w+t 使用 TemporaryFile，你没法知道这个临时文件叫做什么名字。 如果你想知道文件名，甚至想让另一个程序打开这个临时文件，那么你可以使用 NamedTemporaryFile： import redisfrom tempfile import NamedTemporaryFile with NamedTemporaryFile('w+t') as f: # 把文件名通过某种方式传给其他程序 client = redis.Redis() client.set('temp_file', f.name) # 后续操作 只要这个临时文件还没有被关闭，那么另一个程序就可以读取这个临时文件 # 通过with语句创建临时目录 with tempfile.TemporaryDirectory() as tmpdirname: print('创建临时目录', tmpdirname) 常用的函数 tempfile 模块函数 功能描述 tempfile.TemporaryFile(mode='w+b', buffering=None, encoding=None, newline=None, suffix=None, prefix=None, dir=None) 创建临时文件。返回一个类文件对象，也就是支持文件 I/O tempfile.NamedTemporaryFile(mode='w+b', buffering=None, encoding=None, newline=None, suffix=None, prefix=None, dir=None, delete=True) 创建临时文件。功能与上一个函数的功能大致相同，只是它生成的临时文件在文件系统中有文件名 tempfile.SpooledTemporaryFile(max_size=0, mode='w+b', buffering=None, encoding=None, newline=None, suffix=None, prefix=None, dir=None) 创建临时文件。与 TemporaryFile 函数相比，当程序向该临时文件输出数据时，会先输出到内存中，直到超过 max_size 才会真正输出到物理磁盘中。 tempfile.TemporaryDirectory(suffix=None, prefix=None, dir=None) 生成临时目录 tempfile.gettempdir() 获取系统的临时目录 tempfile.gettempdirb() 与 gettempdir() 相同，只是该函数返回字节串 tempfile.gettempprefix() 返回用于生成临时文件的前缀名 tempfile.gettempprefixb() 与 gettempprefix() 相同，只是该函数返回字节串 提示：表中有些函数包含很多参数，但这些参数都具有自己的默认值 因此如果没有特殊要求，可以不对其传参 tempfile 模块还提供了 tempfile.mkstemp() 和 tempfile.mkdtemp() 两个低级别的函数 上面介绍的 4 个用于创建临时文件和临时目录的函数都是高级别的函数，高级别的函数支持自动清理，而且可以与 with 语句一起使用 而这两个低级别的函数则不支持，因此一般推荐使用高级别的函数来创建临时文件和临时目录 此外，tempfile 模块还提供了 tempfile.tempdir 属性，通过对该属性赋值可以改变系统的临时目录 数据解压缩 gzip 格式 读取压缩文件示例： import gzip with gzip.open('/home/joe/file.txt.gz', 'rb') as f: file_content = f.read() 创建GZIP 文件示例： import gzip content = b\"Lots of content here\" with gzip.open('/home/joe/file.txt.gz', 'wb') as f: f.write(content) 使用 GZIP 压缩已有的文件示例： import gzip import shutil with open('/home/joe/file.txt', 'rb') as f_in: with gzip.open('/home/joe/file.txt.gz', 'wb') as f_out: shutil.copyfileobj(f_in, f_out) 使用 GZIP 压缩二进制字符串示例： import gzip s_in = b\"Lots of content here\" s_out = gzip.compress(s_in) ZIP格式 tar格式 如何将整个 tar 归档提取到当前工作目录: import tarfile tar = tarfile.open(\"sample.tar.gz\") tar.extractall() tar.close() 如何通过 TarFile.extractall() 使用生成器函数而非列表来提取一个 tar 归档的子集: import os import tarfile def py_files(members): for tarinfo in members: if os.path.splitext(tarinfo.name)[1] == \".py\": yield tarinfo tar = tarfile.open(\"sample.tar.gz\") tar.extractall(members=py_files(tar)) tar.close() 如何基于一个文件名列表创建未压缩的 tar 归档: import tarfile tar = tarfile.open(\"sample.tar\", \"w\") for name in [\"foo\", \"bar\", \"quux\"]: tar.add(name) tar.close() 使用 with 语句的同一个示例: import tarfile with tarfile.open(\"sample.tar\", \"w\") as tar: for name in [\"foo\", \"bar\", \"quux\"]: tar.add(name) 如何读取一个 gzip 压缩的 tar 归档并显示一些成员信息: import tarfile tar = tarfile.open(\"sample.tar.gz\", \"r:gz\") for tarinfo in tar: print(tarinfo.name, \"is\", tarinfo.size, \"bytes in size and is \", end=\"\") if tarinfo.isreg(): print(\"a regular file.\") elif tarinfo.isdir(): print(\"a directory.\") else: print(\"something else.\") tar.close() 如何创建一个归档并使用 TarFile.add() 中的 filter 形参来重置用户信息: import tarfile def reset(tarinfo): tarinfo.uid = tarinfo.gid = 0 tarinfo.uname = tarinfo.gname = \"root\" return tarinfo tar = tarfile.open(\"sample.tar.gz\", \"w:gz\") tar.add(\"foo\", filter=reset) tar.close() os库 基本概念 os库提供通用的、基本的操作系统交互功能，是Python标准库 包含几百个函数，常用路径操作、进程管理、环境参数等几类 路径操作：os.path子库，处理文件路径及信息 进程管理：启动系统中其他程序 环境参数：获得系统软硬件信息等环境参数 path子库 导入 from os.path import join from IPython.core.interactiveshell import InteractiveShell # InteractiveShell.ast_node_interactivity = \"all\" # 可同时输出多个结果 文件操作 判断是否是文件 filepath最后没有/ 默认取// filepath = '../res/file_op/folder/' 'is file' if os.path.isfile(filepath) else 'is folder' 'is dir' if os.path.isdir(filepath) else 'not dir' 'is folder' 'is dir 判断两个路径都指向相同的文件或目录 # 如果两个路径都指向相同的文件或目录，则返回 True os.path.samefile(path1, path2) 增删文件[夹] 创建文件夹 # 2. 创建、删除文件与文件夹 newfolder = '../res/file_op/mkfolder/' os.mkdir(newfolder) # 创建文件夹,不会递归创建不存在的父目录 os.makedirs(newfolder) # 父目录不存在，则递归创建父目录 删除文件夹 os.rmdir(newfolder) # 如果要删除目录，请使用rmdir(). import shutil shutil.rmtree(path) # 递归删除文件夹 删除文件 if os.path.exists(join(filepath,'file_0.txt')): os.remove(join(filepath,'file_0.txt')) # 删除文件 # join('savepath', 'png/demo') #正确 # join('savepath', '/png/demo') #错误 目录遍历 python中os.walk的用法详解 列出目录下的所有子目录 path = '../res/file_op' for dirpath, dirnames, filenames in os.walk(path): for dirname in dirnames: print(os.path.join(dirpath, dirname)) ../res/file_op\\folder 列出目录下的所有文件 for dirpath, dirnames, filenames in os.walk(path): for filename in filenames: print(os.path.join(dirpath, filename)) ../res/file_op\\file_ico.png ../res/file_op\\folder\\file_1.txt filepath = '../res/file_op' fileList = os.listdir(filepath) # 列出所有文件 for fl in fileList: print(fl) file_1.txt 分割文件目录/文件名和后缀 file_path = \"D:/test/test.py\" (filepath, tempfilename) = os.path.split(file_path) # 等价于 (filepath, tempfilename) = os.path.dirname(path),os.path.basename(path) ==> D:/test test.py (filename, extension) = os.path.splitext(tempfilename) ==> test .py 目录操作 修改目录名称 os.rename('d0/d10/d100', 'D100') # 会导致目录移动！ os.path.abspath(path) # 获取一个路径的完全路径 os.path.splitext(path) # 分离文件名和扩展名 os.path.normpath( \"D://PYE//file.txt\" ) # 归一化path的表示形式，统一用\\\\分隔路径 返回当前程序与文件之间的相对路径 (relative path) path = 'E:\\\\PycharmWS\\\\sj\\\\text-mining-framework' os.getcwd() # 'E:\\\\PycharmWS\\\\sj\\\\text-mining-framework\\\\tests\\\\others' In [3]: os.path.relpath(path) Out[3]: '..\\\\..' 返回 path 的大小，以字节为单位，该文件不存在或不可访问时，抛出 OSError 异常 os.path.getsize( \"D:/PYE/file.txt\" ) # 返回path对应文件的大小，以字节为单位 返回序列 paths 中每个路径名称的最长共同有效子路径 # 如果 paths 同时包含绝对和相对路径名称或者如果 paths 为空则会引发 ValueError os.path.commonpath(paths) 高阶文件操作 复制和删除 复制文件[夹] import shutil # shutil.copy(srcfile, dstfile) : 复制srcfile文件，并命名为dstfile shutil.copy('d0/file1.txt', 'd0/file2.txt') # shutil.copy(srcfile, path) : 复制srcfile文件到path路径下 shutil.copy('d0/file1.txt', '..') # shutil.copytree(srcDir, dstDir) : 递归地复制srcDir目录，并命名为dstDir；dstDir不能是已经存在的！ shutil.copytree('d0/', 'D0') 移动文件[夹] # shutil.move(srcfile, dstfile) : 移动srcfile文件并命名为dstfile，相当于文件重命名 shutil.move('d0/file1.txt', 'd0/FILE1.txt') # shutil.move(srcfile, path) ： 移动srcfile文件到path路径下 shutil.move('d0/file2.txt', '.') # shutil.move(srcDir, dstDir) ： 移动srcDir目录(包括其子目录和文件)到dstDir目录下 shutil.move('d0', 'D0') 返回路径和最后一个文件名组成的元组 # 返回路径和最后一个文件名组成的元组 _, filename = os.path.split(os.getcwd()) print(filename) Out[7]: 'python' 磁盘使用统计 磁盘使用统计数据，形式为一个 named tuple，其中包含 total, used 和 free 属性 分别表示总计、已使用和未使用空间的字节数。 在 Windows 上，path 必须是一个目录；在 Unix 上，它可以是一个文件或一个目录 import shutil shutil.disk_usage(path) Out[19]: usage(total=223217709056, used=82961846272, free=140255862784) os库之环境参数 获取或改变系统环境信息 # 修改当前程序操作的路径 >>>os.chdir(\"D:\") # 返回程序的当前路径 >>>os.getcwd() 'D:\\\\' 获取操作系统环境信息 # 获得当前系统登录用户名称 >>>os.getlogin() 'hyc' # 获得当前系统的CPU数量 >>>os.cpu_count() 8 获取操作系统环境信息 # 获得n个字节长度的随机字符串，通常用于加解密运算 >>>os.urandom(10) b'7\\xbe\\xf2!\\xc1=\\x01gL\\xb3' 其他使用 os库之进程管理 # 执行程序或命令command # 在Windows系统中，返回值为cmd的调用返回信息 import os os.system(\"C:\\\\Windows\\\\System32\\\\calc.exe\") os.system(\"C:\\\\Windows\\\\System32\\\\mspaint.exe D:\\\\PYECourse\\\\grwordcloud.png\") 查看剩余空间大小(ubuntu) def checkDis(path): import os hd = {} disk = os.statvfs(path) percent = (disk.f_blocks - disk.f_bfree) * 100 / \\ (disk.f_blocks - disk.f_bfree + disk.f_bavail) + 1 return 100 - percent Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-12-01 00:57:11 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/13.枚举类Enum.html":{"url":"chapters/13.枚举类Enum.html","title":"枚举类Enum","keywords":"","body":"枚举类Enum枚举类有哪些好处两种枚举类定义方式案例简单枚举类定义高级枚举类定义枚举类比较遍历枚举类 枚举类Enum 枚举类有哪些好处 枚举类可以方便地表示星期，月份等常数类型，如果你不用枚举类，那么你只能用数字或者字符串。如果你使用数字，用1-7来表示星期数，但一个数字在程序中不仅可以表示星期数，可能还有其他许多含义，这样你在写程序时就必须时刻记住这些数字的含义，这降低了程序的可读性，也导致容易出错。而当你使用字符串时，虽然没有明显的缺点，但在内存中字符串所占内存要比数字多，这就降低了程序的效率。 枚举类正好弥补了这两方面的缺点，你可以在代码中使用枚举类，但在内存中存放时使用的是数字，既提高了可读性，又提高了程序效率。更重要的是，Python中的枚举类型是不可变类型，又可以进行迭代，这就意味着你可以随时使用枚举类型而不用担心改变了枚举类型的值。 两种枚举类定义方式 程序有两种方式来定义枚举类： 直接使用 Enum 列出多个枚举值来创建枚举类。 通过继承 Enum 基类来派生枚举类。 案例 简单枚举类定义 # 由于枚举类的“new”方法，将会保证内存中只会存在一个枚举类的实例 # -*- coding: utf-8 -* from enum import Enum # 这样我们就获得了Month类型的枚举类 # value属性则是自动赋给成员的int常量，默认从1开始计数。 seasonEnum = Enum('SeasonEnum', ('SPRING', 'SUMMER', 'FALL', 'WINTER')) print(f'seasonEnum: {seasonEnum}') 高级枚举类定义 代码 from enum import Enum, unique # @unique装饰器可以帮助我们检查保证没有重复值。 # 冬天 = 3这里就会报错，否则SeasonEnum.__members__.items，会把 # WINTER => SeasonEnum.WINTER # 冬天 => SeasonEnum.WINTER # 装饰器“@unique”，它会遍历枚举成员，如果发现有重复就会立即抛出“ValueError” @unique class SeasonEnum(Enum): # SPRING = auto() # 也可以全部使用自动值 SPRING = 0 # Sun的value被设定为0 SUMMER = 1 FALL = 2 WINTER = 3 # 冬天 = 3 # 访问这些枚举类型可以有若干种方法： season_1 = SeasonEnum.SPRING print(f'season_1: {season_1}') print(f\"SeasonEnum['SPRING']: {SeasonEnum['SPRING']}\") print(f'SeasonEnum.SPRING.value: {SeasonEnum.SPRING.value}') print(f'SeasonEnum(1): {SeasonEnum(1)}') print(f'season_1 == SeasonEnum(1): {season_1 == SeasonEnum(1)}') 输出 seasonEnum: season_1: SeasonEnum.SPRING SeasonEnum['SPRING']: SeasonEnum.SPRING SeasonEnum.SPRING.value: 0 SeasonEnum(1): SeasonEnum.SUMMER season_1 == SeasonEnum(1): False 枚举类比较 代码 # 枚举类型不能做大小比较，但是可以做身份比较和等值比较。 print(f'SeasonEnum.SPRING is SeasonEnum.SPRING: {SeasonEnum.SPRING is SeasonEnum.SPRING}') print(f'SeasonEnum.SPRING is SeasonEnum.WINTER: {SeasonEnum.SPRING is SeasonEnum.WINTER}') # 这是身份比较，每个对象都可以进行身份比较，枚举类也不例外。 print(f'SeasonEnum.SPRING == SeasonEnum.SPRING: {SeasonEnum.SPRING == SeasonEnum.SPRING}') print(f'SeasonEnum.SPRING != SeasonEnum.WINTER: {SeasonEnum.SPRING != SeasonEnum.WINTER}') 输出 SeasonEnum.SPRING is SeasonEnum.SPRING: True SeasonEnum.SPRING is SeasonEnum.WINTER: False SeasonEnum.SPRING == SeasonEnum.SPRING: True SeasonEnum.SPRING != SeasonEnum.WINTER: True 遍历枚举类 代码 # 如果尝试遍历枚举类型，则后面重复的不会被打印出来。但是，如果想要获取别名， # 我们可以使用属性“members”，它是一个OrderedDict，包括所有定义的枚举名称，包括别名。 for name, member in SeasonEnum.__members__.items(): print(name, '=>', member) for name, member in enumerate(SeasonEnum): print(name, '=>', member) 输出 SPRING => SeasonEnum.SPRING SUMMER => SeasonEnum.SUMMER FALL => SeasonEnum.FALL WINTER => SeasonEnum.WINTER 0 => SeasonEnum.SPRING 1 => SeasonEnum.SUMMER 2 => SeasonEnum.FALL 3 => SeasonEnum.WINTER Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2023-12-01 00:57:11 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "}}